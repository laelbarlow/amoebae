#!/usr/bin/env python3
# Copyright 2018 Lael D. Barlow
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# 
"""Includes several commands for performing specific steps of biological
sequence data analysis.
"""
# Import built-in modules.
import argparse
import sys
import os
# Add the amoebaelib subdirectory to the sys.path so that the modules therein
# can be imported.
sys.path.append(os.path.join(sys.path[0],'amoebaelib'))
import subprocess
import re
import settings
import shutil
import glob
import time

#import pandas as pd

# Import module specifically written for this script (command-specific modules
# imported below).
import module_amoebae


class DispatchAmoebae(object):
    """Class for dispatching amoebae to perform one of its functions.
    """

    def __init__(self):
        parser = argparse.ArgumentParser(
            description="""""",
            epilog=open(os.path.join(os.path.dirname(os.path.realpath(__file__)),'LICENSE.txt'),
                'r').read(),
            usage='''amoebae <command> [<args>]

Commands for setting up data structure:
    mkdatadir        Make a directory with subdirectories and CSV files for
                     storing sequence data, etc.

Commands for similarity searching:
   setup_hmmdb       Construct an HMM database (with hmmpress).
   add_to_dbs        Format and add a file to a formatted directory.
   list_dbs          Print a list of all usable database files in the database
                     directory as defined in the settings file.
   add_to_queries    Add a query file to a formatted directory.
   list_queries      Print a list of all usable query files in the query
                     directory as defined in the settings file.
   get_redun_hits    Run searches with queries to find redundant hits in
                     databases (for interpreting results).
   setup_fwd_srch    Make directory in which to perform forward searches.
   run_fwd_srch      Perform searches with given queries into given dbs.
   sum_fwd_srch      Append information about forward searches to csv summary
                     file (this is used to organize reverse searches).
   setup_rev_srch    Make a directory in which to perform reverse searches.
   run_rev_srch      Perform searches with given forward search hits into given db.
   sum_rev_srch      Append information about reverse searches to csv summary
                     file.
   interp_srchs      Interpret search results based on summary.
   find_redun_seqs   Identify sequences likely encoded on redundant loci
                     predicted for the same species.
   plot              Plot search results.

Commands for phylogenetic analysis using a reference tree:
   add_to_models     Add an alignment, tree, substitution model, names of
                     clade-defining sequences to a directory with other models.
   list_models       Print a list of all usable model/reference tree names in
                     the models directory as defined in the settings file.
   get_alt_topos     Take a tree and make copies with every alternative
                     topology for the branches connecting the clades of
                     interest.

Commands for phylogenetic analysis without a reference tree:
   prune             Identify sequences in a tree, and remove them from a
                     given alignment for further phylogenetic analysis.
   auto_prune        Automatically identify sequences in a tree, and remove
                     them from a given alignment for further phylogenetic
                     analysis.
   reduce_tree       Remove terminal nodes from a given tree if there are
                     not any sequences with the same name in a given multiple
                     sequence alignment file.
   constrain_mb      Add constraint commands to MrBayes input file based on a
                     given tree topology.
   visualize_tree    Parse phylogenetic analysis output files for a single
                     alignment in a given directory, and write human-readable 
                     tree figures to PDF files.
   replace_seqs      Replace sequences in an alignment with their top hits in a
                     given fasta file (useful if genomes or taxon selection has
                     been updated).

Miscellaneous commands:
   csv_to_fasta      Generate a fasta file from sequences detailed in a
                     spreadsheet of similarity search results.
   check_depend      Check that all the dependencies are properly installed and
                     useable.
   check_imports     Check that all the import statements used in the AMOEBAE
                     repository run without error.

''')
        parser.add_argument('command', help='''Specify one of the functionalities
        of amoebae.''')
        # parse_args defaults to [1:] for args, but you need to
        # exclude the rest of the args too, or validation will fail
        args = parser.parse_args(sys.argv[1:2])

        help_for_commands_under_development = """
   phylo_class       Determine whether sequences can be classified into one of
                     several pre-defined orthogroups using an
                     established backbone phylogeny by either topology testing with
                     IQtree or simply placing it in the tree with an ML search.
   sum_phylo_class   Add phylo_class results to summary spreadsheet.
   interp_phylo      Interpret results of phylogenetic analyses.
   srch_ali_space    Iteratively modify an alignment/tree and assess measures
                     of support for branches of interest to find alignments
                     that support a given topology.
"""

        if not hasattr(self, args.command):
            print('Unrecognized command')
            parser.print_help()
            exit(1)
        # Use dispatch pattern to invoke method with same name.
        getattr(self, args.command)()


    def check_depend(self):
        parser = argparse.ArgumentParser(
            description="""Check that all the dependencies (other than python
            modules) are properly installed and useable.""",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        # now that we're inside a subcommand, ignore the first
        # two argvs, ie the command (add_to_db) and the subcommand(s)
        # (new_file).
        args = parser.parse_args(sys.argv[2:])

        # Check for similarity search programs.
        print('\n\nBLASTP version:')
        print(subprocess.getoutput(['blastp -version | head -n 1']))
        print('\n\nHMMer version:')
        print(subprocess.getoutput("hmmsearch -h | head -n 4"))
        print('\n\nHMMer esl-fetch utilities:')
        print(subprocess.getoutput("esl-sfetch -h | head -n 4"))

        # Check for alignment programs.
        print('\n\nMUSCLE version:')
        print(subprocess.getoutput(['muscle -version']))

        # Check for phylogenetics programs.
        print('\n\nIQ-TREE version:')
        #print(subprocess.getoutput(['iqtree', '-h', '|', 'head', '-n', '1']))
        print(subprocess.getoutput(['iqtree -h | head -n 1']))

        # Check for latex (pdflatex)?
        # ...

        print('\n')


    def check_qt(self):
        parser = argparse.ArgumentParser(
            description="""Check that all the problematic dependency qt is
            running properly.""",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        args = parser.parse_args(sys.argv[2:])

        # Print try importing modules.
        print('Trying to import necessary modules.')
        from ete3 import Tree, faces, AttrFace, TreeStyle, NodeStyle, TextFace
        print('Successfully imported necessary modules.')

        # Check for similarity search programs.
        print('Trying to import run_qt function from check_qt module')
        from check_qt import run_qt
        print('Succesfully imported the run_qt function from the check_qt module')
        
        # Call function to run qt.
        print('Running code that depends on qt.')
        run_qt()


    def check_imports(self):
        parser = argparse.ArgumentParser(
            description="""Check that all the import statements used in the
            AMOEBAE repository run without error.""",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        # now that we're inside a subcommand, ignore the first
        # two argvs, ie the command (add_to_db) and the subcommand(s)
        # (new_file).
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific functions.
        from get_nonredun_import_statements_for_amoebae import find_and_test_all_import_statements_in_amoebae

        # Call function for checking import statements.
        find_and_test_all_import_statements_in_amoebae()


    # Incomplete function:
    def mkdatadir(self):
        parser = argparse.ArgumentParser(
            description="""Make a directory with subdirectories and CSV files for
            storing sequence data, etc.""",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('new_dir_path', help='''Specify the full file path
                that you want the new directory to have.''')
        # now that we're inside a subcommand, ignore the first
        # two argvs, ie the command (add_to_db) and the subcommand(s)
        # (new_file).
        args = parser.parse_args(sys.argv[2:])

        # Define timestamp for directory.

        # Check that the input path does not exist.
        assert not os.path.exists(args.new_dir_path), """The path you specified
        already exists, try a different path."""

        # Make main directory by copying template folder to new path.
        path_to_template_dir = os.path.join(sys.path[0],
        os.path.join('templates', 'template_data_storage_directory')) 
        shutil.copytree(path_to_template_dir, args.new_dir_path)

        # Print message to prompt user to add the path to the main data
        # directory to their settings.py file.
        print("""
        
        To allow AMOEBAE scripts to locate your new data directory, change the
        value of the root_amoebae_data_dir variable in the settings.py file to
        the full path to the directory:

        %s
        """ % args.new_dir_path)


    def add_to_dbs(self):
        parser = argparse.ArgumentParser(
            description='Format and add a file to a formatted directory.',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)

        # NOT prefixing the argument with -- means it's not optional.
        #parser.add_argument('db_dir', help='''Path to directory to which a
        #reformatted version of the input file is to be added.''')
        #parser.add_argument('csv_file', help='''Path to spreadsheet to add
        #information about added file to (CSV format).''')
        parser.add_argument('new_file', help='''Can be a fasta file (prot or
        nucl) or HMM databases, generated using the hmmpress program in the
        HMMer software package. Or a GFF3 annotation file.''')

        # prefixing the argument with -- means it's optional
        parser.add_argument('--split_char', default=' ', help='''Character to
        split the header string on for extracting the accession.''')
        parser.add_argument('--split_pos', default='0', help='''Position that
        the accession will be in after splitting.''')
        parser.add_argument('--skip_header_reformat', action='store_true',\
                help='''Skip reformatting of header lines in input fasta file.''')
        parser.add_argument('--auto_extract_accs', action='store_true',\
                help='''Automatically identify accessions/IDs in sequence
                headers (overrides split_char and split_pos options above).''')

        # now that we're inside a subcommand, ignore the first
        # two argvs, ie the command (add_to_db) and the subcommand(s)
        # (new_file).
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        import gffutils
        import module_add_to_db
        from module_amoebae_hmmscan import determine_if_is_hmmdb

        # Define paths to database directory and genome info spreadsheet file.
        db_dir = settings.dbdirpath
        csv_file = settings.db_info_csv

        # Perform different procedures if the file is a fasta file vs. an HMM
        # database file, or a GFF file.
        outfp = None
        is_hmmdb = determine_if_is_hmmdb(args.new_file)
        if is_hmmdb:
            # Check that the file has the right filename extension.
            assert args.new_file.rsplit('.', 1)[1] == 'hmmdb', """Filename
            extension for input hmm database must be .hmmdb (and this must be
            reflected in the index file names as well)."""

            # Copy the file into the database dir along with index files.
            for i in glob.glob(args.new_file + '*'):
                shutil.copyfile(i, os.path.join(db_dir,
                            os.path.basename(i)))

            # Define output file name for recording in spreadsheet.
            outfp = os.path.join(db_dir, os.path.basename(args.new_file))

        elif args.new_file.rsplit('.', 1)[1].startswith('gff') or\
             args.new_file.rsplit('.', 1)[1].startswith('GFF'):
            # (if input file is a GFF file)

            # Check that the file is gff3.
            assert args.new_file.rsplit('.', 1)[1] == 'gff3', """Input file
            must be in GFF3 format (and name must end with .gff3 extension).""" 

            # Copy file to output directory.
            outfp1 = os.path.join(db_dir,
                    os.path.basename(args.new_file))
            assert not os.path.isfile(outfp1), """File already exists in
            directory: %s""" % outfp1
            shutil.copyfile(args.new_file, outfp1)

            # Make a SQL database.
            sql_path = outfp1.rsplit('.', 1)[0] + '.sql'
            db = gffutils.create_db(outfp1, sql_path, force=True, keep_order=True,
                merge_strategy='merge', sort_attribute_values=True)

            # Define the SQL database file as the one to list in the
            # spreadsheet (because this is the one that will be accessed).
            outfp = sql_path

            # Note: Still need to add additional information to the
            # spreadsheet to associate the gff file with a specific nucleotide
            # sequence file.

        else:
            # (if input file is a fasta file)

            # Get new filename extension to use.
            exten = module_amoebae.get_corr_fasta_exten(args.new_file)

            # Convert headers and write output to new path.
            new_file_new_exten = args.new_file.rsplit('.', 1)[0] + '.' + exten
            outfp = os.path.join(db_dir, os.path.basename(new_file_new_exten))
            module_add_to_db.convert_headers(args.new_file, outfp, args.split_char,
                    args.split_pos, args.skip_header_reformat, args.auto_extract_accs)

            # Make blastable database.
            module_add_to_db.make_blast_db(outfp)

            # Make index file with esl-sfetch from the HMMer3 software package.
            if exten == 'faa' or exten == 'fna':
                module_add_to_db.make_easel_index(outfp)
            else:
                pass # ...

        # Add corresponding line to spreadsheet.
        module_add_to_db.update_csv(outfp, csv_file)


    def list_dbs(self):
        parser = argparse.ArgumentParser(
            description='''Print a list of all usable query files in the query
            directory as defined in the settings file.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        args = parser.parse_args(sys.argv[2:])

        # Get current time for timestamp.
        #timestamp = time.strftime("%Y_%m_%d_%H_%M_%S")

        # Get path to query directory from settings.py file.
        db_dir = settings.dbdirpath

        # Get all files in query directory.
        all_files = glob.glob(os.path.join(db_dir, '*'))

        # Define regular expression for identifying usable query file
        # paths.
        usable_db = re.compile(r'.+\.faa$|.+\.fna$|.+\.hmmdb$')

        # Get list of files that can be used as databases.
        db_file_list = []
        for f in all_files:
            if usable_db.search(f):
                db_file_list.append(f)

        # Check that at least one usable query file was identified.
        assert len(db_file_list) > 0, """Error: No usable database files
        could be identified in database directory: %s""" % db_dir

        # Sort database file list.
        db_file_list.sort()

        # Print database file list.
        for i in db_file_list:
            print(os.path.basename(i))
        

    def add_to_queries(self):
        parser = argparse.ArgumentParser(
            description="""Add a query file to a formatted directory. This
            command adds a given sequence file to the directory with the path
            that you have specified in the settings.py file, and appends a
            corresponding line to the CSV file that you specified (e.g.,
            '0_query_info.csv') to indicate the query title, etc.""",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        #parser.add_argument('query_dir', help='''Path to a directory that will
        #        contain queries to be used for similarity searching.''')
        #parser.add_argument('csv_file', help='''Path to spreadsheet to add
        #        information about added file to (CSV format).''')
        parser.add_argument('query_file', help='''Path to a sequence file in
        FASTA format that can be used as a similarity search query file. Or
        path to a directory containing only files for addition to the queries.
        Note: By default, the portion of the input filename preceding the first
        underscore character will be recorded as the "query title", the
        remaining substring preceding the second underscore character will be
        recorded as the taxon (e.g., "Hsapiens"), and the rest of the filename
        preceding the filename extension will be recorded as the sequence ID.
        So the filename might look like this:
        "QUERYTITLE_HSAPIENS_SEQUENCEID.fa". However, the relevant information
        can be revised in the "Queries/0_query_info.csv" file afterward if
        necessary.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        import module_amoebae_search
        import module_add_to_queries
        from module_amoebae_get_datatype import get_dbtype
        from module_amoebae_nex_to_hmm import afa_to_hmm

        assert not args.query_file.startswith('.nex'), """Input file must be in
        fasta format."""
        assert not args.query_file.startswith('.phy'), """Input file must be in
        fasta format."""

        # Define path to csv file.
        csv_file = settings.query_info_csv
        assert os.path.isfile(csv_file), """Could not identify query info
        spreadsheet file."""

        # Get list of files to add to queries.
        new_query_file_list = []
        if os.path.isfile(args.query_file):
            new_query_file_list = [args.query_file]
        elif os.path.isdir(args.query_file):
            for i in glob.glob(os.path.join(args.query_file, '*')):
                    new_query_file_list.append(i)
        # Check that at least one file was added to the list.
        assert len(new_query_file_list) >= 1, """Could not identify any query
        files to add."""

        # Process each new query file.
        for query_file in new_query_file_list:
            # Detect input file type (FASTA or aligned FASTA).
            filetype = None
            if module_add_to_queries.is_single_fasta(query_file):
                filetype = 'fa'
            else:
                filetype = 'afa'

            # Check that sequences are all the same length if an alignment.
            if filetype == 'afa':
                assert\
                module_add_to_queries.fasta_seqs_all_same_len(query_file),\
                """Error: Sequences in input alignment are not all the same length:
                    %s""" % query_file

            # Detect datatype (prot or nucl).
            datatype = get_dbtype(query_file)

            # Determine query directory path to write output to.
            query_dir = settings.querydirpath

            # Define a modified query file path with extension.
            mod_query_path =\
            module_add_to_queries.get_mod_query_path(query_file, filetype,
                    datatype, query_dir)

            # Check that such a query file does not already exist.
            assert not os.path.isfile(mod_query_path), """Query file already exists
            with path:\n\t%s""" % mod_query_path

            # Write a copy of the query file with modified filename and extension
            # to the given queries directory.
            shutil.copyfile(query_file, mod_query_path)

            # Construct HMM file if input is alignment.
            if filetype == 'afa':
                # Define path for hmm file.
                hmm_path = module_amoebae_search.get_out_hmm_path(mod_query_path)

                # Make HMM out of alignment.
                afa_to_hmm(mod_query_path, hmm_path)

            # Add corresponding line to query info spreadsheet.
            module_add_to_queries.update_query_csv(csv_file, mod_query_path,
                    datatype)

    def list_queries(self):
        parser = argparse.ArgumentParser(
            description='''Print a list of all usable query files in the query
            directory as defined in the settings file.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        args = parser.parse_args(sys.argv[2:])

        # Get current time for timestamp.
        #timestamp = time.strftime("%Y_%m_%d_%H_%M_%S")

        # Get path to query directory from settings.py file.
        query_dir = settings.querydirpath

        # Get all files in query directory.
        all_files = glob.glob(os.path.join(query_dir, '*'))

        # Define regular expression for identifying usable query file
        # paths.
        usable_query = re.compile(r'.+\.faa$|.+\.afaa$|.+\.fna$|.+\.afna$')

        # Get list of files that can be used as queries.
        query_file_list = []
        for f in all_files:
            if usable_query.search(f):
                query_file_list.append(f)

        # Check that at least one usable query file was identified.
        assert len(query_file_list) > 0, """Error: No usable query files
        could be identified in query directory: %s""" % query_dir

        ## Define file path for output.
        #query_list_file_name = 'query_list_for_finding_redundant_hits_' +\
        #timestamp + '.txt'
        #query_list_file = os.path.join(query_dir, query_list_file_name)

        ## Write query file list to output file.
        #with open(query_list_file, 'w') as o:
        #    for i in query_file_list:
        #        o.write(i + '\n')

        # Sort query file list.
        query_file_list.sort()

        # Print query file list.
        for i in query_file_list:
            print(os.path.basename(i))


    def get_redun_hits(self):
        parser = argparse.ArgumentParser(
            description='''Run searches with queries to find redundant hits in
                databases (for interpreting results).''',
            epilog="""Recommendation: For most analyses, use the --query_name
            option and the --db_name option, and run the get_redun_hits command
            for each query separately.  Otherwise, there will be redundant
            information in the output spreadsheet(s).""",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('srch_dir', help='''Path to directory that will
                contain output directory as a subdirectory.''')
        parser.add_argument('--csv_file', help='''Path to spreadsheet to append summary
                of result to for manual annotation.''', default=None)
        parser.add_argument('--query_name', help='''Query filename to use (not
                full path).''', default=None)
        parser.add_argument('--query_list_file', help='''Path to file
                containing a list of query files to use, if no query_name is
                specified (or all queries by default).''', default=None)
        parser.add_argument('--db_name', help='''Name of database file in the
                database directory in which to do searches (not full path).''',
                default=None)
        parser.add_argument('--db_list_file', help='''Path to file
                containing a list of database files to use (if no db_name
                specified).''', default=None)
        parser.add_argument('--query_title', help='''Name to be assigned to
                hits in databases that may be considered redundant with a
                search query to which the same title is assigned, otherwise it
                is taken from the query info spreadsheet specified in the
                settings.py file ('query_info_csv').''', default=None)
        parser.add_argument('--outdir', help='''Path to directory to write
                search results to.''', default=None)

        parser.add_argument('--blast_report_evalue_cutoff', help='''Maximum
        E-value for reporting BLAST hits.''', default=0.05)
        parser.add_argument('--blast_max_target_seqs', help='''Maximum BLAST
                target sequences to consider.''', default=500)
        parser.add_argument('--hmmer_report_evalue_cutoff', help='''Maximum
        E-value for reporting HMMer hits.''', default=0.05)
        parser.add_argument('--hmmer_report_score_cutoff', help='''Minimum
        sequence score for reporting HMMer hits.''', default=5)
        parser.add_argument('--num_threads_similarity_searching',
                help='''Number of threads to use for running searches.''',
                default=4)

        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        import module_amoebae_search

        # Check that arguments used properly.
        assert not (args.query_name != None and args.query_list_file != None),\
        """Error: query_name and query_list_file options cannot both be
        specified as input.""" 
        assert not (args.query_name == None and args.query_list_file == None),\
        """Either query_name or query_list_file option must be specified as
        input.""" 
        assert not (args.db_name != None and args.db_list_file != None),\
        """Error: db_name and db_list_file options cannot both be specified as
        input.""" 
        assert not (args.db_name == None and args.db_list_file == None),\
        """Either db_name or db_list_file option must be specified as input.""" 

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Get paths to query and database directories from settings.py file.
        query_dir = settings.querydirpath
        db_dir = settings.dbdirpath
        
        # Get query filename list (full paths will be determined downstream based
        # on settings).
        query_file_list = []
        if args.query_list_file is not None:
            # Generate a list from file.
            query_file_list =\
            module_amoebae_search.get_query_list_from_file(args.query_list_file)               
        else:
            # Use input query name.
            query_file_list = [args.query_name]
        assert len(query_file_list) >= 1, """Error: No query files in query
        file list."""

        # Get list of database filenames to search in (full paths will be
        # determined downstream based on settings).
        db_file_list = []
        if args.db_name is not None:
            # Use input name.
            db_file_list = [args.db_name]
        elif args.db_list_file is not None:
            # Read list from input file.
            db_file_list =\
            module_amoebae_search.get_db_list_from_file(args.db_list_file)
        assert len(db_file_list) >= 1, """Error: No database files in database
        file list."""

        # Define output directory path.
        outdir = None
        if args.outdir is not None:
            outdir = args.outdir
        else:
            # Generate an output directory path.
            outdir_basename = 'redun_hits_' + timestamp
            outdir = os.path.join(args.srch_dir, outdir_basename)
            os.mkdir(outdir)

        # If the input csv spreadsheet does not exist, then write an initial file.
        csv_file = None
        if args.csv_file is None:
            # Define a path for the output spreadsheet file.
            csv_file = os.path.join(outdir, '0_redun_hits_' + timestamp + '.csv')
        else:
            csv_file = args.csv_file

        # Run searches, interpret searches, and append to spreadsheet, then
        # delete intermediate files (search method is determined downstream
        # based on input filename extensions and settings).
        main_outfp = module_amoebae_search.get_redun_hits_in_dbs(args.query_title,
                query_file_list, db_file_list, csv_file, outdir, timestamp,
                          args.blast_report_evalue_cutoff,
                          args.blast_max_target_seqs,
                          args.hmmer_report_evalue_cutoff,
                          args.hmmer_report_score_cutoff,
                          args.num_threads_similarity_searching)

        # Prompt manual annotation of spreadsheet to define positive
        # (redundant) hits.
        print("""\n\nEdit spreadsheet to classify hits as redundant or not before
        proceeding (modify values in the 'Positive/redundant (+) or negative
        (-) hit for queries with query title' column):\n\n\t%s\n""" % csv_file)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = args.srch_dir
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_outfp)
        

    def setup_fwd_srch(self):
        parser = argparse.ArgumentParser(
            description='''Make a directory in which to write output files from
            similarity searches.''',
            epilog='''Note: Use the bash script to run forward searches on a remote
            server.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        #parser.add_argument('csv_file', help='''Path to summary spreadsheet
        #        (CSV) file, which may already contain search summaries.''')
        parser.add_argument('srch_dir', help='''Path to directory that will
                contain output directory as a subdirectory.''')
        parser.add_argument('query_list_file', help='''Path to file with list
                of queries to search with.''')
        parser.add_argument('db_list_file', help='''Path to file with list
                of databases to search with.''')
        # Optional arguments.
        parser.add_argument('--outdir', help='''Path to directory to put search
                results into (so that this step can be piped together with
                other commands).''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        import module_amoebae_run_searches

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Get paths to query and database directories from settings.py file.
        #query_dir = settings.querydirpath
        #db_dir = settings.dbdirpath

        ## Check that csv file is in the right place.
        #assert args.srch_dir == os.path.dirname(args.csv_file), """Error: Input csv
        #file is not in the specified search directory (srch_dir)."""

        # Define output directory path, and make it.
        outdir = None
        if not args.outdir == None:
            outdir = args.outdir
        else:
            #outputname = os.path.basename(args.csv_file).rsplit('.', 1)[0] +\
            #'_fwd_srch_' + timestamp 
            outputname = 'fwd_srch_' + timestamp 

            outdir = os.path.join(args.srch_dir, outputname) 
        os.mkdir(outdir)
        assert os.path.isdir(outdir), """Could not create directory: %s""" %\
        outdir

        # Define file paths for new query and db list files.
        db_file_path = module_amoebae_run_searches.get_out_db_list_path(outdir)
        query_file_path = module_amoebae_run_searches.get_out_query_list_path(outdir)

        # Write query and db list files to output folder.
        shutil.copyfile(args.db_list_file, db_file_path)
        shutil.copyfile(args.query_list_file, query_file_path)

        ## Write bash script for running searches on bioinfor (remote server).
        #bash_script_path = os.path.join(outdir, '0_run_fwd_srch.sh')
        #content_template = """blabla? use template string to put input file
        #names here."""
        #with open(bash_script_path, 'w') as o:
        #    o.write(content_template)

        # Write output bash script for running searches.
        with open(module_amoebae_run_searches.get_out_bash_path(outdir), 'w') as o:
            o.write(settings.bash_script_content)
            o.write('run_fwd_srch.py' + ' ' + os.path.join(settings.remote_home_dir, os.path.basename(outdir)))

        # Make output bash script executable?...


        ## Get query list from file.
        #query_file_list =\
        #module_amoebae_search.get_query_list_from_file(args.query_list_file)

        ## Get database list from file.
        #db_file_list = module_amoebae_search.get_db_list_from_file(args.db_list_file)

        ## Run searches.
        #module_amoebae_search.run_all_searches(query_file_list, db_file_list, outdir)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        main_out_path = outdir
        outdir = args.srch_dir
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def run_fwd_srch(self):
        parser = argparse.ArgumentParser(
            description='''Perform searches with original queries into subject
            databases.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        #parser.add_argument('csv_file', help='''Path to summary spreadsheet
        #        (CSV) file, which may already contain search summaries.''')
        parser.add_argument('fwd_srch_dir', help='''Path to directory that will
                contain forward search output files.''')
        parser.add_argument('--blast_report_evalue_cutoff', help='''Maximum
        E-value for reporting BLAST hits.''', default=0.05)
        parser.add_argument('--blast_max_target_seqs', help='''Maximum BLAST
                target sequences to consider.''', default=500)
        parser.add_argument('--hmmer_report_evalue_cutoff', help='''Maximum
        E-value for reporting HMMer hits.''', default=0.05)
        parser.add_argument('--hmmer_report_score_cutoff', help='''Minimum
        sequence score for reporting HMMer hits.''', default=5)
        parser.add_argument('--num_threads_similarity_searching',
                help='''Number of threads to use for running searches.''',
                default=4)
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        import module_amoebae_search
        import module_amoebae_run_searches

        # Get current time for timestamp.
        #timestamp = time.strftime("%Y_%m_%d_%H_%M_%S")
        #timestamp = time.strftime("%Y%m%d%H%M%S")

        # Get paths to query and database directories from settings.py file.
        query_dir = settings.querydirpath
        db_dir = settings.dbdirpath

        ## Check that csv file is in the right place.
        #assert args.srch_dir == os.path.dirname(args.csv_file), """Error: Input csv
        #file is not in the specified search directory (srch_dir)."""

        ## Define output directory path, and make it.
        #outdir = None
        #if not args.outdir == None:
        #    outdir = args.outdir
        #else:
        #    #outputname = os.path.basename(args.csv_file).rsplit('.', 1)[0] +\
        #    #'_fwd_srch_' + timestamp 
        #    outputname = 'fwd_srch_' + timestamp 

        #    outdir = os.path.join(args.srch_dir, outputname) 
        #os.mkdir(outdir)
        #assert os.path.isdir(outdir), """Could not create directory: %s""" %\
        #outdir

        # Get query list from file.
        query_file_list =\
        module_amoebae_run_searches.get_query_list_from_file(module_amoebae_run_searches.get_out_query_list_path(args.fwd_srch_dir))

        # Get database list from file.
        db_file_list =\
        module_amoebae_run_searches.get_db_list_from_file(module_amoebae_run_searches.get_out_db_list_path(args.fwd_srch_dir))

        # Run searches.
        module_amoebae_search.run_all_searches(query_file_list, db_file_list,
                args.fwd_srch_dir,
                     args.blast_report_evalue_cutoff,
                     args.blast_max_target_seqs,
                     args.hmmer_report_evalue_cutoff,
                     args.hmmer_report_score_cutoff,
                     args.num_threads_similarity_searching)


    def sum_fwd_srch(self):
        parser = argparse.ArgumentParser(
            description='''Append information about forward searches to csv
            summary file (this is used to organize reverse searches). For
            TBLASTN searches (protein queries, nucleotide target sequences),
            HSPs are clustered into groups that are close enough within the
            target sequence to potentially represent exons from the same coding
            sequence. The nucleotide subsequences in which these clusters of
            HSPs are found are then analyzed using exonerate to identify and
            translate potential exons, in "protein2genome" mode, because
            exonerate, unlike TBLASTN, attempts to identify exon boundaries,
            yielding translations that are less likely to include translations
            of non-coding regions outside exons (which might include apparent
            stop codons).''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('fwd_srch_out', help='''Path to directory where
                forward search results were written.''')
        parser.add_argument('csv_file', help='''Path to summary spreadsheet
                (CSV) file, which may already contain search summaries, or may
                not exist yet.''')
        # Optional arguments.
        parser.add_argument('--max_evalue', help='''Maximum E-value threshold
                for reporting forward search hits.''', default=0.0005)
        parser.add_argument('--max_gap_between_tblastn_hsps', help='''Maximum
        number of nucleotide bases between TBLASTN HSPs to be considered part
        of the same gene locus. This is important, because it will be assumed
        that HSP separated by more than this number of nucleotide bases are not
        part of the same gene or TBLASTN "hit".''', default=10000)
        parser.add_argument('--do_not_use_exonerate', help='''Override the
        default use of exonerate to identify coding sequences and translations,
        and just use TBLASTN instead. This option is provided because
        concatenated TBLASTN HSPs may be more inclusive of sequences within the
        target sequence, and the results of TBLASTN and exonerate may need to
        be compared. Also, note that HSPs identified by TBLASTN but for which
        exonerate yields no alignments will be ignored if exonerate is
        used.''', action='store_true')
        parser.add_argument('--exonerate_score_threshold', help='''Set score
        threshold to be applied when running exonerate on nucleotide sequences
        identified by TBLASTN. The default for setting of exonerate is 100, but
        a lower score is set as default here, because otherwise exonerate
        cannot identify some of the seqeunces identified by TBLASTN. This
        option is only relevant if using exonerate.''',
        default='10')
        parser.add_argument('--max_hits_to_sum', help='''Maximum number of
        forward search hits to list in the summary spreadsheet.
        If zero, then reverse searches will be performed for all hits.''',
        default=0)
        args = parser.parse_args(sys.argv[2:])

        # Print a temporary warning.
        if not args.do_not_use_exonerate:
            print("""\n\nWarning: Using experimental code for running exonerate to
            improve translation of sequences identified by TBLASTN. If you do not
            want to do this, then use the --do_not_use_exonerate option.\n\n""") 
            #input("""Continue?""")

        # Import command-specific modules.
        import module_amoebae_search

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Get paths to query and database directories from settings.py file.
        query_dir = settings.querydirpath
        db_dir = settings.dbdirpath

        # Get query_list_file and db_list_file paths.
        out_query_file = module_amoebae_search.get_out_query_list_path(args.fwd_srch_out)
        assert os.path.isfile(out_query_file), """Could not find file with
        query list with path: %s""" % out_query_file
        out_db_file = module_amoebae_search.get_out_db_list_path(args.fwd_srch_out)
        assert os.path.isfile(out_db_file), """Could not find file with
        database list with path: %s""" % out_db_file

        # Get query and database lists.
        query_file_list = module_amoebae_search.get_query_list_from_file(out_query_file)
        db_file_list = module_amoebae_search.get_db_list_from_file(out_db_file)

        # Loop over files in the correct order, extract information, and write
        # to csv spreadsheet.
        final_outfp = module_amoebae_search.write_fwd_srch_res_to_csv(\
                          args.fwd_srch_out,
                          query_file_list, 
                          db_file_list, 
                          args.csv_file, 
                          timestamp,
                          args.max_evalue, 
                          args.max_gap_between_tblastn_hsps, 
                          args.exonerate_score_threshold,
                          args.do_not_use_exonerate,
                          args.max_hits_to_sum)

        # Prompt manual inspection of spreadsheet.
        print("""\n\nForward search results written/appended to
                spreadsheet:\n\n\t%s\n""" % args.csv_file)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.csv_file)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, final_outfp)


    def setup_rev_srch(self):
        parser = argparse.ArgumentParser(
            description='''Make directory in which to write results of
            reverse searches.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('srch_dir', help='''Path to directory that will
                contain output directory as a subdirectory.''')
        parser.add_argument('csv_file', help='''Path to summary spreadsheet
                (CSV) file, which contains a summary of forward search(es).''')
        parser.add_argument('databases', help='''Database filename (in database
                directory) or path to file with list of database filenames.
                Note that filenames are needed, not file paths.''')
        # Optional arguments.
        parser.add_argument('--outdir', help='''Path to directory to put search
                results into (so that this step can be piped together with
                other commands).''')
        parser.add_argument('--aasubseq', help='''Use only the portion of each
                (amino acid) forward hit sequence that aligns to the original query
                used (top HSP subject sequence). This is default for nucleotide
                hits.''', action='store_true')
        parser.add_argument('--nafullseq', help='''Use the full (nucleic acid)
                forward hit sequence. This is default for amino acid hits.''',
                action='store_true')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        import module_amoebae_run_searches
        import module_amoebae_search

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Get paths to database directory from settings.py file.
        db_dir = settings.dbdirpath

        # Check that csv file is in the right place.
        #assert args.srch_dir == os.path.dirname(args.csv_file), """Input csv
        #file is not in the specified search directory (srch_dir)."""

        # Define output directory path.
        outdir = None
        if not args.outdir == None:
            outdir = args.outdir
        else:
            outputname = os.path.basename(args.csv_file).rsplit('.', 1)[0] +\
            '_rev_srch_' + timestamp 

            outdir = os.path.join(args.srch_dir, outputname) 

        # Create output directory.
        os.mkdir(outdir)

        # Define subdirectory path for output, and create directory.
        query_subdir = module_amoebae_run_searches.get_query_subdir(outdir)
        os.mkdir(query_subdir)


        # Get reverse search query sequences from summary of forward search
        # results, and write to files in subdirectory of output directory.
        # Get sequences to use as reverse search queries.
        #seq_objs = get_seq_from_fasta_db(db_name, accs)
        query_file_list = module_amoebae_search.get_rev_queries(args.csv_file,
                query_subdir, args.aasubseq, args.nafullseq)

        # Get database(s) to search in.
        db_file_list = None
        if os.path.isfile(args.databases):
            # Check that the input file path is not a fasta file.
            assert not open(args.databases, 'r').read().startswith('>'), """Don't input fasta file paths."""
            # Get list from file.
            db_file_list = module_amoebae_search.get_db_list_from_file(args.databases)
        else:
            # then argument is a filename (not a path).
            expected_path_to_file_in_genomes_folder = os.path.join(db_dir, args.databases)
            assert expected_path_to_file_in_genomes_folder,\
            """File with the name %s does not exist in the directory %s""" % (args.databases, db_dir) 

            # Get list from file.
            db_file_list = [args.databases]


        #Check that database was found.
        assert db_file_list is not None, """Could not find database(s) to use
        from argument: %""" % args.databases


        # Define file paths for new query and db list files.
        db_file_path = module_amoebae_run_searches.get_out_db_list_path(outdir)
        query_file_path = module_amoebae_run_searches.get_out_query_list_path(outdir)

        # Write query and db list files to output folder.
        with open(db_file_path, 'w') as o1, open(query_file_path, 'w') as o2:
            for x in db_file_list:
                o1.write(x)
            for y in query_file_list:
                o2.write(os.path.basename(y) + '\n')


        # Write output bash script for running searches.
        with open(module_amoebae_run_searches.get_out_bash_path(outdir), 'w') as o:
            o.write(settings.bash_script_content)
            o.write('run_rev_srch.py' + ' ' + os.path.join(settings.remote_home_dir, os.path.basename(outdir)))

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        main_out_path = outdir
        outdir = args.srch_dir
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def run_rev_srch(self):
        parser = argparse.ArgumentParser(
            description='''Perform searches with forward search hit sequences
            as queries into the original query databases.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('rev_srch_dir', help='''Path to directory that will
                contain output of searches.''')
        parser.add_argument('--blast_report_evalue_cutoff', help='''Maximum
        E-value for reporting BLAST hits.''', default=0.05)
        parser.add_argument('--blast_max_target_seqs', help='''Maximum BLAST
                target sequences to consider.''', default=500)
        parser.add_argument('--hmmer_report_evalue_cutoff', help='''Maximum
        E-value for reporting HMMer hits.''', default=0.05)
        parser.add_argument('--hmmer_report_score_cutoff', help='''Minimum
        sequence score for reporting HMMer hits.''', default=5)
        parser.add_argument('--num_threads_similarity_searching',
                help='''Number of threads to use for running searches.''',
                default=4)
        #parser.add_argument('csv_file', help='''Path to summary spreadsheet
        #        (CSV) file, which contains a summary of forward search(es).''')
        #parser.add_argument('databases', help='''Database filename (in database
        #        directory) or path to file with list of database filenames.''')
        # Optional arguments.
        #parser.add_argument('--outdir', help='''Path to directory to put search
        #        results into (so that this step can be piped together with
        #        other commands).''')
        #parser.add_argument('--aasubseq', help='''Use only the portion of each
        #        (amino acid) forward hit sequence that aligns to the original query
        #        used (top HSP subject sequence). This is default for nucleotide
        #        hits.''', action='store_true')
        #parser.add_argument('--nafullseq', help='''Use the full (nucleic acid)
        #        forward hit sequence. This is default for amino acid hits.''',
        #        action='store_true')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        import module_amoebae_run_searches
        import module_amoebae_search

        ## Get current time for timestamp.
        ##timestamp = time.strftime("%Y_%m_%d_%H_%M_%S")
        #timestamp = time.strftime("%Y%m%d%H%M%S")

        ## Get paths to database directory from settings.py file.
        #db_dir = settings.dbdirpath

        ## Check that csv file is in the right place.
        #assert args.srch_dir == os.path.dirname(args.csv_file), """Input csv
        #file is not in the specified search directory (srch_dir)."""

        ## Define output directory path.
        #outdir = None
        #if not args.outdir == None:
        #    outdir = args.outdir
        #else:
        #    outputname = os.path.basename(args.csv_file).rsplit('.', 1)[0] +\
        #    '_rev_srch_' + timestamp 

        #    outdir = os.path.join(args.srch_dir, outputname) 

        ## Create output directory.
        #os.mkdir(outdir)

        ## Define subdirectory path for output, and create directory.
        #query_subdir = module_amoebae_search.get_query_subdir(outdir)
        #os.mkdir(query_subdir)


        ## Get reverse search query sequences from summary of forward search
        ## results, and write to files in subdirectory of output directory.
        ## Get sequences to use as reverse search queries.
        ##seq_objs = get_seq_from_fasta_db(db_name, accs)
        #query_file_list = module_amoebae_search.get_rev_queries(args.csv_file,
        #        query_subdir, args.aasubseq, args.nafullseq)

        # Get query file path list from file.
        query_file_list =\
        module_amoebae_run_searches.get_query_list_from_file(module_amoebae_run_searches.get_out_query_list_path(args.rev_srch_dir))

        ## Get database(s) to search in.
        #db_file_list = None
        #if os.path.isfile(args.databases):
        #    # Get list from file.
        #    db_file_list = get_db_list_from_file(args.databases)
        #else:
        #    # then argument is a filename.
        #    assert os.path.isfile(os.path.join(db_dir, args.databases)),\
        #    """File doesn't exist: %s""" % args.databases

        #    db_file_list = [args.databases]

        # Get database file path list from file.
        db_file_list =\
        module_amoebae_run_searches.get_db_list_from_file(module_amoebae_run_searches.get_out_db_list_path(args.rev_srch_dir))

        ##Check that database was found.
        #assert db_file_list is not None, """Could not find database(s) to use
        #from argument: %""" % args.databases

        # For reverse searches, the queries are in a subdirectory of the
        # reverse search directory, not in the query file directory specified
        # in the settings.py file.
        query_dir = module_amoebae_run_searches.get_query_subdir(args.rev_srch_dir)

        # Run searches (query_file_list contains full filepaths, so can use
        # query_subdir contents).
        module_amoebae_search.run_all_searches(query_file_list, db_file_list,
                args.rev_srch_dir,
                     args.blast_report_evalue_cutoff,
                     args.blast_max_target_seqs,
                     args.hmmer_report_evalue_cutoff,
                     args.hmmer_report_score_cutoff,
                     args.num_threads_similarity_searching,
                     query_dir)

        # Report output.
        print('\n\nReverse search results written to directory:\n\t' +
                args.rev_srch_dir\
                + '\n\n')


    def sum_rev_srch(self):
        parser = argparse.ArgumentParser(
            description='''Append information about reverse searches to csv summary
                     file. Use information from redundant hit csv file to
                     interpret results.''',
                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('csv_file', help='''Path to summary spreadsheet
                (CSV) file, which may already contain reverse search summaries.''')
        parser.add_argument('rev_srch_out', help='''Path to directory where
                reverse search results were written.''')
        parser.add_argument('--redun_hit_csv', help='''Path to spreadsheet
                (CSV) file, which specifies which hits are redundant positive
                hits for a given query (query title) in a given database. If
                this is not provided, then it is assumed that the top reverse
                search hit is equivalent to the original query.''')
        parser.add_argument('--min_evaldiff', help='''Minimum difference in
                E-value order of magnitude between top reverse search hit and first
                reverse search hit that is not redundant with the original
                query.''', default=5)
        parser.add_argument('--aasubseq', help='''Use only the portion of each
                (amino acid) forward hit sequence that aligns to the original query
                used (top HSP subject sequence). This is default for nucleotide
                hits. Must be selected if selected when the setup_rev_srch
                command was run.''', action='store_true')
        parser.add_argument('--nafullseq', help='''Use the full (nucleic acid)
                forward hit sequence. This is default for amino acid hits. Must
                be selected if selected when the setup_rev_srch command was
                run.''', action='store_true')
        parser.add_argument('--max_rev_srchs', help='''Maximum number of
        forward search hits to perform reverse searches for per query database.
        If zero, then reverse searches will be performed for all hits.''',
        default=0)
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        import module_amoebae_search
        import module_amoebae_run_searches

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Get timestamp of reverse search results.
        rev_srch_timestamp = '_'.join(args.rev_srch_out.split('_')[-3:]) 

        # Get paths to query directory from settings.py file.
        db_dir = settings.dbdirpath

        # Get path for query subdirectory.
        #query_dir = settings.querydirpath
        query_dir = module_amoebae_run_searches.get_query_subdir(args.rev_srch_out)

        # Get query_list_file and db_list_file paths.
        out_query_file =\
        module_amoebae_search.get_out_query_list_path(args.rev_srch_out)
        assert os.path.isfile(out_query_file), """Could not find file with
        query list with path: %s""" % out_query_file

        assert os.path.isdir(args.rev_srch_out)

        out_db_file =\
        module_amoebae_search.get_out_db_list_path(args.rev_srch_out)
        assert os.path.isfile(out_db_file), """Could not find file with
        database list with path: %s""" % out_db_file

        #print('\n\n')
        #print('Contents of out_db_file:')
        with open(out_db_file) as infh:
            for i in infh:
                #print(i)
                assert len(i) < 1000

        # Get query (will be full paths) and database lists.
        query_file_list = module_amoebae_search.get_query_list_from_file(out_query_file)
        db_file_list = module_amoebae_search.get_db_list_from_file(out_db_file)

        ## Get redundant hit spreadsheet filepath.
        #redun_hit_csv = None
        #if os.path.isfile(args.redun_hit_csv):
        #    redun_hit_csv = args.redun_hit_csv
        #else:
        #    redun_hit_csv = os.path.join(

        #if args.redun_hit_csv is not None:
        #    # Get redundant hit dict from from input redundant hit csv file.
        #    redun_hit_dict = module_amoebae_search.get_redun_hit_dict(args.redun_hit_csv)

        # Loop over files corresponding to results for reverse searches into
        # each reverse search database, in the correct order, extract
        # information, and write to csv spreadsheet (will append a set of new
        # columns for search results into each reverse search database). ***Is
        # query_file_list used?
        db_num = 0
        for db_file in db_file_list:
            db_num += 1
            
            # Check that the db_file is a filename not a file path.
            assert '/' not in db_file, """Databases should be specified by
            filenames not by file paths."""

            print('\nSummarizing results from reverse searches into ' +\
                    os.path.basename(db_file) + ' (database #' + str(db_num) +\
                    ')')
            final_outfp = module_amoebae_search.write_rev_srch_res_to_csv(rev_srch_timestamp + '-' + str(db_num), args.rev_srch_out,
                    query_file_list, db_file, args.csv_file, args.redun_hit_csv,
                    args.min_evaldiff, timestamp, args.aasubseq,
                    args.max_rev_srchs)

        # Prompt manual inspection of spreadsheet.
        print("""\n\nForward search results written/appended to
                spreadsheet:\n\n\t%s\n""" %\
                module_amoebae_search.get_csv_with_rev_path(args.csv_file))

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.csv_file)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, final_outfp)


    def setup_hmmdb(self):
        parser = argparse.ArgumentParser(
            description='''Construct an HMM database (with hmmpress). This is
            for later sorting of given sequences into categories based on which
            HMM the score highest against.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('indirpath', help='''Path to directory containing
        amino acid sequence alignment file(s) to be constructed into an HMM
        database using hmmpress from the HMMer3 software package.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_hmmscan import make_hmm_db

        # Call the function imported from the module.
        make_hmm_db(args.indirpath)


    def interp_srchs(self): 
        parser = argparse.ArgumentParser(
            description='''Interpret search results based on final summary, which
            provides a basis for further analyses of positive hits.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('csv_file', help='''Path to spreadsheet with
        forward and reverse search results.''')
        parser.add_argument('--fwd_only', help='''Interpret forward searches
                based on score (HMMer) cutoff.''',
                action='store_true')
        parser.add_argument('--fwd_evalue_cutoff', help='''Specify an (more
                stringent) E-value cutoff for forward search results.''',
                default=None)
        parser.add_argument('--rev_evalue_cutoff', help='''Specify an (more
                stringent) E-value cutoff for reverse search results.''',
                default=None)
        parser.add_argument('--hmmer_cutoff', help='''Specify a score that hits
                must exceed to be included.''', default=20)
        # Is this option obsolete?
        parser.add_argument('--redun_hits', help='''Interpret which hits are
                redundant in output of get_redun_hits command.''',
                action='store_true')
        parser.add_argument('--out_csv_path', help='''Optionally specify an
                output file path, so that this command can be piped together
                with others.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        import module_amoebae_search

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Define output file path. 
        outfp = None
        if args.out_csv_path == None:
            outfp = args.csv_file.rsplit('.', 1)[0] + '_interp_' + timestamp + '.csv'
        else:
            outfp = args.out_csv_path

        if not args.redun_hits:
            if args.fwd_only:
                # Write output file with interpetation column appended.
                module_amoebae_search.write_fwd_srch_interp_csv(args.csv_file,
                        outfp, args.hmmer_cutoff, args.fwd_evalue_cutoff)
            else:
                # Write output file with interpetation column appended.
                module_amoebae_search.write_interp_csv(args.csv_file, outfp,
                        args.fwd_evalue_cutoff, args.rev_evalue_cutoff)

        else:
            # Write output file with interpetation column appended.
            module_amoebae_search.write_redun_hit_interp_csv(args.csv_file, outfp)

        # Prompt manual inspection of spreadsheet.
        print("""\n\nInterpretations written/appended to
                spreadsheet:\n\n\t%s\n""" %\
                outfp)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.csv_file)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, outfp)


    def find_redun_seqs(self):
        parser = argparse.ArgumentParser(
            description='''Identify hit sequences likely encoded by the same
            gene loci in the genome of a given species, or otherwise not
            representing paralogous genes.  

            Criteria are applied in this order:

            1. Peptide hits with the same ID as higher-ranking hits for the
            same query (query title) are excluded.

            2. Nucleotide hits for the same loci as peptide sequence hits are
            excluded.

            3. Sequences with internal stop codons are excluded, as these are
            potentially pseudogenes.

            4. Sequences are excluded if they do not meet several minimum
            length criteria: Absolute minimum length (in amino acids) and
            percent query cover.

            5. Sequences are excluded if they do not overlap to a specified
            degree with all included higher-ranking hits for the same query
            (query title) in sequence data for the same species/genome. This is
            determined by algorithmically comparing pairs of sequences aligned
            to a reference alignment of homologues, and several minimum
            measures of alignment overlap may be specified.

            6. Secondary hit sequences are excluded if they do not meet a
            specified maximum percent identity threshold. Highly identical
            sequences may result from false segmental duplications in the
            genome assembly, may represent alleles, etc. 
            
            Note: Applying these criteria requires a column to be manually
            added to the input csv file prior to running with the header
            "Alignment for sequence comparison" and filled with the appropriate
            alignment name to use (one for each query title, as listed in the
            "Query title" column). Alternatively, you can run this command with
            the --add_ali_col option to automatically identify appropriate
            alignments among your aligned FASTA queries used for running HMMer
            searches.''', 
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('csv_file', help='''Path to spreadsheet with
        interpreted search results outputted by the interp_srchs command.''')
        parser.add_argument('--out_csv_path', help='''Optionally specify an
                output file path, so that this command can be piped together
                with others.''', default=None)
        #parser.add_argument('--metric_name', help="""Name of metric to be used
        #        for evaluating whether two sequences are redundant or not. Can be
        #        either 'percent_identity' or 'gene_model_redundancy_index'.""",\
        #                default='percent_identity')
        #parser.add_argument('--metric_value_minimum', help="""Minimum value of
        #        specified metric for evaluating whether two sequences are redundant or
        #        not.""", default=98.0)
        parser.add_argument('--remove_tblastn_hits_at_annotated_loci',
                help="""Ignore tblastn hits that overlap with any previously
                annotated loci. The rationale for this would be that the
                corresponding protein sequences should have been retrieved if
                the tblastn hit were a true positive anyway. If this option is
                not specified, then sequences will still be excluded if they
                specifically correspond to the same loci as do higher-ranking
                hits.""",
                action='store_true')
        parser.add_argument('--just_look_for_genes_in_gff3',
                help="""When looking for records in GFF3 annotation files that
                overlap with subsequences identified by similarity searching
                (TBLASTN), ignore records that are not explicitly "gene" (for
                example, "CDS", "mRNA", and "exon"). This option should
                probably not be selected, because in some GFF3 annotation files
                do not include "gene" records, but do include predicted coding
                sequences for genes.""",
                action='store_true')
        parser.add_argument('--ignore_gff3', help="""Disregard any information
                regarding redundancy of identified nucleotide sequences with
                identified protein sequences that may be found in GFF3
                annotation files.""",
                action='store_true')
        parser.add_argument('--allow_internal_stops', help="""Include sequences
        that have internal stop codons (anywhere other than the N-terminal
        position).""", default=True)
        parser.add_argument('--min_length', help="""Absolute minimum length (in
                AA) of a hit sequence to be considered a potential distinct
                paralogue.""", default=55)
        parser.add_argument('--min_percent_length', help="""Minimum length (in
                AA) of a hit sequence as a percentage of query length for the
                hit to be considered a potential distinct paralogue.""",
                default=15)
        parser.add_argument('--min_percent_query_cover', help="""Minimum number
        of residues aligning with the original query as a percentage of the
        original query sequence length.""",
                default=0)
        parser.add_argument('--overlap_required', help="""True if hits
                must overlap with a higher-ranking hit to be considered
                potential unique paralogues.""", action='store_true')
        parser.add_argument('--max_percent_ident', help="""Maximum percent
        identity (among aligning residues) for evaluating whether two sequences
        are redundant or not (secondary hits showing a percent identity with a
        higher-ranking hit exceeding this value will be excluded).""",
        default=98.0)
        parser.add_argument('--min_alig_res_in_overlap', help="""Minimum number
        of residues which must align for two sequences to be considered as
        potentially distinct hits. This is only relevant if the
        overlap_required option is specified.""",
                default=20)
        parser.add_argument('--min_ident_res_in_overlap', help="""Minimum
        number of aligning residues which must be identical for two sequences
        to be considered as potentially distinct hits. This is only relevant if
        the overlap_required option is specified.""",
                default=10)
        parser.add_argument('--min_sim_res_in_overlap', help="""Minimum number
        of aligning residues which must be similar for two sequences to be
        considered as potentially distinct hits. This is only relevant if the
        overlap_required option is specified.""",
                default=15)
        parser.add_argument('--min_ident_span_len', help="""Minimum number of
        aligning residues which are identical that must exist in at least one
        continuous span for two sequences to be considered as potentially
        distinct hits (not counting positions where both sequences have gaps).
        This is only relevant if the overlap_required option is specified.""",
        default=0)
        parser.add_argument('--min_sim_span_len', help="""Minimum number of
        aligning residues which are similar (or identical) that must exist in
        at least one continuous span for two sequences to be considered as
        potentially distinct hits (not counting positions where both sequences
        have gaps).  This is only relevant if the overlap_required option is
        specified.""", default=0)
        parser.add_argument('--min_percent_ident_in_overlap', help="""Minimum
        percent identity between the two sequences of interest in the
        alignment.This is only relevant if the overlap_required option is
        specified.""", default=0)
        parser.add_argument('--min_percent_sim_in_overlap', help="""Minimum
        percent similarity (including identity) between the two sequences of
        interest in the alignment.This is only relevant if the overlap_required
        option is specified.""", default=0)
        parser.add_argument('--min_percent_overlap', help="""Minimum number of
        aligning residues between the two sequences of interest as a percentage
        of the length of the second sequence (the last sequence in the
        alignment), not including gaps, for the two sequences to be considered
        as potentially distinct hits.  This is only relevant if the
        overlap_required option is specified.""", default=0)
        parser.add_argument('--add_ali_col', help="""Add a column to the csv
        file listing which alignment file in the queries directory to use for
        comparing sequences. Aligned FASTA queries are selected that match the
        query titles of the original queries used to retrieve each of the
        relevant hits listed in the csv file. No other options need to be
        specified in this case.""", action='store_true')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_paralogue_counter import count_paralogues3, add_alignment_column

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        ## Define output file path. 
        #outfp = None
        #if args.out_csv_path == None:
        #    outfp = args.csv_file.rsplit('.', 1)[0] + '_redun_seqs_' + timestamp + '.csv'
        #else:
        #    outfp = args.out_csv_path
        outfp = args.out_csv_path
        final_outfp = None

        # Define parameters.
        alignmentdir = settings.querydirpath

        if not args.add_ali_col: 
            # Write output file with interpetation column appended.
            final_outfp = count_paralogues3(args.csv_file,
                                            #args.metric_name,
                                            #args.metric_value_minimum,
                                            args.max_percent_ident,
                                            timestamp,
                                            args.overlap_required,
                                            args.allow_internal_stops,
                                            args.min_length,
                                            args.min_percent_length,
                                            args.min_percent_query_cover,
                                            args.remove_tblastn_hits_at_annotated_loci,
                                            args.min_alig_res_in_overlap,
                                            args.min_ident_res_in_overlap,
                                            args.min_sim_res_in_overlap,
                                            args.min_ident_span_len,
                                            args.min_sim_span_len,
                                            args.min_percent_ident_in_overlap,
                                            args.min_percent_sim_in_overlap,
                                            args.min_percent_overlap,
                                            args.just_look_for_genes_in_gff3,
                                            args.ignore_gff3,
                                            outfp)
        else:
            # Write output file listing which alignment to use.
            final_outfp = add_alignment_column(args.csv_file, outfp)

        # Prompt manual inspection of spreadsheet.
        print("""\n\nResults written/appended to
                spreadsheet:\n\n\t%s\n""" %\
                final_outfp)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.csv_file)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, final_outfp)


    def add_to_models(self):
        parser = argparse.ArgumentParser(
            description='''Add a phylogenetic model for relationships between
            members of a gene family (sequence_data matrix, data type, tree
            topology, type sequence defining each clade of interest, and
            substitution model) to a directory for use in classifying sequence
            (via the 'phylo_class' command.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        #parser.add_argument('model_dir', help='''Path to directory to which a
        #reformatted version of the input files is to be added.''')
        #parser.add_argument('csv_file', help='''Path to spreadsheet to add
        #information about added files to (CSV format).''')
        parser.add_argument('model_name', help='''An arbitrary name for the
                model (which will refer to the alignment, tree, substitution
                model, etc. collectively).''')
        parser.add_argument('alignment', help='''A multiple amino acid sequence
                alignment in nexus format.''')
        parser.add_argument('tree_topology', help='''Text file containing a
                tree (identified previously using MrBayes, etc) containing the
                names of all the sequences in the alignment, in newick
                format.''')
        parser.add_argument('subs_model', help='''The name of the substitution
                model used to recover the provided topology (chosen with
                ModelFinder or similar software).''')
        parser.add_argument('type_seqs', help='''Names of sequences (sequence
                headers) that are to be used to define clades of interest. A
                csv file with seq names in one column and clade names in the
                next column.''')
        parser.add_argument('taxon', help='''Taxonomic group represented in the
                model (e.g., "Eukaryotes", or "Amorphea").''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        import module_add_to_models

        # Parse input files to verify formatting.
        assert args.alignment.endswith('.nex'), """Input alignment must be in
        nexus format (.nex)."""

        # Check that the appropriate directory and info file exist, and are
        # specified in the settings module.
        assert os.path.isdir(settings.model_dir_path), """Models directory does
        not exist as specified in the settings module."""
        assert os.path.isfile(settings.model_info_csv), """Models information
        file does not exist as specified in the settings module."""

        # Write input files to new path.
        alignmentfp = os.path.join(settings.model_dir_path,
                args.model_name + '_' + os.path.basename(args.alignment))
        assert not os.path.isfile(alignmentfp), """File already exists: %s""" % alignmentfp
        shutil.copyfile(args.alignment, alignmentfp)

        topologyfp = os.path.join(settings.model_dir_path,
                args.model_name + '_' + os.path.basename(args.tree_topology))
        assert not os.path.isfile(topologyfp), """File already exists: %s""" % topologyfp
        shutil.copyfile(args.tree_topology, topologyfp)

        type_seqsfp = os.path.join(settings.model_dir_path,
                args.model_name + '_' + os.path.basename(args.type_seqs))
        assert not os.path.isfile(type_seqsfp), """File already exists: %s""" % type_seqsfp
        shutil.copyfile(args.type_seqs, type_seqsfp)

        # Add corresponding line to spreadsheet.
        module_add_to_models.update_models_csv(args.model_name,
                settings.model_info_csv, alignmentfp, topologyfp, args.subs_model,
                type_seqsfp, args.taxon)


    def list_models(self):
        parser = argparse.ArgumentParser(
            description='''Print a list of all usable model/reference tree
            names in the models directory as defined in the settings file.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        args = parser.parse_args(sys.argv[2:])

        # Get current time for timestamp.
        #timestamp = time.strftime("%Y_%m_%d_%H_%M_%S")

        # Get path to models directory from settings.py file.
        model_dir = settings.model_dir_path

        # Get all files in models directory.
        all_files = glob.glob(os.path.join(model_dir, '*'))

        # Should actually look in CSV file...
        assert 2 != 2

        # Define regular expression for identifying usable query file
        # paths.
        usable_model = re.compile(r'.+\.faa$|.+\.fna$|.+\.hmmdb$')

        # Get list of files that can be used as databases.
        db_file_list = []
        for f in all_files:
            if usable_db.search(f):
                db_file_list.append(f)

        # Check that at least one usable query file was identified.
        assert len(db_file_list) > 0, """Error: No usable database files
        could be identified in database directory: %s""" % db_dir

        # Sort database file list.
        db_file_list.sort()

        # Print database file list.
        for i in db_file_list:
            print(os.path.basename(i))


    def phylo_class(self):
        parser = argparse.ArgumentParser(
            description='''Determine whether sequences can be classified into
            one of several pre-defined orthogroups using an established
            backbone phylogeny for topology testing with IQtree.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('file_with_seqs', help='''Path to a csv file
        listing sequences to classify, or a fasta file with sequences to
        classify.''')
        parser.add_argument('--model_name', help='''The name for the model listed
        in the csv file (which will refers collectively to the alignment, tree,
        substitution model, etc. that you want to use). Only specify if input
        file with sequences is a fasta file.''')
        parser.add_argument('--place', help='''Run a ML search to place
        each sequence (anywhere) in a constrained tree. ***NOTE: NOT FUNCTIONAL
        YET.''', action='store_true')
        parser.add_argument('--add_model_name_col', help='''Just add a column
        to contain names of the model backbone tree to use for each relevant
        sequence/row.''', action='store_true')
        #parser.add_argument('csv_file', help='''Path to csv spreadsheet with
        #similarity search hits to be classified using phylogenetic
        #analysis.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_phylo_clas import classify_seq_with_constrained_tree2

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        outfile = None
        if args.add_model_name_col:
            # Add column to input file.

            # ********Need to make this automatically figure out which model to
            # use?
            #add_model_column(args.file_with_seqs)

            # Temporary code:
            tempfile = args.file_with_seqs + '_TEMP.csv'
            newfile = args.file_with_seqs.rsplit('.', 1)[0] + '_with_model_col.csv'
            with open(args.file_with_seqs) as infh, open(tempfile, 'w') as o:
                inum = 0
                for i in infh:
                    inum += 1
                    if inum == 1:
                        o.write(i.strip() + ',' + 'Model/backbone tree name' +\
                                '\n')
                    else:
                        o.write(i.strip() + ',\n')
            with open(tempfile) as infh, open(newfile, 'w') as o:
                for i in infh:
                    o.write(i)
            os.remove(tempfile)
            outfile = newfile

        else:
            # Call module to run trees, etc.

            #classify_seq_with_constrained_tree(alignmentfp, tree_topologyfp, subs_model,
            #        type_seqsfp, args.fasta)
            #classify_seq_with_constrained_tree2(alignmentfp, tree_topologyfp, subs_model,
            #        type_seqsfp, args.fasta)
            outfile = classify_seq_with_constrained_tree2(args.file_with_seqs,\
                    args.model_name, False, True, args.place)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.file_with_seqs)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, outfile)


    def sum_phylo_class(self):
        parser = argparse.ArgumentParser(
            description='''Add phylo_class results to summary spreadsheet.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('csv_file', help='''Path to summary spreadsheet
                (CSV) file, which may already contain phylo_class summaries.''')
        parser.add_argument('phylo_class_out', help='''Path to directory where
                phylo_class results were written.''')
        #parser.add_argument('--max_pvalue', help='''Maximum p-value for
        #        classifying a given sequence into one of several clades in the
        #        model backbone tree.''', default=0.05)
        parser.add_argument('--minimum_confidence', help='''Minimum
                confidence that a classification needs to be for counting.''',
                default=0.95)
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_phylo_clas import get_phylo_class_csv
        from module_amoebae_phylo_clas import write_phylo_class_to_csv

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Get timestamp of phylo_class results.
        phylo_class_timestamp = '_'.join(args.phylo_class_out.split('_')[-3:]) 

        # Get phylo_class full summary csv file path.
        out_phylo_class_file =\
        get_phylo_class_csv(args.phylo_class_out)
        assert os.path.isfile(out_phylo_class_file), """Could not find full
        summary csv file: %s""" % out_phylo_class_file

        ## Get maximum p-value to use.  #if not args.min_evaldiff is None:
        #    min_evaldiff = args.min_evaldiff
        #else:
        #    # By default use the one in the settings file.
        #    min_evaldiff = settings.min_evaldiff

        #final_outfp = write_phylo_class_to_csv(phylo_class_timestamp,
        #        args.phylo_class_out, args.csv_file, args.max_pvalue, timestamp)
        final_outfp = write_phylo_class_to_csv(phylo_class_timestamp,
                args.phylo_class_out, args.csv_file,
                float(args.minimum_confidence), timestamp)


        ## Prompt manual inspection of spreadsheet.
        #print("""\n\nPhylogenetic classifications written/appended to
        #        spreadsheet:\n\n\t%s\n""" %\
        #        get_csv_with_phylo_class_sum(args.csv_file)) 

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.csv_file)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, final_outfp)


    def interp_phylo(self): 
        parser = argparse.ArgumentParser(
            description='''Interpret phylogenetic classification results.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('csv_file', help='''Path to spreadsheet with
        forward and reverse search results.''')
        parser.add_argument('--out_csv_path', help='''Optionally specify an
                output file path, so that this command can be piped together
                with others.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        #import module_amoebae_...
        print('This command does not work yet.')
        assert 2 != 2

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Define output file path. 
        outfp = None
        if args.out_csv_path == None:
            outfp = args.csv_file.rsplit('.', 1)[0] + '_interp_' + timestamp + '.csv'
        else:
            outfp = args.out_csv_path

        # Write output file with interpetation column appended.
        #module_amoebae_search.write_redun_hit_interp_csv(args.csv_file, outfp)

        # Prompt manual inspection of spreadsheet.
        print("""\n\nInterpretations written/appended to
                spreadsheet:\n\n\t%s\n""" %\
                outfp)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.csv_file)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, outfp)


    def plot(self):
        parser = argparse.ArgumentParser(
            description='''Plot results of similarity search and sequence
            classification analyses. The outputs are PDF files.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('csv_file', help='''Path to a spreadsheet with the
                relevant results to be plotted. This can be either a CSV file output
                of the sum_rev_srch command or from the find_redun_seqs
                command. If the output of the sum_rev_srch command is used,
                however, redundant hits will be counted (e.g., BLASTP and
                TBLASTN hits corresponding to the same or highly identical genomic loci).''')
        parser.add_argument('--csv_file2', help='''Path to a second spreadsheet
                with relevant results to be compared to the first and plotted.''',
                default=None)
        parser.add_argument('--complex_info', help='''Path to file that
                specifies which query titles represent components of which
                protein complexes (or otherwise grouped proteins).''',
                default=None)
        parser.add_argument('--row_order', help='''Path to file that
                specifies the order in which data for each species will be
                displayed.''',
                default=None)
        parser.add_argument('--out_pdf', help='''Path to output pdf file.''',
                default=None)
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_plot import plot_amoebae_res

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        print('Running plot_amoebae_res function')
        main_out_path = plot_amoebae_res(args.csv_file, args.complex_info,
                args.out_pdf, args.csv_file2, args.row_order)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.csv_file)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def csv_to_fasta(self):
        parser = argparse.ArgumentParser(
            description="""Extract sequences described in a spreadsheet output by
            AMOEBAE, and write to a file in FASTA format.""",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('csv_file', help='''Path to csv file listing
                sequences.''')
        parser.add_argument('--output_dir', help='''Path for output directory
        to contain FASTA files.''', default=None)
        parser.add_argument('--abbrev', help='''Add species name instead of
                sequence description from fasta header. Applicable when the
                output file is to be used for alignment and phylogenetic
                analysis.''', action='store_true')
        parser.add_argument('--paralogue_names', help='''Use species name,
        query title, and paralogue number instead of sequence description from
        fasta header. Applicable when the output file is to be used for
        alignment and phylogenetic analysis. Does not work if the abbrev option
        is specified.''', action='store_true')
        parser.add_argument('--only_descr', help='''Use the description but not
        the ID as the new fasta sequence header. Does not work if the abbrev
        option is specified.''', action='store_true')
        parser.add_argument('--subseq', help='''Write subsequences that aligned
                to forward search query, instead of the full sequences.''', action='store_true')
        parser.add_argument('--all_hits', help='''Write all forward hits
                listed in the input csv file.''', action='store_true')
        parser.add_argument('--split_by_query_title', help='''Write sequences
                to files according to the query title of the query which
                retrieved them in a similarity search.''', action='store_true')
        parser.add_argument('--split_by_top_rev_srch_hit', help='''Write
                sequences to files according to the top hit that they retrieve
                in a reverse search, for each sequence that meets the reverse
                search criteria. (Provide the reverse search identifier, eg,
                "rev_srch_20180924122402-1")''', default=None)
        args = parser.parse_args(sys.argv[2:])

        output_dir = None
        if args.output_dir is None:
           #output_fasta = args.csv_file.rsplit('.', 1)[0] + '.fa' 
           output_dir = args.csv_file.rsplit('.', 1)[0]

        module_amoebae.write_seqs_to_fasta(args.csv_file, output_dir,
                args.abbrev, args.paralogue_names, args.only_descr, args.subseq, args.all_hits,
                args.split_by_top_rev_srch_hit, args.split_by_query_title)


    def prune(self):
        parser = argparse.ArgumentParser(
            description='''Identify sequences in a tree, and remove them from a
            given alignment for further phylogenetic analysis.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('tree_file', help='''Tree in newick format (coded
                names, because ETE3 cannot parse taxon names with space
                characters without quotation marks around them).''')
        parser.add_argument('alignment', help='''Dataset used to make the tree
                (nexus alignment) (original alignment with original taxon names
                either trimmed or untrimmed).''')
        parser.add_argument('name_replace_table', help='''File for decoding
                names in input tree file.''')
        parser.add_argument('--include_seqs', help='''Include only listed
                sequences/nodes instead of removing them.''',
                action='store_true')
        parser.add_argument('--output_file', help='''Path to output file.''')
        #parser.add_argument('--optional_argument_name', help='''optional
        #argument help.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_prune import manually_select_nodes_and_remove_seqs

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")


        # Call function from module.
        main_out_path = manually_select_nodes_and_remove_seqs(args.tree_file, args.alignment,
                args.output_file, args.name_replace_table, args.include_seqs)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.tree_file)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)

    def auto_prune(self):
        parser = argparse.ArgumentParser(
            description='''Automatically identify sequences in a tree, and
            remove them from a given alignment for further phylogenetic
            analysis.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        #parser.add_argument('tree_file', help='''Tree in newick format (coded
        #        names, because ETE3 cannot parse taxon names with space
        #        characters without quotation marks around them).''')
        #parser.add_argument('alignment', help='''Dataset used to make the tree
        #        (nexus alignment) (original alignment with original taxon names
        #        either trimmed or untrimmed).''')
        #parser.add_argument('name_replace_table', help='''File for decoding
        #        names in input tree file.''')
        parser.add_argument('in_dir', help='''Path to directory that contains
                the phylogenetic analysis output files (sequence name
                conversion table file and original nexus alignment file can be
                in the parent directory to this directory as long as their
                names are mostly identical.''')
        parser.add_argument('--max_bl_iqr_above_third_quartile',
        help='''Inclusion threshold for number of interquartile ranges above
        the third quartile of terminal branch lengths the length of a terminal
        branch can be before it is considered an outlier (length is total
        distance from root node after rooting on midpoint, or the longest
        terminal branch on either side of the midpoint).''', default=1.5)
        parser.add_argument('--remove_redun_seqs', help='''Remove taxonomically
                redundant sequences (longest branch of two sister branches when
                both are sequences from the same species.''',
                default=True)
        parser.add_argument('--remove_redun_seqs_threshold', help='''Minimum
        support required to consider one of two sister branches/sequences
        taxonomically redundant. Note: only used if the remove_redun_seqs
        option is specified.''', default=0.95)
        parser.add_argument('--output_file', help='''Path to output file.''')
        #parser.add_argument('--optional_argument_name', help='''optional
        #argument help.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        #from module_amoebae_prune import automatically_select_nodes_and_remove_seqs
        from module_amoebae_prune import automatically_select_nodes_and_remove_seqs_in_dir

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")


        # Call function from module.
        #main_out_path = automatically_select_nodes_and_remove_seqs(args.tree_file,
        #                                                           args.alignment,
        #                                                           args.max_bl_iqr_above_third_quartile,
        #                                                           args.remove_redun_seqs,
        #                                                           args.remove_redun_seqs_threshold,
        #                                                           args.output_file,
        #                                                           args.name_replace_table
        #                                                           )
        main_out_path =\
        automatically_select_nodes_and_remove_seqs_in_dir(args.in_dir,
                                                          args.max_bl_iqr_above_third_quartile,
                                                          args.remove_redun_seqs,
                                                          args.remove_redun_seqs_threshold,
                                                          args.output_file
                                                          )

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        #outdir = os.path.dirname(args.tree_file)
        outdir = args.in_dir
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def reduce_tree(self):
        parser = argparse.ArgumentParser(
            description='''Remove terminal nodes from a given tree if there are
            not any sequences with the same name in a given alignment.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('alignment', help='''Alignment in nexus format with
                sequences representing a subset of those represented in the
                input tree.''')
        parser.add_argument('tree_file', help='''Tree in newick format.''')
        parser.add_argument('--output_file', help='''Path to output file.''',
                default=None)
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_prune import remove_nodes_to_match_ali

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Call function from module.
        main_out_path = remove_nodes_to_match_ali(args.alignment, args.tree_file, 
                args.output_file)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.tree_file)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def setup_topo_constraint(self):
        parser = argparse.ArgumentParser(
            description='''Takes a tree topology, prompts manual input, and
            outputs a tree to use as a topology constraint.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('tree_file', help='''Tree in newick format (coded
                names, because ETE3 cannot parse taxon names with space
                characters without quotation marks around them).''')
        #parser.add_argument('alignment', help='''Dataset used to make the tree
        #        (nexus alignment) (original sequence names, can be trimmed or
        #        untrimmed).''')
        parser.add_argument('name_replace_table', help='''File for decoding
                names in input tree file.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_prune import manually_select_nodes_to_constrain

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Call function from module.
        main_out_path = manually_select_nodes_to_constrain(args.tree_file, args.name_replace_table)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.tree_file)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    # OBSOLETE
    def polyt_backbone(self):
        parser = argparse.ArgumentParser(
            description='''Take a tree and make internal branches/nodes outside
            specified clades of interest into a polytomy.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('model_name', help='''Name of model/backbone tree
                to modify (other info provided in the model info csv file).''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_phylo_clas import polyt_model_backbone

        ## Record start time.
        #start_time = time.time()
        ## Get current time for timestamp.
        #timestamp = time.strftime("%Y%m%d%H%M%S")


        polyt_model_backbone(args.model_name)

        ## Record end time.
        #end_time = time.time()
        ## Define command line input.
        #commandline = ' '.join(sys.argv)
        ## Define output directory.
        #outdir = os.path.dirname(args.tree_file)
        ## Record info to log file.
        #module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
        #        start_time, end_time, timestamp, main_out_path)


    def get_alt_topos(self):
        parser = argparse.ArgumentParser(
            description='''Take a tree and make copies with every alternative
            topology for the branches connecting the clades of interest. Output
            as additional models in the Models directory.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('model_name', help='''Name of model/backbone tree
                to modify (other info provided in the model info csv file).''')
        parser.add_argument('out_dir_path', help='''Path to directory in which
                output directory will be written.''')
        parser.add_argument('--polytomy', help='''Just make one big polytomy
                connecting the clades of interest intead of making alternative
                bifurcating trees.''',
                action='store_true')
        parser.add_argument('--not_polytomy_clades', help='''Do not make
        subtrees/clades of interest polytomies in output topologies.''',
        action='store_true')
        parser.add_argument('--keep_original_backbone', help='''Keep the
                original backbone topology instead of generating a polytomy or
                alternative resolved topologies.''',
        action='store_true')
        parser.add_argument('--iqtree_au_test', help='''Test all the relevant
                alternative topologies against each other using Approximately
                Unbiased (AU) test with IQ-tree.''',
                action='store_true')
        # This option removed because should always do this anyway:
        #parser.add_argument('--polytomy_clades', help='''Make subtrees/clades of
        #        interest polytomies in output topologies.''',
        #        action='store_true')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_phylo_clas import get_all_alt_model_backbones

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Define output directory path.
        main_out_path = os.path.join(args.out_dir_path, 'get_alt_topos_output_' + timestamp)

        # Call function from the module.
        #get_all_alt_model_backbones(args.model_name, args.out_dir_path,
        #        main_out_path, args.polytomy,
        #        args.polytomy_clades)
        get_all_alt_model_backbones(args.model_name, args.out_dir_path,
                main_out_path, args.polytomy, args.not_polytomy_clades,
                args.keep_original_backbone, args.iqtree_au_test)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = main_out_path
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def constrain_mb(self):
        parser = argparse.ArgumentParser(
            description='''Add constraint commands to MrBayes input file.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('alignment', help='''Nexus alignment for input to
        Mrbayes (without any constraint commands).''')
        parser.add_argument('tree', help='''Tree in newick format with same
        taxon names as in alignment. To be used as a topology constraint (all
        nodes).''')
        parser.add_argument('--out_alignment', help='''Path to nexus alignment
        for input to Mrbayes with constraints added.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_constrain_mb import constrain_mb_with_tree

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        main_out_path = constrain_mb_with_tree(args.alignment, args.tree, args.out_alignment)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.alignment)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)

        
    def select_seqs(self):
        parser = argparse.ArgumentParser(
            description='''Perform an ML search with IQtree to find the most
            likely position of each sequence in a previously established
            constrained backbone tree.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('file_with_seqs', help='''Path to a csv file
        listing sequences to classify, or a fasta file with sequences to
        classify.''')
        parser.add_argument('model_name', help='''The name for the model listed
        in the csv file (which refers collectively to the alignment, tree,
        substitution model, etc. that you want to use). Only specify if input
        file with sequences is a fasta file.''')
        parser.add_argument('essential_taxa_file', help='''Path to file
        containing a list of essential taxa which should ideally be represented
        in each clade of interest in the tree.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_select_seqs import optimize_sequence_selection2

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Get list of taxa from file.
        essential_taxa = []
        with open(args.essential_taxa_file) as infh:
            for i in infh:
                if not i.startswith('#') and not i.startswith('\n'):
                    taxon = i.strip()
                    essential_taxa.append(taxon)

        # Call module to run trees, etc.
        #main_out_path = optimize_sequence_selection(args.file_with_seqs, args.model_name,
        #        essential_taxa)
        main_out_path = optimize_sequence_selection2(args.file_with_seqs, args.model_name,
                essential_taxa, timestamp)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.file_with_seqs)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def select_positions(self):
        parser = argparse.ArgumentParser(
            description='''Systematically remove position/columns from an input
            alignment for a tree to find the mask that results in the best
            resolution/backbone support for the tree. Note: this should be done
            for multiple alternative topologies to avoid cherry-picking.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('model_name', help='''The name for the model listed
        in the csv file (which refers collectively to the alignment, tree,
        substitution model, etc. that you want to use).''')
        parser.add_argument('outdirpath', help='''Path to directory that output
        files should be written to.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_amoebae_select_positions import optimize_position_selection

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Call module to run trees, etc.
        main_out_path = optimize_position_selection(args.model_name,
                args.outdirpath, timestamp)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.file_with_seqs)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def srch_ali_space(self):
        parser = argparse.ArgumentParser(
            description='''Iteratively modify an alignment/tree and assess
            measures of support for branches of interest to find alignments
            that support a given topology.''',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        parser.add_argument('model_name', help='''The name for the model listed
        in the csv file (which refers collectively to the alignment, tree,
        substitution model, etc. that you want to use). Only specify if input
        file with sequences is a fasta file.''')
        parser.add_argument('out_dir_path', help='''Path to directory in which
        output directory should be written.''')
        parser.add_argument('--mod_type', help='''Name of type of modification to
        apply iteratively to the alignment and tree in the search for
        alignments that result in better support for clades of interest.
        Options: remove_seqs, add_seqs, remove_columns, mixed, intensive.''',
        default='remove_seqs')
        parser.add_argument('--iterations', help='''Number of iterations of
        modification and assessment to do (by default as many iterations as
        possible will be done until no further modifications of the type
        specified improve the support).''')
        parser.add_argument('--file_with_seqs', help='''Path to a csv file
        listing sequences to classify, or a fasta file with sequences to
        classify.''')
        parser.add_argument('--essential_taxa_file', help='''Path to file
        containing a list of essential taxa which should ideally be represented
        in each clade of interest in the tree.''')
        parser.add_argument('--keep_all_output', help='''Keep all intermediate
        output files such as alignment files and .png images of trees
        instead of deleting them.''', action='store_true')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        #from module_amoebae_select_seqs import optimize_sequence_selection2
        #from module_amoebae_select_positions import optimize_position_selection
        from search_alignment_space import search_alignment_space

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Call module to run trees, etc.
        main_out_path = search_alignment_space(args.model_name,
                                               args.out_dir_path,
                                               args.mod_type,
                                               args.iterations,
                                               args.keep_all_output,
                                               timestamp,
                                               args.file_with_seqs, 
                                               args.essential_taxa_file)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline,
                                                       args.out_dir_path,
                                                       start_time,
                                                       end_time,
                                                       timestamp,
                                                       main_out_path)


    def visualize_tree(self):
        parser = argparse.ArgumentParser(
            description="""Parse phylogenetic analysis output files in a given
            directory, and write human-readable tree figures to PDF files.""")
        parser.add_argument('input_directory', help='''Path to directory
                containing input files (must contain a .table file for decoding
                taxon names.''')
        parser.add_argument('method', help='''Name of tree searching program
                used. Either iqtree, raxml, or mrbayes accepted.''')
        parser.add_argument('--root_taxon', help='''Name of species to root on
                (e.g., "Klebsormidium_nitens").''')
        parser.add_argument('--highlight_paralogues', help='''Highlight clades
                that contain paralogues found in at least one other clade in
                the tree.''', action='store_true')
        parser.add_argument('--add_clade_names_from_file', help='''Use a file
        in the parent directory with clade names corresponding to
        representative sequences to add clade names to all the taxon names in
        the output trees.''', action='store_true')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from visualize_trees import visualize_tree_in_dir

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Define a list of taxa to root the tree on.
        taxa_to_root_on = []
        if args.root_taxon is not None:
            taxa_to_root_on = [args.root_taxon] # Figure out how to take multiple from commandline later.

        # Call function from module to make pdfs.
        output_files = visualize_tree_in_dir(args.input_directory, args.method,
                                             timestamp, taxa_to_root_on,
                                             args.highlight_paralogues,
                                             args.add_clade_names_from_file)

        # Define main output file path.
        main_out_path = output_files[0]

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = args.input_directory
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def replace_seqs(self):
        parser = argparse.ArgumentParser(
            description='''Replace sequences in an alignment the full-length
            sequences from the relevant file(s) in the Genomes directory, or
            with their top hits in a given fasta file. And, align, mask, and
            trim the identified sequences to the input alignment''')
        parser.add_argument('alignment', help='''Path to multiple sequence
        alignment file in nexus format (trimmed alignment).''')
        parser.add_argument('--fasta_file', help='''Path to file containing
        sequences with which to replace sequences in the alignment. If this
        option is not specified, then full-length sequences will be retrieved
        from files in the Genomes directory.''')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from replace_seqs import replace_seqs_in_alignment_with_seqs_from_fasta

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Do stuff.
        main_out_path =\
        replace_seqs_in_alignment_with_seqs_from_fasta(args.alignment,
                args.fasta_file)


        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.alignment)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def align_fa(self):
        parser = argparse.ArgumentParser(
            description="""This command is for calling alignment software
            (MUSCLE) to align FASTA files and save them in nexus format.  This
            assumes that you want to use default settings for MUSCLE. By
            default, the output file is written to the same directory as the
            input file.""")
        parser.add_argument('input_file_path', help='''Path to a multiple
        peptide sequence file in FASTA format.''')
        parser.add_argument('--output_file_path', help='''Path to output
        multiple sequence alignment file.''',
        default=None)
        parser.add_argument('--output_format', help='''Multiple sequence
        alignment format for output file. Valid options are 'fasta' or
        'nexus'.''', choices=['fasta','nexus'], default='nexus')
        args = parser.parse_args(sys.argv[2:])

        # Import command-specific modules.
        from module_afa_to_nex import align_one_fa

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Do stuff.

        # Set amino acid scoring matrix path to use if mafft program called
        # (default is settings.blosum62_path).
        #print('Matrix: ' + settings.align_fa_matrix)
        aamatrix_path = None
        if settings.align_fa_matrix == 'blosum62':
            aamatrix_path = settings.blosum62_path
        elif settings.align_fa_matrix == 'cc140':
            aamatrix_path = settings.cc140_path
        else:
            print('Error! matrix not specified.')
        #print('Matrix file: ' + aamatrix_path)

        # Set alignment program to use (default is 'muscle', use 'altmafft' if
        # experimenting with different parameters).
        program = settings.align_fa_program 
        #print('Program: ' + program + '\n')

        # Determine what format the output file should be in.
        conv_to_nexus_format = None
        if args.output_format == 'fasta':
            conv_to_nexus_format = False
        elif args.output_format == 'nexus':
            conv_to_nexus_format = True
        assert conv_to_nexus_format is not None, """Output format must be fasta
        or nexus."""

        # Align the fasta file.
        main_out_path = align_one_fa(args.input_file_path,
                                             args.output_file_path,
                                             program,
                                             aamatrix_path,
                                             conv_to_nexus_format)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output file/directory.
        outdir = main_out_path
        if not os.path.isdir(outdir):
            outdir = os.path.dirname(main_out_path)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    def afa_to_nex(self):
        parser = argparse.ArgumentParser(
            description="""Convert an alignment file in fasta format to nexus
            format.""")
        parser.add_argument('input_fasta_path', help='''Path to a multiple
                sequence alignment file in FASTA format.''')
        parser.add_argument('--output_nexus_path', help='''Path to output
                multiple sequence alignment file in nexus format.''',
                default=None)
        args = parser.parse_args(sys.argv[2:])

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")

        # Import command-specific modules.
        from module_afa_to_nex import afa_to_nex

        # Do stuff.
        main_out_path = None
        if args.output_nexus_path is None:
            main_out_path = args.input_fasta_path.rsplit('.', 1)[0] + '.nex'
        else:
            main_out_path = args.output_nexus_path
        assert main_out_path is not None
        afa_to_nex(args.input_fasta_path, main_out_path)

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(main_out_path)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


    # TEMPLATE FUNCTION:
    def commandx(self):
        parser = argparse.ArgumentParser(
            description='Do commandx.')
        parser.add_argument('positional_argument_name', help='''positional
        argument help.''')
        parser.add_argument('--optional_argument_name', help='''optional
        argument help.''')
        args = parser.parse_args(sys.argv[2:])

        # Record start time.
        start_time = time.time()
        # Get current time for timestamp.
        timestamp = time.strftime("%Y%m%d%H%M%S")


        # Do stuff.
        print('Running commandx')

        # Record end time.
        end_time = time.time()
        # Define command line input.
        commandline = ' '.join(sys.argv)
        # Define output directory.
        outdir = os.path.dirname(args.file_with_seqs)
        # Record info to log file.
        module_amoebae.record_amoebae_info_in_log_file(commandline, outdir,
                start_time, end_time, timestamp, main_out_path)


if __name__ == '__main__':
    DispatchAmoebae()

