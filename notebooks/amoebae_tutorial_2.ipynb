{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Warning: This tutorial is currently under development.</span>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This tutorial will walk you through a preliminary similarity searching analysis making use of scripts in the AMOEBAE toolkit. As a simple example, we will consider the the distribution of orthologues of subunits of the Adaptor Protein (AP) 2 vesicle adaptor complex, and several other membrane-trafficking proteins, in three model eukaryotes: the plant *Arabidopsis thaliana*, the yeast *Saccharomyces cerevisiae*, the fungus *Allomyces macrogynus*, the amoeba *Dictyostelium discoideum*, and the pathogenic protist *Trypanosoma brucei*. AP-2 subunits are homologous to subunits of other AP complexes (Robinson, 2004; Hirst et al., 2011), and published work has traced their evolution among plants (Larson et al., 2019), Fungi (Barlow et al., 2014), and trypanosomatid parasites (Manna et al., 2013). Thus, the protein subunits of the AP-2 complex provide a useful test of similarity searching methods to distinguish between orthologues and paralogues, which can be compared to the results of previous comprehensive studies. The membrane trafficking proteins Sec12 (a component of the COPII vesicle coat complex), SNAP33 (a Qbc-SNARE), and Rab2 (a small GTPase) are included to further explore the potential sources of error involved in identification of orthologous proteins. The end result of running this code successfully is a spreadsheet summarizing results of similarity searches, as well as a plot summarizing the results.\n",
    "\n",
    "While AMOEBAE was not originally written to be used via the command line, Jupyter notebooks provide an easy means of guiding new users through an example analysis with limited need for manual input.\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "\n",
    "-  Perform similarity searches using the BLASTP, TBLASN, HMMer algorithms simultaneously using AMOBEAE code.\n",
    "\n",
    "-  Apply a reciprocal-best-hit search strategy using AMOEBAE code.\n",
    "\n",
    "- Practice interpreting interesting similarity search results obtained using AMOEBAE.\n",
    " \n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- MacOS or Linux operating system (or possibly a work-around on windows, although this has not been tested).\n",
    "\n",
    "- Before running this code, you will need to have set up AMOEBAE according to the instructions in the main documentation file.\n",
    "\n",
    "- The code in this notebook will take approximately <span style=\"color:red\">XXXXXX</span> minutes to run.\n",
    "\n",
    "## Testing\n",
    "If you wish to simply run all the code in this notebook for testing purposes, there are two option:\n",
    "\n",
    "- Select Cell > Run All from the menu above.\n",
    "\n",
    "- Alternatively, close this browser window, navigate to the directory in the container in which this notebook runs, and use the runipy program to run the notebook as follows:\n",
    "    \n",
    "    runipy -o amoebae_tutorial_2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record the specific version of AMOEBAE code used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record git repository version information.\n",
    "script_dir = os.path.dirname(os.path.realpath(__file__)) \n",
    "git_hash = str(subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=script_dir).strip())\n",
    "git_branch = str(subprocess.check_output([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=script_dir).strip())  \n",
    "print('Git repository (code) version: ' + git_hash + ' (branch name: ' + git_branch + ')\\n\\n')\n",
    "\n",
    "# Record system information.\n",
    "print('System info: ' + str(platform.uname()) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that dependencies are installed\n",
    "You should have already pulled the amoebae git repository to your computer as described in the main documentation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BLASTP version:\n",
      "blastp: 2.10.0+\n",
      "\n",
      "\n",
      "HMMer version:\n",
      "# hmmsearch :: search profile(s) against a sequence database\n",
      "# HMMER 3.3 (Nov 2019); http://hmmer.org/\n",
      "# Copyright (C) 2019 Howard Hughes Medical Institute.\n",
      "# Freely distributed under the BSD open source license.\n",
      "\n",
      "\n",
      "HMMer esl-fetch utilities:\n",
      "# esl-sfetch :: retrieve sequence(s) from a file\n",
      "# Easel 0.46 (Nov 2019)\n",
      "# Copyright (C) 2019 Howard Hughes Medical Institute.\n",
      "# Freely distributed under the BSD open source license.\n",
      "\n",
      "\n",
      "MUSCLE version:\n",
      "MUSCLE v3.8.31 by Robert C. Edgar\n",
      "\n",
      "\n",
      "IQ-TREE version:\n",
      "IQ-TREE multicore version 1.6.12 for Linux 64-bit built Aug 15 2019\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "amoebae check_depend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-redundant list of import statements:\n",
      "\n",
      "1. import sys  # add_seq_man.py\n",
      "2. import os  # add_seq_man.py\n",
      "3. import shutil  # add_seq_man.py\n",
      "4. import time  # add_seq_man.py\n",
      "5. from module_afa_to_nex import afa_to_nex, nex_to_afa  # add_seq_man.py\n",
      "6. from afa_to_fa import afa_to_fa  # add_seq_man.py\n",
      "7. from module_afa_to_nex import align_one_fa  # add_seq_man.py\n",
      "8. from subprocess import call  # add_seq_man.py\n",
      "9. from parse_mod_num import update_mod_num_numeric  # add_seq_man.py\n",
      "10. import subprocess  # boots_on_best_ml.py\n",
      "11. import glob  # boots_on_best_ml.py\n",
      "12. import settings  # boots_on_best_ml.py\n",
      "13. from module_amoebae_name_replace import write_newick_tree_with_uncoded_names  # boots_on_best_ml.py\n",
      "14. import re  # boots_on_mb.py\n",
      "15. from ete3 import Tree  # boots_on_mb.py\n",
      "16. from settings import raxmlname  # boots_on_mb.py\n",
      "17. from module_boots_on_mb import reformat_combined_supports, combine_supports,\\  # boots_on_mb.py\n",
      "18. mbcontre_to_newick_w_probs, contre_to_newick  # boots_on_mb.py\n",
      "19. import math  # calculate_possible_seq_sets_to_tree.py\n",
      "20. import argparse  # run_rev_srch.py\n",
      "21. import module_amoebae_run_searches  # run_rev_srch.py\n",
      "22. from module_get_fas_from_db_dir import get_seq_obj_from_db_fasta  # nex_to_full-length_fa.py\n",
      "23. from Bio import AlignIO  # nex_to_full-length_fa.py\n",
      "24. import module_afa_to_nex  # nex_to_full-length_fa.py\n",
      "25. from Bio import SeqIO  # append_afas.py\n",
      "26. from Bio.Alphabet import IUPAC, Gapped  # append_afas.py\n",
      "27. from module_afa_to_nex import afa_to_nex  # append_afas.py\n",
      "28. import itertools  # nhmmer_scaffolds.py\n",
      "29. from Bio.Seq import Seq  # nhmmer_scaffolds.py\n",
      "30. from Bio.Alphabet import IUPAC  # nhmmer_scaffolds.py\n",
      "31. import module_nhmmer_scaffolds  # nhmmer_scaffolds.py\n",
      "32. from module_paralogue_counter import count_paralogues2  # paralogue_counter2.py\n",
      "33. from module_amoebae_trim_nex import trim_nex  # trim_nex.py\n",
      "34. from misc_functions import get_fa_record_text_from_obj  # add_only_new_fas.py\n",
      "35. from Bio.SeqIO.QualityIO import FastqGeneralIterator  # convert_fastq_to_fasta.py\n",
      "36. import random  # convert_fastq_to_fasta.py\n",
      "37. from module_amoebae_name_replace import write_newick_tree_with_coded_names  # code_newick.py\n",
      "38. from module_amoebae_name_replace import codenames_nex  # codenames_nex.py\n",
      "39. from datetime import datetime  # run_snare_superfam_topology_test.py\n",
      "40. from module_amoebae_get_datatype import get_dbtype  # get_datatype.py\n",
      "41. from module_afa_to_nex import nex_to_afa, afa_to_nex  # reduce_fasta.py\n",
      "42. from module_amoebae import get_seqs_from_fasta_db  # get_seqs_from_fasta.py\n",
      "43. from module_afa_to_nex import nex_to_afa, delete_extra_mesquite_lines  # nex_to_fa.py\n",
      "44. import getpass  # run_iqtree_on_cipres.py\n",
      "45. from time import sleep  # run_iqtree_on_cipres.py\n",
      "46. from string import Template  # run_iqtree_on_cipres.py\n",
      "47. from ete3 import Tree, TreeStyle, TextFace  # test_ete3_reroot.py\n",
      "48. from module_afa_to_nex import nex_to_afa, afa_to_nex, delete_extra_mesquite_lines  # re-order_taxa_nex.py\n",
      "49. from parse_mod_num import update_mod_num_alphabetic  # rm_seq_man.py\n",
      "50. from misc_functions import launch, query_yes_no  # rm_seq_man.py\n",
      "51. from realign_nex import realign_nex  # rm_seq_man.py\n",
      "52. from module_align_to_profile_iter import do_align_iteratively  # align_to_profile_iter.py\n",
      "53. from module_amoebae_nex_to_hmm import nex_to_hmm  # nex_to_hmm.py\n",
      "54. import copy  # rough_translation.py\n",
      "55. from misc_functions import get_abbrev_fa_record_text_from_obj  # interpret_ncoils_fasta_output.py\n",
      "56. from PyPDF2 import PdfFileWriter, PdfFileReader  # add_filenames_to_pdfs.py\n",
      "57. import io  # add_filenames_to_pdfs.py\n",
      "58. from reportlab.pdfgen import canvas  # add_filenames_to_pdfs.py\n",
      "59. from reportlab.lib.pagesizes import letter  # add_filenames_to_pdfs.py\n",
      "60. import datetime  # find_human_contam_in_plasmo_reads.py\n",
      "61. import warnings  # find_human_contam_in_plasmo_reads.py\n",
      "62. from Bio import BiopythonExperimentalWarning  # find_human_contam_in_plasmo_reads.py\n",
      "63. from Bio.Blast import NCBIXML  # find_human_contam_in_plasmo_reads.py\n",
      "64. from nex_to_fa import nex_to_fa  # all_nex_to_fa_and_afa.py\n",
      "65. from module_afa_to_nex import delete_extra_mesquite_lines  # get_residue_numbers_for_position.py\n",
      "66. import numpy as np  # get_residue_numbers_for_position.py\n",
      "67. from Bio.SeqRecord import SeqRecord  # get_residue_numbers_for_position.py\n",
      "68. import collections  # get_residue_numbers_for_position.py\n",
      "69. import module_dacksify_pos_hmmer_hits  # get_residue_numbers_for_position.py\n",
      "70. from module_amoebae_phylo_clas import ModelInfoFromCSV  # setup_pairwise_rooting_analysis2.py\n",
      "71. from search_alignment_space import get_type_seqs_dict, get_nodes_of_interest,\\  # setup_pairwise_rooting_analysis2.py\n",
      "72. get_corresponding_node  # setup_pairwise_rooting_analysis2.py\n",
      "73. from module_amoebae import mask_nex2  # setup_pairwise_rooting_analysis2.py\n",
      "74. from module_amoebae_constrain_mb import constrain_mb_with_tree  # setup_pairwise_rooting_analysis2.py\n",
      "75. import module_search_scaffolds  # search_scaffolds.py\n",
      "76. from module_mask_nex import mask_nex  # mask_nex.py\n",
      "77. import gffutils  # experimental_gff3_sql_parser.py\n",
      "78. from module_search_scaffolds import check_if_two_hsp_ranges_overlap  # experimental_gff3_sql_parser.py\n",
      "79. from module_afa_to_nex import afa_to_nex, nex_to_afa, delete_extra_mesquite_lines  # realign_nex.py\n",
      "80. import pylab  # phylo_signal_finder.py\n",
      "81. from random import shuffle  # test_iqtree_constraint_interpretation.py\n",
      "82. import unittest  # test_iqtree_constraint_interpretation.py\n",
      "83. import pandas as pd  # module_similarity_score.py\n",
      "84. import module_amoebae  # module_amoebae_search.py\n",
      "85. from module_amoebae_srchresfile import SrchResFile  # module_amoebae_search.py\n",
      "86. import module_amoebae_column_header_lists  # module_amoebae_search.py\n",
      "87. from module_amoebae_run_searches import get_query_list_from_file,\\  # module_amoebae_search.py\n",
      "88. get_db_list_from_file, get_out_query_list_path, get_out_db_list_path,\\  # module_amoebae_search.py\n",
      "89. determine_search_method, search_result_filepath, run_any_search,\\  # module_amoebae_search.py\n",
      "90. run_all_searches, get_out_hmm_path, get_query_subdir  # module_amoebae_search.py\n",
      "91. from module_search_scaffolds import split_tblastn_hits_into_separate_genes,\\  # module_amoebae_search.py\n",
      "92. get_hit_seq_record_and_coord, get_hit_seq_record_and_coord2, get_cluster_range  # module_amoebae_search.py\n",
      "93. from Bio.Blast import Record  # module_search_scaffolds.py\n",
      "94. from run_exonerate import ExonerateLocusResult, get_subseq_from_nucl,\\  # module_search_scaffolds.py\n",
      "95. run_exonerate_as_subprocess  # module_search_scaffolds.py\n",
      "96. from search_alignment_space import get_corresponding_node  # replace_nodes_in_tree.py\n",
      "97. import module_add_to_db  # module_add_to_queries_test.py\n",
      "98. import statistics  # module_amoebae_select_seqs.py\n",
      "99. from module_paralogue_counter import get_seq_obj_from_srch_res_csv_info  # module_amoebae_select_seqs.py\n",
      "100. from module_amoebae_phylo_clas import ModelInfoFromCSV,\\  # module_amoebae_select_seqs.py\n",
      "101. get_clade_name_from_model, code_names_in_ali, quote_tree, code_tree,\\  # module_amoebae_select_seqs.py\n",
      "102. uncode_tree, uncode_tree_obj  # module_amoebae_select_seqs.py\n",
      "103. from module_paralogue_counter import add_seq_to_alignment3  # module_amoebae_select_seqs.py\n",
      "104. from module_afa_to_nex import delete_extra_mesquite_lines, afa_to_nex, nex_to_afa, nex_to_phylip  # module_amoebae_select_seqs.py\n",
      "105. from Bio.Align import MultipleSeqAlignment  # module_amoebae_select_seqs.py\n",
      "106. from ete3 import Tree, faces, AttrFace, TreeStyle, NodeStyle, TextFace  # module_amoebae_select_seqs.py\n",
      "107. from module_amoebae_select_seqs import get_clade_name_from_model2,\\  # module_amoebae_select_positions.py\n",
      "108. get_nodes_of_interest, get_list_of_leaf_names_for_node, TaxonomicInfo,\\  # module_amoebae_select_positions.py\n",
      "109. get_taxonomic_info, define_nodestyles_dict_for_colourcoding,\\  # module_amoebae_select_positions.py\n",
      "110. define_textface_for_labeling_stem, get_corresponding_node,\\  # module_amoebae_select_positions.py\n",
      "111. get_ml_tree_branch_lengths, get_branch_length_info, reduce_alignment  # module_amoebae_select_positions.pyAll import statements ran successfully.\n",
      "\n",
      "112. import matplotlib.pyplot as plt  # generate_sankey_diagram.py\n",
      "113. from matplotlib.sankey import Sankey  # generate_sankey_diagram.py\n",
      "114. from visualize_trees import get_nodes_with_paralogues  # visualize_trees_test.py\n",
      "115. from module_amoebae_get_datatype import get_datatype_for_sequence_string  # test_module_amoebae_get_datatype.py\n",
      "116. from test_module_amoebae_get_datatype_testseqs import testseq1  # test_module_amoebae_get_datatype.py\n",
      "117. from get_alt_topos import get_total_num_topos_for_n_taxa,\\  # test_get_alt_topos.py\n",
      "118. get_all_alt_topologies,\\  # test_get_alt_topos.py\n",
      "119. get_all_unique_unrooted_bifurcating_topologies_for_n_taxa,\\  # test_get_alt_topos.py\n",
      "120. trees, get_polytomy_for_treenode  # test_get_alt_topos.py\n",
      "121. from module_amoebae_select_seqs import get_ml_tree_branch_lengths  # search_alignment_space.py\n",
      "122. from module_amoebae import find_input_file_in_parent_directory  # search_alignment_space.py\n",
      "123. import platform  # search_alignment_space.py\n",
      "124. import sys, os  # get_nonredun_import_statements_for_amoebae.py\n",
      "125. from math import factorial  # get_alt_topos.py\n",
      "126. from itertools import product  # get_alt_topos.py\n",
      "127. from module_afa_to_nex import nex_to_afa, afa_to_nex, determine_alphabet  # replace_seqs.py\n",
      "128. from module_add_to_db import make_blast_db  # replace_seqs.py\n",
      "129. from Bio import SearchIO  # replace_seqs.py\n",
      "130. from module_paralogue_counter import add_seq_to_alignment3,\\  # module_amoebae_phylo_clas.py\n",
      "131. modify_seq_descr_for_tree  # module_amoebae_phylo_clas.py\n",
      "132. from module_amoebae_name_replace import write_afa_with_code_names,\\  # module_amoebae_phylo_clas.py\n",
      "133. codenames_nex, write_newick_tree_with_coded_names  # module_amoebae_phylo_clas.py\n",
      "134. from get_alt_topos import get_all_alt_topologies, get_polytomy_for_treenode  # module_amoebae_phylo_clas.py\n",
      "135. from module_amoebae_name_replace import get_conversion_dict_from_table  # module_amoebae_prune.py\n",
      "136. write_newick_tree_with_uncoded_names,\\  # module_amoebae_prune.py\n",
      "137. write_newick_tree_with_coded_names  # module_amoebae_prune.py\n",
      "138. from ete3 import Tree, TreeStyle, Tree, TextFace, add_face_to_node  # module_amoebae_prune.py\n",
      "139. from visualize_trees import translate_int_node_names_to_support, visualize_tree  # module_amoebae_prune.py\n",
      "140. import matplotlib  # module_amoebae_plot.py\n",
      "141. from module_amoebae import get_seqs_from_fasta_db, get_subseq_from_fasta_db  # module_amoebae_srchresfile.py\n",
      "142. from module_search_scaffolds import get_blastp_hit_seq_obj_and_coord, get_tblastn_hit_seq_obj_and_coord  # module_amoebae_srchresfile.py\n",
      "143. from module_afa_to_nex import nex_to_afa, afa_to_nex, delete_extra_mesquite_lines, nex_to_phylip, nex_to_mbnex  # module_amoebae_name_replace.py\n",
      "144. import module_amoebae_srchresfile  # module_amoebae_srchresfile_test.py\n",
      "145. import smtplib  # module_eml.py\n",
      "146. from email.mime.multipart import MIMEMultipart  # module_eml.py\n",
      "147. from email.mime.text import MIMEText  # module_eml.py\n",
      "148. from ete3 import Tree, NodeStyle, TreeStyle, TextFace  # visualize_trees.py\n",
      "149. from matplotlib.backends.backend_pdf import PdfPages  # visualize_trees.py\n",
      "150. import matplotlib.colors as mc  # visualize_trees.py\n",
      "151. import colorsys  # visualize_trees.py\n",
      "152. from module_boots_on_mb import mbcontre_to_newick_w_probs  # visualize_trees.py\n",
      "153. from matplotlib.pyplot import imread  # visualize_trees.py\n",
      "154. from search_alignment_space import get_type_seqs_dict, get_nodes_of_interest, get_clade_name_from_model2  # visualize_trees.py\n",
      "155. from Bio import AlignIO, SeqIO  # module_paralogue_counter.py\n",
      "156. from module_afa_to_nex import afa_to_nex, delete_extra_mesquite_lines  # module_paralogue_counter.py\n",
      "157. from module_similarity_score import get_similarity_score, get_score_dataframe_from_file  # module_paralogue_counter.py\n",
      "158. from module_amoebae import get_seq_obj_from_srch_res_csv_info,\\  # module_paralogue_counter.py\n",
      "159. get_hit_range_from_hsp_ranges  # module_paralogue_counter.py\n",
      "160. from generate_sankey_diagram import generate_sankey  # module_paralogue_counter.py\n",
      "161. from generate_histogram_plot import generate_histogram,\\  # module_paralogue_counter.py\n",
      "162. generate_double_histogram, generate_bar_chart  # module_paralogue_counter.py\n",
      "\n",
      "Running output script to test import statements...\n",
      "\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "amoebae check_imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import some basic python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "from Bio import SeqIO\n",
    "from Bio import Entrez\n",
    "import glob\n",
    "from Bio.Blast import NCBIXML\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "sys.path.append('/opt/notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record the specific version of AMOEBAE code used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record git repository version information.\n",
    "wd = !pwd\n",
    "script_dir = wd[0] \n",
    "git_hash = str(subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=script_dir).strip())\n",
    "git_branch = str(subprocess.check_output([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=script_dir).strip())  \n",
    "print('\\nGit repository (code) version: ' + git_hash + ' (branch name: ' + git_branch + ')\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download peptide and nucleotide sequences for specific genomes.\n",
    "\n",
    "Let's download the predicted peptide sequences, genomic assembly (nucleotide\n",
    "sequences of assembled chromosomes), and annotation files (in GFF3 format) for the following eukaryotes from NCBI:\n",
    "\n",
    "- *Arabidopsis thaliana*\n",
    "- *Trypanosoma brucei*\n",
    "- *Dictyostelium discoideum*\n",
    "- *Allomyces macrogynus*\n",
    "- *Saccharomyces cerevisiae*\n",
    "\n",
    "\n",
    "This could take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.61 ms, sys: 83.7 ms, total: 90.3 ms\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initiate a list of file paths for downloaded sequence and annotation files.\n",
    "datafile_path_list = []\n",
    "\n",
    "# Define a dictionary of source URLs and new filenames for sequence and annotation files.\n",
    "datafile_dict = {\"Arabidopsis_thaliana.faa.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_protein.faa.gz\",\n",
    "                 \"Arabidopsis_thaliana.fna.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_genomic.fna.gz\",\n",
    "                 \"Arabidopsis_thaliana.gff3.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_genomic.gff.gz\",\n",
    "                 \"Saccharomyces_cerevisiae.faa.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_protein.faa.gz\",\n",
    "                 \"Saccharomyces_cerevisiae.fna.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_genomic.fna.gz\",\n",
    "                 \"Saccharomyces_cerevisiae.gff3.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_genomic.gff.gz\",\n",
    "                 \"Trypanosoma_brucei.faa.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/210/295/GCF_000210295.1_ASM21029v1/GCF_000210295.1_ASM21029v1_protein.faa.gz\",\n",
    "                 \"Trypanosoma_brucei.fna.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/210/295/GCF_000210295.1_ASM21029v1/GCF_000210295.1_ASM21029v1_genomic.fna.gz\",\n",
    "                 \"Trypanosoma_brucei.gff3.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/210/295/GCF_000210295.1_ASM21029v1/GCF_000210295.1_ASM21029v1_genomic.gff.gz\",\n",
    "                 \"Dictyostelium_discoideum.faa.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/004/695/GCF_000004695.1_dicty_2.7/GCF_000004695.1_dicty_2.7_protein.faa.gz\",\n",
    "                 \"Dictyostelium_discoideum.fna.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/004/695/GCF_000004695.1_dicty_2.7/GCF_000004695.1_dicty_2.7_genomic.fna.gz\",\n",
    "                 \"Dictyostelium_discoideum.gff3.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/004/695/GCF_000004695.1_dicty_2.7/GCF_000004695.1_dicty_2.7_genomic.gff.gz\",\n",
    "                 \"Allomyces_macrogynus.faa.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/151/295/GCA_000151295.1_A_macrogynus_V3/GCA_000151295.1_A_macrogynus_V3_protein.faa.gz\",\n",
    "                 \"Allomyces_macrogynus.fna.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/151/295/GCA_000151295.1_A_macrogynus_V3/GCA_000151295.1_A_macrogynus_V3_genomic.fna.gz\",\n",
    "                 \"Allomyces_macrogynus.gff3.gz\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/151/295/GCA_000151295.1_A_macrogynus_V3/GCA_000151295.1_A_macrogynus_V3_genomic.gff.gz\"\n",
    "          }\n",
    "\n",
    "# Make a new temporary directory to store data files.\n",
    "temp_db_dir_name = 'temporary_db_dir'\n",
    "assert not os.path.isdir(temp_db_dir_name)\n",
    "os.mkdir(temp_db_dir_name)\n",
    "\n",
    "# Download all the data files via NCBI's FTP server.\n",
    "for filename in datafile_dict.keys():\n",
    "    url = datafile_dict[filename]\n",
    "    filepath = os.path.join(temp_db_dir_name, filename)\n",
    "    if not os.path.isfile(filepath):\n",
    "        subprocess.call(['curl', url, '--output', filepath + '.gz'])\n",
    "        subprocess.call(['gunzip', filepath + '.gz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate a data directory structure\n",
    "To generate a directory structure and spreadsheets for storing formatted sequence files\n",
    "and metadata for each sequence file, use the 'mkdatadir' command (this takes a\n",
    "single argument which is the full path that you want your new directory to be\n",
    "written to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        \n",
      "        To allow AMOEBAE scripts to locate your new data directory, change the\n",
      "        value of the root_amoebae_data_dir variable in the settings.py file to\n",
      "        the full path to the directory:\n",
      "\n",
      "        AMOEBAE_Data\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export DATADIR=\"AMOEBAE_Data\"\n",
    "amoebae mkdatadir $DATADIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will prompt you to set the 'root\\_amoebae\\_data\\_dir' variable in the\n",
    "settings.py file to this new directory path so that AMOEBAE scripts can locate\n",
    "your files.\n",
    "\n",
    "This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMOEBAE_Data\n"
     ]
    }
   ],
   "source": [
    "# Check that the path indicated in the settings file is correct.\n",
    "import settings\n",
    "print(settings.root_amoebae_data_dir)\n",
    "assert settings.root_amoebae_data_dir == \"AMOEBAE_Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare databases for searching\n",
    "To generate a directory structure and spreadsheets for storing formatted sequence files\n",
    "and metadata for each sequence file, use the 'mkdatadir' command (this takes a\n",
    "single argument which is the full path that you want your new directory to be\n",
    "written to).\n",
    "\n",
    "This will take several minutes, because the FASTA files need to be re-written with re-formatted sequence headers and the GFF3 files need to be converted to SQL databases using gffutils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Building a new DB, current time: 02/23/2020 21:56:50\n",
      "New DB name:   /opt/notebooks/notebooks/AMOEBAE_Data/Genomes/Allomyces_macrogynus.faa\n",
      "New DB title:  AMOEBAE_Data/Genomes/Allomyces_macrogynus.faa\n",
      "Sequence type: Protein\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 19447 sequences in 2.13164 seconds.\n",
      "\n",
      "\n",
      "Creating SSI index for AMOEBAE_Data/Genomes/Allomyces_macrogynus.faa...    done.\n",
      "Indexed 19447 sequences (19447 names).\n",
      "SSI index written to file AMOEBAE_Data/Genomes/Allomyces_macrogynus.faa.ssi\n",
      "\n",
      "\n",
      "Building a new DB, current time: 02/23/2020 21:56:55\n",
      "New DB name:   /opt/notebooks/notebooks/AMOEBAE_Data/Genomes/Allomyces_macrogynus.fna\n",
      "New DB title:  AMOEBAE_Data/Genomes/Allomyces_macrogynus.fna\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 101 sequences in 1.09458 seconds.\n",
      "\n",
      "\n",
      "Creating SSI index for AMOEBAE_Data/Genomes/Allomyces_macrogynus.fna...    done.\n",
      "Indexed 101 sequences (101 names).\n",
      "SSI index written to file AMOEBAE_Data/Genomes/Allomyces_macrogynus.fna.ssi\n",
      "\n",
      "\n",
      "Building a new DB, current time: 02/23/2020 21:58:31\n",
      "New DB name:   /opt/notebooks/notebooks/AMOEBAE_Data/Genomes/Arabidopsis_thaliana.faa\n",
      "New DB title:  AMOEBAE_Data/Genomes/Arabidopsis_thaliana.faa\n",
      "Sequence type: Protein\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 48265 sequences in 3.71195 seconds.\n",
      "\n",
      "\n",
      "Creating SSI index for AMOEBAE_Data/Genomes/Arabidopsis_thaliana.faa...    done.\n",
      "Indexed 48265 sequences (48265 names).\n",
      "SSI index written to file AMOEBAE_Data/Genomes/Arabidopsis_thaliana.faa.ssi\n",
      "\n",
      "\n",
      "Building a new DB, current time: 02/23/2020 21:58:39\n",
      "New DB name:   /opt/notebooks/notebooks/AMOEBAE_Data/Genomes/Arabidopsis_thaliana.fna\n",
      "New DB title:  AMOEBAE_Data/Genomes/Arabidopsis_thaliana.fna\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 7 sequences in 2.43417 seconds.\n",
      "\n",
      "\n",
      "Creating SSI index for AMOEBAE_Data/Genomes/Arabidopsis_thaliana.fna...    done.\n",
      "Indexed 7 sequences (7 names).\n",
      "SSI index written to file AMOEBAE_Data/Genomes/Arabidopsis_thaliana.fna.ssi\n",
      "\n",
      "\n",
      "Building a new DB, current time: 02/23/2020 22:05:26\n",
      "New DB name:   /opt/notebooks/notebooks/AMOEBAE_Data/Genomes/Dictyostelium_discoideum.faa\n",
      "New DB title:  AMOEBAE_Data/Genomes/Dictyostelium_discoideum.faa\n",
      "Sequence type: Protein\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 13315 sequences in 1.45438 seconds.\n",
      "\n",
      "\n",
      "Creating SSI index for AMOEBAE_Data/Genomes/Dictyostelium_discoideum.faa...    done.\n",
      "Indexed 13315 sequences (13315 names).\n",
      "SSI index written to file AMOEBAE_Data/Genomes/Dictyostelium_discoideum.faa.ssi\n",
      "\n",
      "\n",
      "Building a new DB, current time: 02/23/2020 22:05:30\n",
      "New DB name:   /opt/notebooks/notebooks/AMOEBAE_Data/Genomes/Dictyostelium_discoideum.fna\n",
      "New DB title:  AMOEBAE_Data/Genomes/Dictyostelium_discoideum.fna\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 41 sequences in 0.65231 seconds.\n",
      "\n",
      "\n",
      "Creating SSI index for AMOEBAE_Data/Genomes/Dictyostelium_discoideum.fna...    done.\n",
      "Indexed 41 sequences (41 names).\n",
      "SSI index written to file AMOEBAE_Data/Genomes/Dictyostelium_discoideum.fna.ssi\n",
      "\n",
      "\n",
      "Building a new DB, current time: 02/23/2020 22:06:02\n",
      "New DB name:   /opt/notebooks/notebooks/AMOEBAE_Data/Genomes/Saccharomyces_cerevisiae.faa\n",
      "New DB title:  AMOEBAE_Data/Genomes/Saccharomyces_cerevisiae.faa\n",
      "Sequence type: Protein\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 6002 sequences in 0.531907 seconds.\n",
      "\n",
      "\n",
      "Creating SSI index for AMOEBAE_Data/Genomes/Saccharomyces_cerevisiae.faa...    done.\n",
      "Indexed 6002 sequences (6002 names).\n",
      "SSI index written to file AMOEBAE_Data/Genomes/Saccharomyces_cerevisiae.faa.ssi\n",
      "\n",
      "\n",
      "Building a new DB, current time: 02/23/2020 22:06:04\n",
      "New DB name:   /opt/notebooks/notebooks/AMOEBAE_Data/Genomes/Saccharomyces_cerevisiae.fna\n",
      "New DB title:  AMOEBAE_Data/Genomes/Saccharomyces_cerevisiae.fna\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 17 sequences in 0.196987 seconds.\n",
      "\n",
      "\n",
      "Creating SSI index for AMOEBAE_Data/Genomes/Saccharomyces_cerevisiae.fna...    done.\n",
      "Indexed 17 sequences (17 names).\n",
      "SSI index written to file AMOEBAE_Data/Genomes/Saccharomyces_cerevisiae.fna.ssi\n",
      "\n",
      "\n",
      "Building a new DB, current time: 02/23/2020 22:06:18\n",
      "New DB name:   /opt/notebooks/notebooks/AMOEBAE_Data/Genomes/Trypanosoma_brucei.faa\n",
      "New DB title:  AMOEBAE_Data/Genomes/Trypanosoma_brucei.faa\n",
      "Sequence type: Protein\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 9822 sequences in 0.862367 seconds.\n",
      "\n",
      "\n",
      "Creating SSI index for AMOEBAE_Data/Genomes/Trypanosoma_brucei.faa...    done.\n",
      "Indexed 9822 sequences (9822 names).\n",
      "SSI index written to file AMOEBAE_Data/Genomes/Trypanosoma_brucei.faa.ssi\n",
      "\n",
      "\n",
      "Building a new DB, current time: 02/23/2020 22:06:20\n",
      "New DB name:   /opt/notebooks/notebooks/AMOEBAE_Data/Genomes/Trypanosoma_brucei.fna\n",
      "New DB title:  AMOEBAE_Data/Genomes/Trypanosoma_brucei.fna\n",
      "Sequence type: Nucleotide\n",
      "Keep MBits: T\n",
      "Maximum file size: 1000000000B\n",
      "Adding sequences from FASTA; added 11 sequences in 0.413367 seconds.\n",
      "\n",
      "\n",
      "Creating SSI index for AMOEBAE_Data/Genomes/Trypanosoma_brucei.fna...    done.\n",
      "Indexed 11 sequences (11 names).\n",
      "SSI index written to file AMOEBAE_Data/Genomes/Trypanosoma_brucei.fna.ssi\n",
      "Preparing sequence databases for searching took the following amount of time: 0hrs 9min 52sec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "for X in temporary_db_dir/*; do amoebae add_to_dbs $X; done\n",
    "\n",
    "ELAPSED=\"Preparing sequence databases for searching took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List the databases now accessible by AMOEBAE.\n",
    "amoebae list_dbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take some time, because an SQL database will be generated to store information from the GFF3 annotation file (this is what is will be listed in the genome info CSV file).\n",
    "\n",
    "When this is finished, copy the name of the .sql file to the row for the corresponding genomic assembly (.fna) file in the column with the header \"Annotations file\", and do the same for the row describing the corresponding peptide sequence (.faa) file. This allows the correct GFF3 file to be used for the assembly (.fna file) and predicted amino acid sequences (.faa).\n",
    "\n",
    "Next you must manually modify the spreadsheet so that it has the correct metadata for this sequence file. Open it with Excel or Open Office, and enter the following information:\n",
    "- Fill the \"Superbranch\", \"Supergroup\", \"Group\", and \"Species (if applicable)\" fields with the values \"Diaphoretickes\", \"Archaeplastida\", \"Embryophyta\", and \"Arabidopsis thaliana\", respectively. These are arbitrary selected taxonomic groups to which Arabidopsis belongs (Adl et al., 2018), but if note similar taxonomic information for each genome you download then it will help to keep organized.\n",
    "- Fill the \"Taxon\" field with the abbreviation \"Athaliana\". This is used for abbreviating names when necessary.\n",
    "- Fill in the other fields as you see fit. It is recommended that you keep track of where you downloaded files from, and which assembly you used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional:\n",
    "\n",
    "#Update information in genome info table.\n",
    "\n",
    "# Parse the CSV file.\n",
    "\n",
    "# loop over rows.\n",
    "\n",
    "# If the filename matches one of the keys in the datafile_dict dict, then enter the corresponding URL in the \"Source\" column.\n",
    "\n",
    "# Save the updated dataframe to the original file path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter your email to access the NCBI protein database via NCBI Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = input(\"Enter your email address here: \")  # Tell NCBI who you are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download single-sequence queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define a dictionary with NCBI sequence accessions as keys and filenames to write\n",
    "# the corresponding sequences to as values.\n",
    "query_dict = {\"NP_194077.1\": \"AP1beta_Athaliana_NP_194077.1_query.faa\",\n",
    "              \"NP_851058.1\": \"AP2alpha_Athaliana_NP_851058.1_query.faa\",\n",
    "              \"NP_974895.1\": \"AP2mu_Athaliana_NP_974895.1_query.faa\",\n",
    "              \"NP_175219.1\": \"AP2sigma_Athaliana_NP_175219.1_query.faa\",\n",
    "              \"NP_566961.1\": \"Sec12_Athaliana_NP_566961.1_query.faa\",\n",
    "              \"NP_200929.1\": \"SNAP33_Athaliana_NP_200929.1_query.faa\",\n",
    "              \"NP_193449.1\": \"Rab2_Athaliana_NP_193449.1_query.faa\"\n",
    "          }\n",
    "\n",
    "# Make a new temporary directory to store sequence files.\n",
    "temp_query_dir_name = 'temporary_query_dir'\n",
    "assert not os.path.isdir(temp_query_dir_name), \"\"\"Directory already exists.\"\"\"\n",
    "os.mkdir(temp_query_dir_name)\n",
    "\n",
    "# Loop over keys in the query_dict dictionary.\n",
    "for accession in query_dict.keys():\n",
    "    # Retrieve the corresponding filename from the dictionary.\n",
    "    filename = query_dict[accession]\n",
    "    # Only download sequences that have not already been downloaded.\n",
    "    if not os.path.isfile(filename):\n",
    "        # Download the sequence from NCBI via Entrez, using the Biopython module.\n",
    "        net_handle = Entrez.efetch(db=\"protein\", id=accession, rettype=\"fasta\", retmode=\"text\")\n",
    "        out_handle = open(os.path.join(temp_query_dir_name, filename), \"w\")\n",
    "        out_handle.write(net_handle.read())\n",
    "        out_handle.close()\n",
    "        net_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare single-sequence queries for searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries must be formatted and stored in a similar manner to genomic data files. The query files will include FASTA files containing one sequence and FASTA files containing multiple sequences.\n",
    "Now we are going to generate the query files and add them to your AMOEBAE_Data/ Queries directory, in a similar way to how we added genomic data files to the AMOEBA E_Data/Genomes directory. Since you already downloaded all the peptide sequences for Arabidopsis thaliana, you can retrieve these from your downloaded data using one of the scripts in the amoebae/misc_scripts folder. First, letâ€™s generate a query for the A. thaliana AP-1/2 beta subunit(s), which is a component of both the AP-1 and AP-2 complexes, using a representative sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "for QUERYFILE in temporary_query_dir/*.faa; do amoebae add_to_queries $QUERYFILE; done\n",
    "\n",
    "ELAPSED=\"Preparing query sequences for searching took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae list_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the information in the spreadsheet (AMOEBAE_Data/Queries/0_query_in fo.csv). Make sure that the query titles AP1beta, AP2alpha, AP2mu, and AP2sigma are entered in the appropriate rows in the \"Query title\" column. This allows multiple query files to be associated with the same query title if they are to be used to search for the same set of homologues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct alignments for profile similarity searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define a dictionary of NCBI sequence accessions and filenames to which to write the corresponding sequences.\n",
    "query_title_dict = {\"AP1beta\": \"NP_194077.1,CBI34366.3,XP_015631818.1,XP_024516549.1,OAE33273.1\",\n",
    "                    \"AP2alpha\": \"NP_851058.1,XP_002270388.1,XP_015631820.1,PTQ35247.1,XP_024525508.1\",\n",
    "                    \"AP2mu\": \"NP_974895.1,XP_002281297.1,XP_015627628.1,OAE25965.1,XP_002973295.1\",\n",
    "                    \"AP2sigma\": \"NP_175219.1,XP_015618362.1,PTQ50284.1,XP_002275803.1,XP_024518676.1\",\n",
    "                    \"Sec12\": \"NP_566961.1,XP_002262948.1,XP_015647566.1,OAE21792.1,XP_024530559.1\",\n",
    "                    \"SNAP33\": \"NP_200929.1,XP_002284486.1,AAW82752.1,EFJ31467.1,OAE29824.1,XP_006270633.1,XP_006010378.1,XP_006625751.1,NP_001080510.1,XP_020370357.1,XP_015181699.1,XP_031769811.1\",\n",
    "                    \"Rab2\": \"NP_193449.1,XP_003635585.2,XP_015626284.1,XP_002965710.1,PTQ28228.1\"\n",
    "                   }\n",
    "                    \n",
    "\n",
    "# Make a new temporary directory to store sequence files.\n",
    "temp_alignment_dir_name = 'temporary_alignment_dir'\n",
    "assert not os.path.isdir(temp_alignment_dir_name), \"\"\"Directory already exists.\"\"\"\n",
    "os.mkdir(temp_alignment_dir_name)\n",
    "\n",
    "# Download query sequences and write to multiple-sequence FASTA files.\n",
    "for query_title in query_title_dict.keys():\n",
    "    accession_list_string = query_title_dict[query_title]\n",
    "    filepath = os.path.join(temp_alignment_dir_name, query_title + '_hmm1.faa')\n",
    "    if not os.path.isfile(filepath):\n",
    "        net_handle = Entrez.efetch(db=\"protein\", id=accession_list_string, rettype=\"fasta\", retmode=\"text\")\n",
    "        out_handle = open(filepath, \"w\")\n",
    "        out_handle.write(net_handle.read())\n",
    "        out_handle.close()\n",
    "        net_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "for X in temporary_alignment_dir/*.faa; do amoebae align_fa $X --output_format fasta; done\n",
    "\n",
    "ELAPSED=\"Aligning FASTA files took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visually inspect alignments\n",
    "Alignments used as queries should be visually inspected to make sure that there are no obvious errors in the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "for QUERYFILE in temporary_alignment_dir/*.afaa; do amoebae afa_to_nex $QUERYFILE; done\n",
    "echo \"Alignments to observe:\"\n",
    "ls temporary_alignment_dir/*.nex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare query alignments for searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "for QUERYFILE in temporary_alignment_dir/*.afaa; do amoebae add_to_queries $QUERYFILE; done\n",
    "\n",
    "ELAPSED=\"Preparing HMM queries from alignments took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae list_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate lists of potential redundant sequences among *A. thaliana* peptide sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, a reciprocal-best-hit search strategy will be used. If you are using a reciprocal- best-hit search strategy, then your initial round of searches will be performed using your original queries (assembled above) to search your genomes of interest. This initial round of searches will be referred to herein as \"forward searches\", and subsequent searches using forward search hits as queries into reference genomes will be referred to as \"reverse searches\".\n",
    "\n",
    "A slight complication to this search strategy is that the NCBI RefSeq peptide sequences for the A. thaliana genome include alternative transcripts and lineage-specific inparalogues (as do other databases), implying that if these were retrieved as the top hits in the reverse searches instead of the original query sequence, then this would still potentially be a positive result. So, to properly interpret reverse search results it will be necessary to determine which sequences in our A. thaliana .faa file are redundant for our purposes. To do this we will use the get_redun_hits command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Optional. Get the help output for the get_redun_hits command.\n",
    "amoebae get_redun_hits -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env REDUNHITDIR=Redundant_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "# Make a directory to store information about redundant hits.\n",
    "#mkdir $REDUNHITDIR\n",
    "\n",
    "# Write a file listing names of query files to be used.\n",
    "amoebae list_queries > $REDUNHITDIR/queries.txt\n",
    "\n",
    "# Use AMOEBAE to retrieve potential redundant hit sequences.\n",
    "amoebae get_redun_hits $REDUNHITDIR --query_list_file $REDUNHITDIR/queries.txt --db_name Athaliana_database.faa\n",
    "\n",
    "ELAPSED=\"Retrieving potentially redundant sequences took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a directory in the Redundant_hits folder with a .csv file. Open the CSV file. This file contains a summary of BLASTP or HMMer search results for searches with the specified queries into the *S. cerevisiae* predicted proteins. In the column with the header \"Positive/redundant (+) or negative (-) hit for queries with query title (edit this column)\", change the â€™-â€™ to â€™+â€™ for hits that are the original query, or redundant with the original query for the purposes of this analysis.\n",
    "It should be apparent upon inspection of the ranking of hits and comparison of the associated E-values which hits are redundant with your queries. The redundant accessions for each query (both single sequence and HMM queries for the same AP-2 subunit) should be similar to the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify redundant sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary with query titles as keys and lists of sequence IDs as values, where the IDs are for A. thaliana sequences that are redundant with the original A. thaliana query sequence.\n",
    "redun_seq_dict = {\"AP1beta\":  [\"NP_194077.1\",\n",
    "                               \"NP_192877.1\",\n",
    "                               \"NP_001328014.1\",\n",
    "                               \"NP_001190701.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"AP2alpha\": [\"NP_851058.1\",\n",
    "                               \"NP_851057.1\",\n",
    "                               \"NP_197669.1\",\n",
    "                               \"NP_001330971.1\",\n",
    "                               \"NP_001330970.1\",\n",
    "                               \"NP_001330969.1\",\n",
    "                               \"NP_197670.1\",\n",
    "                               \"NP_001330127.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"AP2mu\":    [\"NP_974895.1\",\n",
    "                               \"NP_199475.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"AP2sigma\": [\"NP_175219.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"Sec12\":    [\"NP_566961.1\",\n",
    "                               \"NP_568738.1\",\n",
    "                               \"NP_680414.1\",\n",
    "                               \"NP_178256.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"SNAP33\":   [\"NP_200929.1\",\n",
    "                               \"NP_001332102.1\",\n",
    "                               \"NP_172842.1\",\n",
    "                               \"NP_001318998.1\",\n",
    "                               \"NP_196405.1\",\n",
    "                               \"NP_001318503.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"Rab2\":     [\"NP_193449.1\",\n",
    "                               \"NP_193450.1\",\n",
    "                               \"NP_195311.1\",\n",
    "                               \"NP_001078499.1\"\n",
    "                               ]\n",
    "                   }\n",
    "\n",
    "\n",
    "# Identify path to redundant seqs CSV file.\n",
    "redundant_seqs_csv = glob.glob(os.path.join('Redundant_hits', os.path.join('redun_hits_*', '0_redun_hits_*.csv')))[0]\n",
    "\n",
    "# Define path for new modified redundant seqs CSV file.\n",
    "redundant_seqs_csv2 = redundant_seqs_csv.rsplit(\".\", 1)[0] + '_2.csv'\n",
    "\n",
    "# Open the redundant seqs CSV file, and a new one.\n",
    "with open(redundant_seqs_csv) as infh, open(redundant_seqs_csv2, 'w') as o:\n",
    "    # Loop over lines in the CSV file.\n",
    "    for i in infh:\n",
    "        if not i.startswith(\"Query Title\"):\n",
    "            # Identify query title in line.\n",
    "            line_query_title = i.split(',')[0].strip()\n",
    "            # Identify accession/id for sequence hit represented in this row.\n",
    "            line_accession = i.split(',')[9].strip().strip('\\\"')\n",
    "            # Loop over keys (query titles) in the redundant seqs dictionary.\n",
    "            query_title_in_keys = False\n",
    "            for query_title in redun_seq_dict.keys():\n",
    "                if line_query_title == query_title:\n",
    "                    query_title_in_keys = True\n",
    "                    #print('YYY')\n",
    "                    #print(line_accession)\n",
    "                    #print(redun_seq_dict[line_query_title])\n",
    "                    # Determine whether the accession is a redundant accession.\n",
    "                    for acc in redun_seq_dict[line_query_title]:\n",
    "                        #print(line_accession, acc)\n",
    "                        if line_accession == acc:\n",
    "                            # Change the - to + so that the accession will be included in the list of redundant accessions used by AMOEBAE.\n",
    "                            i = ','.join(i.split(',')[:4]) + ',+,' + ','.join(i.split(',')[5:])\n",
    "                            break\n",
    "                # Break loop if the corresponding query title was found already.\n",
    "                if query_title_in_keys:\n",
    "                    break\n",
    "            # Check that a query title could be recognized as one that is a key in the dictionary.\n",
    "            assert query_title_in_keys, \"\"\"Could not find query title %s in dictionary.\"\"\" % line_query_title\n",
    "        # Write (modified) line to new CSV file.\n",
    "        o.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the contents of the file listing redundant sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run forward searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin searching, make a new folder to contain search results, and write text files listing the names (not full paths) of FASTA files you want to use as queries and those that you want to search in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env SRCHRESDIR=AMOEBAE_Search_Results_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Make a new directory to contain search results.\n",
    "mkdir $SRCHRESDIR\n",
    "# Write query and database list files.\n",
    "amoebae list_queries > $SRCHRESDIR/queries.txt\n",
    "amoebae list_dbs > $SRCHRESDIR/databases.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up searches using the setup_fwd_srch command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Optional. Get the help output for the setup_fwd_srch command.\n",
    "amoebae setup_fwd_srch -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%env FWDSRCHDIR=fwd_srch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set up forward searches.\n",
    "amoebae setup_fwd_srch $SRCHRESDIR\\\n",
    "                       $SRCHRESDIR/queries.txt\\\n",
    "                       $SRCHRESDIR/databases.txt\\\n",
    "                       --outdir $SRCHRESDIR/$FWDSRCHDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a new sub-directory with a name that starts with \"fwd_srch_\". Now run the searches with this directory as input via the run_fwd_srch command. Forward search criteria may be selected at this point (view the relevant optional arguments via the -h option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tree $SRCHRESDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "# Run forward searches. This could take a while.\n",
    "amoebae run_fwd_srch $SRCHRESDIR/$FWDSRCHDIR\n",
    "\n",
    "ELAPSED=\"Running forward searches took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run BLASTP or HMMer for searches into the .faa files (depending on whether queries are single- or multi-fasta), or TBLASTN for searches into the .fna files with any single-fasta queries.\n",
    "\n",
    "# Summarize forward search results\n",
    "\n",
    "Now we can generate a summary of the raw output files. Important criteria may be customized here as well. Specifically the forward search E-value threshold, and the maximum number of nucleotide bases allowed between TBLASTN HSPs to be considered part of the same gene (view optional arguments via the -h option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae sum_fwd_srch -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Summarize forward search results in a CSV file.\n",
    "# ***Note that only the top 5 hits for each individual search will be reported, as specified here. \n",
    "# This is simply to save time, and previous analyses have confirmed that the number of positive hits will not exceed 5 for any of the searches.\n",
    "!amoebae sum_fwd_srch $SRCHRESDIR/$FWDSRCHDIR\\\n",
    "                     $SRCHRESDIR/$FWDSRCHDIR'_sum.csv'\\\n",
    "                     --max_hits_to_sum 5\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the resulting CSV file. Note that maximum E-value cutoffs, and other criteria were applied as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the CSV file using the pandas library.\n",
    "df = pd.read_csv(os.path.join(os.environ['SRCHRESDIR'],os.environ['FWDSRCHDIR']) + '_sum.csv_out.csv')\n",
    "# Display the data in an HTML table.\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run reverse searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to determine which of the \"forward hits\" in these search results are really specific to our original A. thaliana queries, letâ€™s search with these hits as queries back into the A. thaliana genome (i.e., perform \"reverse\" searches).\n",
    "\n",
    "Similar to the forward searches, we need to first set up the reverse search directory:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%env REVSRCHDIR=rev_srch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae setup_rev_srch -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae setup_rev_srch $SRCHRESDIR\\\n",
    "                       $SRCHRESDIR/$FWDSRCHDIR'_sum.csv_out.csv'\\\n",
    "                       Athaliana_database.faa\\\n",
    "                       --outdir $SRCHRESDIR/$REVSRCHDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a new directory with \"rev_srch_\" and a timestamp in the name. Run reverse searches using the path to this directory as an input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# View reverse search directory contents.\n",
    "tree $SRCHRESDIR/$REVSRCHDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!amoebae run_rev_srch $SRCHRESDIR/$REVSRCHDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize reverse search results\n",
    "\n",
    "Now append columns summarizing the results of these reverse searches to our CSV file. This is where the file listing redundant hits for each query title is used. Also, a criterion is applied here based on the order of magnitude difference in E-value between the original query (or redundant hits) in the reverse search results compared to other hits (if present), and this can be optionally modified (view optional arguments via the -h option).\n",
    "\n",
    "This could take a while.\n",
    "\n",
    "**Error: extra quotation marks were written to the redundant hits CSV file... This is a real problem**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "CSVLIST=($REDUNHITDIR/redun_hits_*/0_redun_hits_*_2.csv)\n",
    "amoebae sum_rev_srch $SRCHRESDIR/$FWDSRCHDIR'_sum.csv_out.csv'\\\n",
    "                     $SRCHRESDIR/$REVSRCHDIR\\\n",
    "                     --redun_hit_csv ${CSVLIST[-1]}\n",
    "                     \n",
    "ELAPSED=\"Summarizing these results took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, this will output a CSV file with the same path as the forward search summary CSV file, but with a \"_1\" added before the filename extension. Examine the resulting CSV file.\n",
    "You could run additional reverse searches into different files, appending columns to the same summary spreadsheet. Reverse searches into the A. thaliana peptide sequences is all that is necessary for this tutorial.\n",
    "\n",
    "Next run the interp_srchs command to do an additional interpretation of the results (if reverse searches into multiple reference databases were performed then this would be done following summarization of all the reverse searches). Again, customized criteria may be applied at this point using the optional arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae interp_srchs $SRCHRESDIR/$FWDSRCHDIR'_sum.csv_out_1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, examine the resulting CSV file to see whether the results match your expectations. You will notice that the results in this file do not account for the fact that the HMMer, BLASTP, and TBLASTN hits are redundant in many cases as might be expected if each of these search algorithms were effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine which positive hits are redundant\n",
    "\n",
    "We need to determine which hits correspond to the same loci based on having identical accessions or being associated with the same locus in the GFF3 annotation file, or likely represent distinct paralogous gene loci based on sequence similarity in a multiple sequence alignment (see Larson et al. (2019) for explanation of how these are identified).\n",
    "To do this, first we will append a column listing what alignment to use (by default it will be the alignments that are used as queries for the corresponding query title):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "CSVLIST=( $SRCHRESDIR/${FWDSRCHDIR}_sum.csv_out_1_interp_*.csv )\n",
    "amoebae find_redun_seqs ${CSVLIST[-1]} --add_ali_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now identify distinct paralogues (use the -h option to view optional arguments):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae find_redun_seqs -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "CSVLIST=( $SRCHRESDIR/${FWDSRCHDIR}_sum.csv_out_1_interp_*_with_ali_col.csv )\n",
    "\n",
    "amoebae find_redun_seqs ${CSVLIST[-1]}\n",
    "\n",
    "ELAPSED=\"Finding redundant sequences took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output another copy of the CSV file with additional columns. Take some time to decide whether you agree with the exclusion of some of the hits, as indicated in the appended columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the CSV file using the pandas library.\n",
    "csv_file = glob.glob(os.path.join(os.environ['SRCHRESDIR'],'*_paralogue_count_*.csv'))[0]\n",
    "df = pd.read_csv(csv_file)\n",
    "# Display the data in an HTML table.\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the final search results\n",
    "\n",
    "Finally, we can plot the results of the searches. To customize the organization of the output coulson plot, an additional input CSV file may be optionally provided here. This file simply contains the names of protein complexes in the first column and query titles for proteins that you want to include in each complex in the second column (see example file provided with this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \\\n",
    "\"AP-2,AP1beta\n",
    "AP-2,AP2alpha\n",
    "AP-2,AP2mu\n",
    "AP-2,AP2sigma\n",
    "COPII,Sec12\n",
    "SNAREs,SNAP33\n",
    "Rabs,Rab2\" > $SRCHRESDIR/complex_info_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "# Problem: species name not automatically added to genome_info.csv..\n",
    "CSVLIST=( $SRCHRESDIR/${FWDSRCHDIR}_sum.csv_out_1_interp_*_with_ali_col_paralogue_count_*.csv )\n",
    "amoebae plot ${CSVLIST[-1]}\\\n",
    "             --complex_info $SRCHRESDIR/complex_info_1.csv\\\n",
    "             --out_pdf $SRCHRESDIR/plot.pdf\n",
    "\n",
    "ELAPSED=\"Plotting these results took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the resulting PDF files. Your coulson plot should look something like that in Figure 1. Compare with the results of searches for AP-2 subunits published by Manna et al. (2013), Barlow et al. (2014), and Larson et al. (2019). You will need to customize formatting of coulson plots output by the â€™plotâ€™ command using software such as Adobe Illustrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"AMOEBAE_Search_Results_1/plot_coulson_both.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: A coulson plot summarizing similarity search results for AP-2 complex subunits in Trypanosoma brucei gambiense and Saccharomyces cerevisiae peptide and nucleotide se- quences using Arabidopsis thaliana queries and Hidden Markov Models generated from align- ments of embryophyte orthologues. BLASTP and TBLASTN were used to search peptide and nucleotide sequences, respectively, with single sequence queries, and the HMMer3 pack- age was used to perform profile searches. Subplot sectors with blue fill indicate that one or more sequences were found to meet the search criteria applied (with the number being indicated within each subplot sector). Note that the ancestral eukaryotic AP-1 and AP-2 complexes shared a single beta subunit (Dacks et al., 2008). This is why identified \"AP1beta\" orthologues are shown as a component of the AP-2 complex here, even though T. brucei lacks an AP-2 complex (Manna et al., 2013). These results are comparable to the relevant results published by Manna et al. (2013), Barlow et al. (2014), and Larson et al. (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation and re-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear that AMOEBAE identifies \"positive\" and \"negative\" results simply by applying criteria that the user specifies. So, it is entirely the users responsibility to select appropriate criteria and interpret the results critically.\n",
    "\n",
    "Points to consider regarding interpretation of the results of the analysis in this tutorial include the following:\n",
    "- The BLASTP and HMMer searches (both followed by reverse BLASTP searches) yielded the same results in this analysis.\n",
    "- The TBLASTN searches were able to identify all of the genes represented by the peptide sequences identified by BLASTP and HMMer searches.\n",
    "- A TBLASTN hit in the A. thaliana chromosome 5 (NC_003076.8) met the forward and reverse search criteria, but was excluded because the translation of the region that aligned to the query was only 50 amino acids long (this sequence also contained stop codons). If you look on the NCBI genome browser for A. thaliana you will see that this region on chromosome 5 (as indicated in the summary CSV file) corresponds to a pseudogene for AP-2 sigma with the gene ID AT5G42568.\n",
    "- The two A. thaliana AP-1/2 beta paralogues and the two S. cerevisiae paralogues are brassicalid and fungal inparalogues, respectively, which arose from independent gene duplications. Phylogenetic analysis would be required to determine this (see Larson et al. (2019) and Barlow et al. (2014)).\n",
    "- An Arabidopsis thaliana AP-2 mu splice variant was excluded after running the â€™find _redun_seqsâ€™ command, because it was found to be encoded by the same gene as the other splice variant based on information in the GFF3 annotation file.\n",
    "- An A. thaliana AP-2 alpha gene was excluded after running the â€™find_redun_seqsâ€™ command, because it shows over 98% identity with the other AP-2 alpha gene. The summary CSV file indicates which file contains an alignment of these two sequences (see Larson et al. (2019) for relevant discussion).\n",
    "\n",
    "If the analysis in this tutorial were a project you were working on for publication, then upon completing the above analysis steps you work would have only just begun. AMOEBAE merely finds sequences that match your specified search criteria, which may or may not be sufficient to accurately identify homologues of interest. Careful inspection of the summary CSV file will reveal that minor adjustments to the search criteria would cause the analysis to yield different results. Moreover, there are many different possibilities that would lead to innacurate results based on the criteria applied in the above analysis. A comprehensive discussion of this is beyond the scope of this tutorial, but one obvious example would be if an identified sequence contained a domain that was not present in the query sequence, causing sequences to be retrieved in the reverse search with no homology to the original query. Therefore, it is recommended that you commit to an iterative approach to analysis involving adjustment of search criteria and re-analysis to include sequences that you know are homologues of interest, but to exclude those that you know are not homologues of interest.\n",
    "\n",
    "To generate an alignment of similar sequences identified using AMOEBAE, use the â€™csv_to _fastaâ€™ command to generate FASTA files for alignment, and then align using your preferred software (e.g., MUSCLE or MAFFT). For visually assessing the sequences for possible issues such as contrasting domain topologies, you may wish to generate FASTA files including all your forward search results for each query title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "CSVLIST=( $SRCHRESDIR/${FWDSRCHDIR}_sum.csv_out_1_interp_*_with_ali_col_paralogue_count_*.csv )\n",
    "\n",
    "amoebae csv_to_fasta ${CSVLIST[-1]} --all_hits --split_by_query_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are planning to run a phylognetic analysis, you may wish to generate a FASTA file with only those sequences that match all your search criteria, and with abbreviated sequence headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "CSVLIST=( $SRCHRESDIR/${FWDSRCHDIR}_sum.csv_out_1_interp_*_with_ali_col_paralogue_count_*.csv )\n",
    "\n",
    "amoebae csv_to_fasta ${CSVLIST[-1]} --abbrev --split_by_query_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete search output files (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete temporary files.\n",
    "rm -r temporary_alignment_dir\n",
    "rm -r temporary_db_dir\n",
    "rm -r temporary_query_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete all data and results files (WARNING you may want to keep these!).\n",
    "rm -r AMOEBAE_Data\n",
    "rm -r AMOEBAE_Search_Results_1\n",
    "rm -r Redundant_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to go from here?\n",
    "\n",
    "You can customize this notebook to search with different queries in different genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Barlow, L.D., Dacks, J.B., Wideman, J.G., 2014. From all to (nearly) none: Tracing adaptin evolution in Fungi. Cellular Logistics 4, e28114. https://doi.org/10.4161/cl.28114\n",
    "\n",
    "Hirst, J., D. Barlow, L., Francisco, G.C., Sahlender, D.A., Seaman, M.N.J., Dacks, J.B., Robinson, M.S., 2011. The Fifth Adaptor Protein Complex. PLoS Biology 9, e1001170. https://doi.org/10.1371/journal.pbio.1001170\n",
    "\n",
    "Larson, R.T., Dacks, J.B., Barlow, L.D., 2019. Recent gene duplications dominate evolutionary dynamics of adaptor protein complex subunits in embryophytes. Traffic 20, 961â€“973. https://doi.org/10.1111/tra.12698\n",
    "\n",
    "Manna, P.T., Kelly, S., Field, M.C., 2013. Adaptin evolution in kinetoplastids and emergence of the variant surface glycoprotein coat in African trypanosomatids. Molecular Phylogenetics and Evolution 67, 123â€“128. https://doi.org/10.1016/j.ympev.2013.01.002\n",
    "\n",
    "Robinson, M.S., 2004. Adaptable adaptors for coated vesicles. Trends in Cell Biology 14, 167â€“174. https://doi.org/10.1016/j.tcb.2004.02.002\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
