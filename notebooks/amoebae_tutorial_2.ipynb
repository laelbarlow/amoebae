{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial will walk you through a preliminary similarity searching analysis making use of scripts in the AMOEBAE toolkit.\n",
    "While AMOEBAE was not originally written to be used via the command line, Jupyter notebooks provide an easy means of guiding new users through an example analysis with limited need for manual input.\n",
    "The end result of running this code successfully is a spreadsheet summarizing results of similarity searches, as well as a plot to visualize the results.\n",
    "\n",
    "\n",
    "As a simple example, we will consider the the distribution of orthologues of subunits of the Adaptor Protein (AP) 2 vesicle adaptor complex, and several other membrane-trafficking proteins, in five model eukaryotes: the plant *Arabidopsis thaliana*, the yeast *Saccharomyces cerevisiae*, the fungus *Allomyces macrogynus*, the amoeba *Dictyostelium discoideum*, and the pathogenic protist *Trypanosoma brucei*. AP-2 subunits are homologous to subunits of other AP complexes (Robinson, 2004; Hirst et al., 2011), and published work has traced their evolution among plants (Larson et al., 2019), Fungi (Barlow et al., 2014), and trypanosomatid parasites (Manna et al., 2013). Thus, the protein subunits of the AP-2 complex provide a useful test of similarity searching methods to distinguish between orthologues and paralogues, which can be compared to the results of previous studies. In addition, the membrane trafficking proteins Sec12 (a component of the COPII vesicle coat complex), SNAP33 (a Qbc-SNARE), and Rab2 (a small GTPase) are included to further explore the potential sources of error involved in identification of orthologous proteins. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Objectives of this tutorial\n",
    "\n",
    "\n",
    "-  Perform similarity searches using the BLASTP, TBLASN, HMMer algorithms simultaneously using AMOBEAE scripts.\n",
    "\n",
    "-  Apply a reciprocal-best-hit search strategy using AMOEBAE code.\n",
    "\n",
    "- Practice interpreting similarity search results obtained using AMOEBAE.\n",
    " \n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Before running this code, you will need to have set up AMOEBAE according to the instructions in the main documentation file here (which you likely have already done): [AMOEBAE_documentation.pdf](\n",
    "https://github.com/laelbarlow/amoebae/blob/master/documentation/AMOEBAE_documentation.pdf).\n",
    "\n",
    "- MacOS or Linux operating system (or possibly a work-around on windows, although this has not been tested).\n",
    "\n",
    "- Approximately 3GB of storage space.\n",
    "\n",
    "- An internet connection.\n",
    "\n",
    "- At least an hour of your time (the code in this notebook will take approximately 60 minutes to run).\n",
    "\n",
    "- Running the code in this notebook is more computationally intensive than webbrowsing for example, so if you are running this on a laptop computer, then make sure it is connected to an electrical outlet.\n",
    "\n",
    "## Testing\n",
    "If you wish to simply run all the code in this notebook for testing purposes:\n",
    "\n",
    "- First, modify the cell in the section labeled \"Enter your email to access the NCBI protein database via NCBI Entrez\" below such that the value of Entrez.email is hard-coded as your email address.\n",
    "\n",
    "- Then select \"Cell\" > \"Run All\" from the Jupyter menu above.\n",
    "\n",
    "- Alternatively, close this browser window, navigate to the directory in the container in which this notebook runs, and use the runipy program to run the notebook as follows:\n",
    "    \n",
    "        runipy -o amoebae_tutorial_2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary steps\n",
    "\n",
    "## Check that dependencies are installed\n",
    "You should have already pulled the amoebae git repository to your computer as described in the main documentation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This command simply prints the versions of some dependencies which are now available for use by amoebae.\n",
    "amoebae check_depend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This command tests all the import statements in amoebae modules.\n",
    "amoebae check_imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some basic python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "from Bio import SeqIO\n",
    "from Bio import Entrez\n",
    "import glob\n",
    "from Bio.Blast import NCBIXML\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Image\n",
    "sys.path.append('/opt/notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the specific version of AMOEBAE code used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record git repository version information.\n",
    "wd = !pwd\n",
    "script_dir = wd[0] \n",
    "git_hash = str(subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=script_dir).strip())\n",
    "git_branch = str(subprocess.check_output([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=script_dir).strip())  \n",
    "print('\\nGit repository (code) version: ' + git_hash + ' (branch name: ' + git_branch + ')\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up sequence databases for searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download peptide and nucleotide sequences for specific genomes.\n",
    "\n",
    "Let's download the predicted peptide sequences, genomic assembly (nucleotide\n",
    "sequences of assembled chromosomes), and annotation files (in GFF3 format) for the following eukaryotes from NCBI:\n",
    "\n",
    "- *Arabidopsis thaliana*\n",
    "- *Trypanosoma brucei*\n",
    "- *Dictyostelium discoideum*\n",
    "- *Allomyces macrogynus*\n",
    "- *Saccharomyces cerevisiae*\n",
    "\n",
    "\n",
    "This will take approximately 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initiate a list of file paths for downloaded sequence and annotation files.\n",
    "datafile_path_list = []\n",
    "\n",
    "# Define a dictionary of source URLs and new filenames for sequence and annotation files.\n",
    "# Note that the filenames (besides extension) are the species name with underscores instead of spaces.\n",
    "datafile_dict = {\"Arabidopsis_thaliana.faa\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_protein.faa.gz\",\n",
    "                 \"Arabidopsis_thaliana.fna\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_genomic.fna.gz\",\n",
    "                 \"Arabidopsis_thaliana.gff3\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_genomic.gff.gz\",\n",
    "                 \"Saccharomyces_cerevisiae.faa\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_protein.faa.gz\",\n",
    "                 \"Saccharomyces_cerevisiae.fna\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_genomic.fna.gz\",\n",
    "                 \"Saccharomyces_cerevisiae.gff3\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_genomic.gff.gz\",\n",
    "                 \"Trypanosoma_brucei.faa\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/210/295/GCF_000210295.1_ASM21029v1/GCF_000210295.1_ASM21029v1_protein.faa.gz\",\n",
    "                 \"Trypanosoma_brucei.fna\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/210/295/GCF_000210295.1_ASM21029v1/GCF_000210295.1_ASM21029v1_genomic.fna.gz\",\n",
    "                 \"Trypanosoma_brucei.gff3\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/210/295/GCF_000210295.1_ASM21029v1/GCF_000210295.1_ASM21029v1_genomic.gff.gz\",\n",
    "                 \"Dictyostelium_discoideum.faa\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/004/695/GCF_000004695.1_dicty_2.7/GCF_000004695.1_dicty_2.7_protein.faa.gz\",\n",
    "                 \"Dictyostelium_discoideum.fna\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/004/695/GCF_000004695.1_dicty_2.7/GCF_000004695.1_dicty_2.7_genomic.fna.gz\",\n",
    "                 \"Dictyostelium_discoideum.gff3\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/004/695/GCF_000004695.1_dicty_2.7/GCF_000004695.1_dicty_2.7_genomic.gff.gz\",\n",
    "                 \"Allomyces_macrogynus.faa\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/151/295/GCA_000151295.1_A_macrogynus_V3/GCA_000151295.1_A_macrogynus_V3_protein.faa.gz\",\n",
    "                 \"Allomyces_macrogynus.fna\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/151/295/GCA_000151295.1_A_macrogynus_V3/GCA_000151295.1_A_macrogynus_V3_genomic.fna.gz\",\n",
    "                 \"Allomyces_macrogynus.gff3\": \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/151/295/GCA_000151295.1_A_macrogynus_V3/GCA_000151295.1_A_macrogynus_V3_genomic.gff.gz\"\n",
    "          }\n",
    "\n",
    "# Make a new temporary directory to store data files.\n",
    "temp_db_dir_name = 'temporary_db_dir'\n",
    "if not os.path.isdir(temp_db_dir_name):\n",
    "    os.mkdir(temp_db_dir_name)\n",
    "\n",
    "# Download all the data files via NCBI's FTP server.\n",
    "for filename in datafile_dict.keys():\n",
    "    url = datafile_dict[filename]\n",
    "    filepath = os.path.join(temp_db_dir_name, filename)\n",
    "    if not os.path.isfile(filepath):\n",
    "        subprocess.call(['curl', url, '--output', filepath + '.gz'])\n",
    "        subprocess.call(['gunzip', filepath + '.gz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate a data directory structure\n",
    "To generate a directory structure and spreadsheets for storing formatted sequence files\n",
    "and metadata for each sequence file, use the 'mkdatadir' command (this takes a\n",
    "single argument which is the full path that you want your new directory to be\n",
    "written to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env DATADIR=AMOEBAE_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae mkdatadir $DATADIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will prompt you to set the 'root\\_amoebae\\_data\\_dir' variable in the\n",
    "settings.py file to this new directory path so that AMOEBAE scripts can locate\n",
    "your files.\n",
    "\n",
    "This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check that the path indicated in the settings file is correct.\n",
    "import settings\n",
    "print(settings.root_amoebae_data_dir)\n",
    "assert settings.root_amoebae_data_dir == \"AMOEBAE_Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare databases for searching\n",
    "To generate a directory structure and spreadsheets for storing formatted sequence files\n",
    "and metadata for each sequence file, use the 'mkdatadir' command (this takes a\n",
    "single argument which is the full path that you want your new directory to be\n",
    "written to).\n",
    "\n",
    "This will take at least 11 minutes, because the FASTA files need to be re-written with re-formatted sequence headers and the GFF3 files need to be converted to SQL databases using gffutils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "for X in temporary_db_dir/*; do amoebae add_to_dbs $X; done\n",
    "\n",
    "ELAPSED=\"Preparing sequence databases for searching took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List the databases now accessible by AMOEBAE.\n",
    "amoebae list_dbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding each file to the AMOEBAE_Data directory, a line is added to the 0_genome_info.csv file with information describing this file. Information from this CSV file is used in downstream analysis steps, so it is important to ensure that it is accurate. \n",
    "\n",
    "You can view the contents of this CSV file here (empty fields are displayed as \"NaN\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the path to the CSV file.\n",
    "csv_file_path = os.path.join(os.path.join(os.environ['DATADIR'], 'Genomes'), '0_genome_info.csv')\n",
    "# Load data from the CSV file using the pandas library.\n",
    "df = pd.read_csv(csv_file_path)\n",
    "# Display the data in an HTML table.\n",
    "print(\"Contents of the file %s:\" % csv_file_path)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommended that you add any relevant information to complete this table, although this is not necessary to complete this tutorial. This can serve as a useful record for your own reference, as well as as a supplementary file in publications.\n",
    "\n",
    "By default, AMOEBAE assumes that GFF3 annotation files will be given the same filename (besides the extension) as the nucleotide FASTA file containing the sequences that the annotations are for. For example, to retrieve annotations for assembled chromosome sequences in a file named \"Arabidopsis_thaliana.fna\", AMOEBAE will look for a file named \"Arabidopsis_thaliana.sql\" (which is generated using a file with the name \"Arabidopsis_thaliana.gff3). If you wish to use different annotation files, open the CSV file in a spreadsheet program such as Excel or Open Office then copy the name of the .sql file to the row for the corresponding genomic assembly (.fna) file in the column with the header \"Annotations file\", and do the same for the row describing the corresponding peptide sequence (.faa) file. This allows the correct GFF3 file to be used for the assembly (.fna file) and predicted amino acid sequences (.faa).\n",
    "\n",
    "Taxonomic information for each genome is arbitrarily divided into four hierarchical categories: \"Superbranch\", \"Supergroup\", \"Group\", and \"Species (if applicable)\". For files containing sequences or annotations for *Arabidopsis thaliana*, I would enter the values \"Diaphoretickes\", \"Archaeplastida\", \"Embryophyta\", and \"Arabidopsis thaliana\", respectively. These are arbitrary selected taxonomic groups to which Arabidopsis belongs (Adl et al., 2018). This can be useful when you have many genomes represented in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up queries\n",
    "\n",
    "## Enter your email to access the NCBI protein database via NCBI Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out this line and use the line at the bottom of this cell instead, if you want to run all cells at once.\n",
    "Entrez.email = input(\"Enter your email address here: \")  # Tell NCBI who you are.\n",
    "\n",
    "# Use the line at the top of this cell instead.\n",
    "#Entrez.email = \"yourname@email.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download single-sequence queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define a dictionary with NCBI sequence accessions as keys and filenames to write\n",
    "# the corresponding sequences to as values.\n",
    "query_dict = {\"NP_194077.1\": \"AP1beta_Athaliana_NP_194077.1_query.faa\",\n",
    "              \"NP_851058.1\": \"AP2alpha_Athaliana_NP_851058.1_query.faa\",\n",
    "              \"NP_974895.1\": \"AP2mu_Athaliana_NP_974895.1_query.faa\",\n",
    "              \"NP_175219.1\": \"AP2sigma_Athaliana_NP_175219.1_query.faa\",\n",
    "              \"NP_566961.1\": \"Sec12_Athaliana_NP_566961.1_query.faa\",\n",
    "              \"NP_200929.1\": \"SNAP33_Athaliana_NP_200929.1_query.faa\",\n",
    "              \"NP_193449.1\": \"Rab2_Athaliana_NP_193449.1_query.faa\"\n",
    "          }\n",
    "\n",
    "# Make a new temporary directory to store sequence files.\n",
    "temp_query_dir_name = 'temporary_query_dir'\n",
    "if not os.path.isdir(temp_query_dir_name):\n",
    "    os.mkdir(temp_query_dir_name)\n",
    "\n",
    "# Loop over keys in the query_dict dictionary.\n",
    "for accession in query_dict.keys():\n",
    "    # Retrieve the corresponding filename from the dictionary.\n",
    "    filename = query_dict[accession]\n",
    "    filepath = os.path.join(temp_query_dir_name, filename)\n",
    "    # Only download sequences that have not already been downloaded.\n",
    "    if not os.path.isfile(filepath):\n",
    "        # Download the sequence from NCBI via Entrez, using the Biopython module.\n",
    "        net_handle = Entrez.efetch(db=\"protein\", id=accession, rettype=\"fasta\", retmode=\"text\")\n",
    "        out_handle = open(filepath, \"w\")\n",
    "        out_handle.write(net_handle.read())\n",
    "        out_handle.close()\n",
    "        net_handle.close()\n",
    "    # Check that the sequence was actually downloaded.\n",
    "    assert os.path.isfile(filepath), \"\"\"The sequence with the following accession could not be downloaded from NCBI: %s\\n\n",
    "    Try re-running this cell.\"\"\" % accession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare single-sequence queries for searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries must be formatted and stored in a similar manner to genomic data files. The query files will include FASTA files containing one sequence and FASTA files containing multiple sequences.\n",
    "Now we are going to generate the query files and add them to your AMOEBAE_Data/ Queries directory, in a similar way to how we added genomic data files to the AMOEBAE_Data/Genomes directory. Since you already downloaded all the peptide sequences for Arabidopsis thaliana, you can retrieve these from your downloaded data using one of the scripts in the amoebae/misc_scripts folder. First, let’s generate a query for the A. thaliana AP-1/2 beta subunit(s), which is a component of both the AP-1 and AP-2 complexes, using a representative sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "for QUERYFILE in temporary_query_dir/*.faa; do amoebae add_to_queries $QUERYFILE; done\n",
    "\n",
    "ELAPSED=\"Preparing query sequences for searching took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae list_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct alignments for profile similarity searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define a dictionary of NCBI sequence accessions and filenames to which to write the corresponding sequences.\n",
    "query_title_dict = {\"AP1beta\": \"NP_194077.1,CBI34366.3,XP_015631818.1,XP_024516549.1,OAE33273.1\",\n",
    "                    \"AP2alpha\": \"NP_851058.1,XP_002270388.1,XP_015631820.1,PTQ35247.1,XP_024525508.1\",\n",
    "                    \"AP2mu\": \"NP_974895.1,XP_002281297.1,XP_015627628.1,OAE25965.1,XP_002973295.1\",\n",
    "                    \"AP2sigma\": \"NP_175219.1,XP_015618362.1,PTQ50284.1,XP_002275803.1,XP_024518676.1\",\n",
    "                    \"Sec12\": \"NP_566961.1,XP_002262948.1,XP_015647566.1,OAE21792.1,XP_024530559.1\",\n",
    "                    \"SNAP33\": \"NP_200929.1,XP_002284486.1,AAW82752.1,EFJ31467.1,OAE29824.1,XP_006270633.1,XP_006010378.1,XP_006625751.1,NP_001080510.1,XP_020370357.1,XP_015181699.1,XP_031769811.1\",\n",
    "                    \"Rab2\": \"NP_193449.1,XP_003635585.2,XP_015626284.1,XP_002965710.1,PTQ28228.1\"\n",
    "                   }\n",
    "                    \n",
    "\n",
    "# Make a new temporary directory to store sequence files.\n",
    "temp_alignment_dir_name = 'temporary_alignment_dir'\n",
    "assert not os.path.isdir(temp_alignment_dir_name), \"\"\"Directory already exists.\"\"\"\n",
    "os.mkdir(temp_alignment_dir_name)\n",
    "\n",
    "# Download query sequences and write to multiple-sequence FASTA files.\n",
    "for query_title in query_title_dict.keys():\n",
    "    accession_list_string = query_title_dict[query_title]\n",
    "    filepath = os.path.join(temp_alignment_dir_name, query_title + '_hmm1.faa')\n",
    "    if not os.path.isfile(filepath):\n",
    "        net_handle = Entrez.efetch(db=\"protein\", id=accession_list_string, rettype=\"fasta\", retmode=\"text\")\n",
    "        out_handle = open(filepath, \"w\")\n",
    "        out_handle.write(net_handle.read())\n",
    "        out_handle.close()\n",
    "        net_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "for X in temporary_alignment_dir/*.faa; do amoebae align_fa $X --output_format fasta; done\n",
    "\n",
    "ELAPSED=\"Aligning FASTA files took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the AMOEBAE_Data/Genomes/0_genome_info.csv file, the AMOEBAE_Data/Queries/0_query_info.csv file contains information about each query file, which can be manually edited. One of the most important pieces of information is the \"Query title\". Different query files, such as a single FASTA sequence and an HMM, can have the same Query title if they are to be used to search for homologues or orthologues of the same protein(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the path to the CSV file.\n",
    "csv_file_path = os.path.join(os.path.join(os.environ['DATADIR'], 'Queries'), '0_query_info.csv')\n",
    "# Load data from the CSV file using the pandas library.\n",
    "df = pd.read_csv(csv_file_path)\n",
    "# Display the data in an HTML table.\n",
    "print(\"Contents of the file %s:\" % csv_file_path)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visually inspect alignments\n",
    "Alignments used as queries should be visually inspected to make sure that there are no obvious errors in the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "for QUERYFILE in temporary_alignment_dir/*.afaa; do amoebae afa_to_nex $QUERYFILE; done\n",
    "echo \"Alignments to observe:\"\n",
    "ls temporary_alignment_dir/*.nex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare query alignments for searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "for QUERYFILE in temporary_alignment_dir/*.afaa; do amoebae add_to_queries $QUERYFILE; done\n",
    "\n",
    "ELAPSED=\"Preparing HMM queries from alignments took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae list_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate lists of potential redundant sequences among *A. thaliana* peptide sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, a reciprocal-best-hit search strategy will be used. If you are using a reciprocal- best-hit search strategy, then your initial round of searches will be performed using your original queries (assembled above) to search your genomes of interest. This initial round of searches will be referred to herein as \"forward searches\", and subsequent searches using forward search hits as queries into reference genomes will be referred to as \"reverse searches\".\n",
    "\n",
    "A slight complication to this search strategy is that the NCBI RefSeq peptide sequences for the *A. thaliana* genome include alternative transcripts and lineage-specific inparalogues (as do other databases), implying that if these were retrieved as the top hits in the reverse searches instead of the original query sequence, then this would still potentially be a positive result. So, to properly interpret reverse search results it will be necessary to determine which sequences in our A. thaliana.faa file are redundant for our purposes. To do this we will use the get_redun_hits command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Optional. Get the help output for the get_redun_hits command.\n",
    "amoebae get_redun_hits -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env REDUNHITDIR=Redundant_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will take approximately 5 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "# Make a directory to store information about redundant hits.\n",
    "mkdir $REDUNHITDIR\n",
    "\n",
    "# Write a file listing names of query files to be used.\n",
    "amoebae list_queries > $REDUNHITDIR/queries.txt\n",
    "\n",
    "# Use AMOEBAE to retrieve potential redundant hit sequences.\n",
    "amoebae get_redun_hits $REDUNHITDIR --query_list_file $REDUNHITDIR/queries.txt --db_name Arabidopsis_thaliana.faa\n",
    "\n",
    "ELAPSED=\"Retrieving potentially redundant sequences took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a directory in the Redundant_hits folder with a .csv file. This file contains a summary of BLASTP or HMMer search results for searches with the specified queries into the *A. thaliana* predicted proteins. \n",
    "\n",
    "It should be apparent upon inspection of the ranking of hits and comparison of the associated E-values which hits are likely redundant with your queries (see cell below). To tell AMOEBAE which hits you want to consider as redundant for the purposes of downstream steps, the values in the column with the header \"Positive/redundant (+) or negative (-) hit for queries with query title (edit this column)\" must be changed from ’-’ to ’+’ for hits that are redundant (this will be done automatically for this tutorial (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the path to the CSV file.\n",
    "csv_file_path = glob.glob(os.path.join('Redundant_hits', os.path.join('redun_hits_*', '0_redun_hits_*.csv')))[0]\n",
    "# Load data from the CSV file using the pandas library.\n",
    "df = pd.read_csv(csv_file_path)\n",
    "# Display the data in an HTML table.\n",
    "print(\"Contents of the file %s:\" % csv_file_path)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify redundant sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Define a dictionary with query titles as keys and lists of sequence IDs as values, where the IDs are for A. thaliana sequences that are redundant with the original A. thaliana query sequence.\n",
    "redun_seq_dict = {\"AP1beta\":  [\"NP_194077.1\",\n",
    "                               \"NP_192877.1\",\n",
    "                               \"NP_001328014.1\",\n",
    "                               \"NP_001190701.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"AP2alpha\": [\"NP_851058.1\",\n",
    "                               \"NP_851057.1\",\n",
    "                               \"NP_197669.1\",\n",
    "                               \"NP_001330971.1\",\n",
    "                               \"NP_001330970.1\",\n",
    "                               \"NP_001330969.1\",\n",
    "                               \"NP_197670.1\",\n",
    "                               \"NP_001330127.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"AP2mu\":    [\"NP_974895.1\",\n",
    "                               \"NP_199475.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"AP2sigma\": [\"NP_175219.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"Sec12\":    [\"NP_566961.1\",\n",
    "                               \"NP_568738.1\",\n",
    "                               \"NP_680414.1\",\n",
    "                               \"NP_178256.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"SNAP33\":   [\"NP_200929.1\",\n",
    "                               \"NP_001332102.1\",\n",
    "                               \"NP_172842.1\",\n",
    "                               \"NP_001318998.1\",\n",
    "                               \"NP_196405.1\",\n",
    "                               \"NP_001318503.1\"\n",
    "                               ],\n",
    "                  \n",
    "                  \"Rab2\":     [\"NP_193449.1\",\n",
    "                               \"NP_193450.1\",\n",
    "                               \"NP_195311.1\",\n",
    "                               \"NP_001078499.1\"\n",
    "                               ]\n",
    "                   }\n",
    "\n",
    "\n",
    "# Identify path to redundant seqs CSV file.\n",
    "redundant_seqs_csv = glob.glob(os.path.join('Redundant_hits', os.path.join('redun_hits_*', '0_redun_hits_*.csv')))[0]\n",
    "\n",
    "# Define path for new modified redundant seqs CSV file.\n",
    "redundant_seqs_csv2 = redundant_seqs_csv.rsplit(\".\", 1)[0] + '_2.csv'\n",
    "\n",
    "# Open the redundant seqs CSV file, and a new one.\n",
    "with open(redundant_seqs_csv) as infh, open(redundant_seqs_csv2, 'w') as o:\n",
    "    # Loop over lines in the CSV file.\n",
    "    for i in infh:\n",
    "        if not i.startswith(\"Query Title\"):\n",
    "            # Identify query title in line.\n",
    "            line_query_title = i.split(',')[0].strip()\n",
    "            # Identify accession/id for sequence hit represented in this row.\n",
    "            line_accession = i.split(',')[9].strip().strip('\\\"')\n",
    "            # Loop over keys (query titles) in the redundant seqs dictionary.\n",
    "            query_title_in_keys = False\n",
    "            for query_title in redun_seq_dict.keys():\n",
    "                if line_query_title == query_title:\n",
    "                    query_title_in_keys = True\n",
    "                    #print('YYY')\n",
    "                    #print(line_accession)\n",
    "                    #print(redun_seq_dict[line_query_title])\n",
    "                    # Determine whether the accession is a redundant accession.\n",
    "                    for acc in redun_seq_dict[line_query_title]:\n",
    "                        #print(line_accession, acc)\n",
    "                        if line_accession == acc:\n",
    "                            # Change the - to + so that the accession will be included in the list of redundant accessions used by AMOEBAE.\n",
    "                            i = ','.join(i.split(',')[:4]) + ',+,' + ','.join(i.split(',')[5:])\n",
    "                            break\n",
    "                # Break loop if the corresponding query title was found already.\n",
    "                if query_title_in_keys:\n",
    "                    break\n",
    "            # Check that a query title could be recognized as one that is a key in the dictionary.\n",
    "            assert query_title_in_keys, \"\"\"Could not find query title %s in dictionary.\"\"\" % line_query_title\n",
    "        # Write (modified) line to new CSV file.\n",
    "        o.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the file has been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the path to the CSV file.\n",
    "csv_file_path = glob.glob(os.path.join('Redundant_hits', os.path.join('redun_hits_*', '0_redun_hits_*2.csv')))[0]\n",
    "# Load data from the CSV file using the pandas library.\n",
    "df = pd.read_csv(csv_file_path)\n",
    "# Display the data in an HTML table.\n",
    "print(\"Contents of the file %s:\" % csv_file_path)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run forward searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin searching, make a new folder to contain search results, and write text files listing the names (not full paths) of FASTA files you want to use as queries and those that you want to search in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env SRCHRESDIR=AMOEBAE_Search_Results_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Make a new directory to contain search results.\n",
    "mkdir $SRCHRESDIR\n",
    "# Write query and database list files.\n",
    "amoebae list_queries > $SRCHRESDIR/queries.txt\n",
    "amoebae list_dbs > $SRCHRESDIR/databases.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up searches using the setup_fwd_srch command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Optional. Get the help output for the setup_fwd_srch command.\n",
    "amoebae setup_fwd_srch -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%env FWDSRCHDIR=fwd_srch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set up forward searches.\n",
    "amoebae setup_fwd_srch $SRCHRESDIR\\\n",
    "                       $SRCHRESDIR/queries.txt\\\n",
    "                       $SRCHRESDIR/databases.txt\\\n",
    "                       --outdir $SRCHRESDIR/$FWDSRCHDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a new sub-directory with a name that starts with \"fwd_srch_\". Now run the searches with this directory as input via the run_fwd_srch command. Forward search criteria may be selected at this point (view the relevant optional arguments via the -h option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tree $SRCHRESDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "# Run forward searches. This could take a while.\n",
    "amoebae run_fwd_srch $SRCHRESDIR/$FWDSRCHDIR\n",
    "\n",
    "ELAPSED=\"Running forward searches took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run BLASTP or HMMer for searches into the .faa files (depending on whether queries are single- or multi-fasta), or TBLASTN for searches into the .fna files with any single-fasta queries.\n",
    "\n",
    "# Summarize forward search results\n",
    "\n",
    "Now we can generate a summary of the raw output files. Important criteria may be customized here as well. Specifically the forward search E-value threshold, and the maximum number of nucleotide bases allowed between TBLASTN HSPs to be considered part of the same gene (view optional arguments via the -h option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae sum_fwd_srch -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Summarize forward search results in a CSV file.\n",
    "# ***Note that only the top 5 hits for each individual search will be reported, as specified here. \n",
    "# This is simply to save time, and previous analyses have confirmed that the number of positive hits will not exceed 5 for any of the searches.\n",
    "!amoebae sum_fwd_srch $SRCHRESDIR/$FWDSRCHDIR\\\n",
    "                     $SRCHRESDIR/$FWDSRCHDIR'_sum.csv'\\\n",
    "                     --max_hits_to_sum 5\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the resulting CSV file. Note that maximum E-value cutoffs, and other criteria were applied as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the CSV file using the pandas library.\n",
    "df = pd.read_csv(os.path.join(os.environ['SRCHRESDIR'],os.environ['FWDSRCHDIR']) + '_sum.csv_out.csv')\n",
    "# Display the data in an HTML table.\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequent steps in this tutorial will result in copies of this spreadsheet with appended columns to describe further results relevant to each forward search hit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run reverse searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to determine which of the \"forward hits\" in these search results are really specific to our original A. thaliana queries, let’s search with these hits as queries back into the A. thaliana genome (i.e., perform \"reverse\" searches).\n",
    "\n",
    "Similar to the forward searches, we need to first set up the reverse search directory:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%env REVSRCHDIR=rev_srch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae setup_rev_srch -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Important: the --aasubseq option is used here.\n",
    "amoebae setup_rev_srch $SRCHRESDIR\\\n",
    "                       $SRCHRESDIR/$FWDSRCHDIR'_sum.csv_out.csv'\\\n",
    "                       Arabidopsis_thaliana.faa\\\n",
    "                       --outdir $SRCHRESDIR/$REVSRCHDIR\\\n",
    "                       --aasubseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a new directory with \"rev_srch_\" and a timestamp in the name. Run reverse searches using the path to this directory as an input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# View reverse search directory contents.\n",
    "tree $SRCHRESDIR/$REVSRCHDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running reverse searches will take approximately 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!amoebae run_rev_srch $SRCHRESDIR/$REVSRCHDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize reverse search results\n",
    "\n",
    "Now append columns summarizing the results of these reverse searches to our CSV file. This is where the file listing redundant hits for each query title is used. Also, a criterion is applied here based on the order of magnitude difference in E-value between the original query (or redundant hits) in the reverse search results compared to other hits (if present), and this can be optionally modified (view optional arguments via the -h option).\n",
    "\n",
    "This could take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae sum_rev_srch -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing reverse search results should take approximately 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "# Important: The --aasubseq option is used here, because it was used when the setup_rev_srch command was run above.\n",
    "CSVLIST=($REDUNHITDIR/redun_hits_*/0_redun_hits_*_2.csv)\n",
    "amoebae sum_rev_srch $SRCHRESDIR/$FWDSRCHDIR'_sum.csv_out.csv'\\\n",
    "                     $SRCHRESDIR/$REVSRCHDIR\\\n",
    "                     --redun_hit_csv ${CSVLIST[-1]}\\\n",
    "                     --aasubseq\\\n",
    "                     --min_evaldiff 2\n",
    "                     \n",
    "                     \n",
    "ELAPSED=\"Summarizing these results took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, this will output a CSV file with the same path as the forward search summary CSV file, but with a \"_1\" added before the filename extension. Examine the resulting CSV file.\n",
    "You could run additional reverse searches into different files, appending columns to the same summary spreadsheet. Reverse searches into the A. thaliana peptide sequences is all that is necessary for this tutorial.\n",
    "\n",
    "Next run the interp_srchs command to do an additional interpretation of the results (if reverse searches into multiple reference databases were performed then this would be done following summarization of all the reverse searches). Again, customized criteria may be applied at this point using the optional arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae interp_srchs $SRCHRESDIR/$FWDSRCHDIR'_sum.csv_out_1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, examine the resulting CSV file to see whether the results match your expectations. You will notice that the results in this file do not account for the fact that the HMMer, BLASTP, and TBLASTN hits are redundant in many cases as might be expected if each of these search algorithms were effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine which positive hits are redundant\n",
    "\n",
    "We need to determine which hits likely correspond to the same loci based on having identical accessions or being associated with the same locus in the GFF3 annotation file, or likely represent distinct paralogous gene loci based on sequence similarity in a multiple sequence alignment (see Larson et al. (2019) for explanation of how these are identified).\n",
    "To do this, first we will append a column listing what alignment to use (by default it will be the alignments that are used as queries for the corresponding query title):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae find_redun_seqs -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "CSVLIST=( $SRCHRESDIR/${FWDSRCHDIR}_sum.csv_out_1_interp_*.csv )\n",
    "amoebae find_redun_seqs ${CSVLIST[-1]} --add_ali_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now identify distinct paralogues. This should take about 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0\n",
    "\n",
    "CSVLIST=( $SRCHRESDIR/${FWDSRCHDIR}_sum.csv_out_1_interp_*_with_ali_col.csv )\n",
    "\n",
    "amoebae find_redun_seqs ${CSVLIST[-1]}\n",
    "\n",
    "ELAPSED=\"Finding redundant sequences took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output another copy of the CSV file with additional columns. Take some time to decide whether you agree with the exclusion of some of the hits, as indicated in the appended columns. You may wish to include an abridged version of this type of final output table as a supplementary file for publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the CSV file using the pandas library.\n",
    "csv_file = glob.glob(os.path.join(os.environ['SRCHRESDIR'],'*_paralogue_count_*.csv'))[0]\n",
    "df = pd.read_csv(csv_file)\n",
    "# Display the data in an HTML table.\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the final search results\n",
    "\n",
    "Finally, we can plot the results of the searches. To customize the organization of the output coulson plot, an additional input CSV file may be optionally provided here. This file simply contains the names of protein complexes in the first column and query titles for proteins that you want to include in each complex in the second column (see example file provided with this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Write a file indicating how columns in the output coulson plot should be constructed.\n",
    "echo \\\n",
    "\"AP-2,AP1beta\n",
    "AP-2,AP2alpha\n",
    "AP-2,AP2mu\n",
    "AP-2,AP2sigma\n",
    "COPII,Sec12\n",
    "SNAREs,SNAP33\n",
    "Rabs,Rab2\" > $SRCHRESDIR/complex_info_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a file indicating the order in which results should be displayed.\n",
    "with open(os.path.join(os.environ['SRCHRESDIR'], \"databases.txt\")) as infh,\\\n",
    "open(os.path.join(os.environ['SRCHRESDIR'], \"coulson_row_order.txt\"), 'w') as o:\n",
    "    lines = infh.readlines()\n",
    "    order_list = [\"Arabidopsis\",\n",
    "                  \"Saccharomyces\",\n",
    "                  \"Allomyces\",\n",
    "                  \"Dictyostelium\",\n",
    "                  \"Trypanosoma\"\n",
    "                 ]\n",
    "    for genus in order_list:\n",
    "        for line in lines:\n",
    "            if line.startswith(genus):\n",
    "                o.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "amoebae plot -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SECONDS=0 \n",
    "\n",
    "CSVLIST=( $SRCHRESDIR/${FWDSRCHDIR}_sum.csv_out_1_interp_*_with_ali_col_paralogue_count_*.csv )\n",
    "amoebae plot ${CSVLIST[-1]}\\\n",
    "             --complex_info $SRCHRESDIR/complex_info_1.csv\\\n",
    "             --row_order $SRCHRESDIR/coulson_row_order.txt\\\n",
    "             --out_pdf $SRCHRESDIR/plot.pdf\n",
    "\n",
    "ELAPSED=\"Plotting these results took the following amount of time: $(($SECONDS / 3600))hrs $((($SECONDS / 60) % 60))min $(($SECONDS % 60))sec\"\n",
    "echo $ELAPSED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the resulting PDF files. Your coulson plot should look something like that in Figure 1. Compare with the results of searches for AP-2 subunits published by Manna et al. (2013), Barlow et al. (2014), and Larson et al. (2019). You will need to customize formatting of coulson plots output by the ’plot’ command using software such as Adobe Illustrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=os.path.join(os.environ['SRCHRESDIR'], \"plot_coulson_both.png\"), width=\"600px\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1:** A coulson plot summarizing similarity search results for AP-2 complex subunits in *Trypanosoma brucei gambiense* and *Saccharomyces cerevisiae* peptide and nucleotide sequences using *Arabidopsis thaliana* queries and Hidden Markov Models generated from alignments of embryophyte orthologues. BLASTP and TBLASTN were used to search peptide and nucleotide sequences, respectively, with single sequence queries, and the HMMer3 pack- age was used to perform profile searches. Subplot sectors with blue fill indicate that one or more sequences were found to meet the search criteria applied (with the number being indicated within each subplot sector). Note that the ancestral eukaryotic AP-1 and AP-2 complexes shared a single beta subunit (Dacks et al., 2008). This is why identified \"AP1beta\" orthologues are shown as a component of the AP-2 complex here, even though *T. brucei* lacks an AP-2 complex (Manna et al., 2013). These results are comparable to the relevant results published by Manna et al. (2013), Barlow et al. (2014), and Larson et al. (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation and re-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear that AMOEBAE identifies \"positive\" and \"negative\" results simply by applying criteria that the user specifies. So, it is entirely the users responsibility to select appropriate criteria and interpret the results critically.\n",
    "\n",
    "Points to consider regarding interpretation of the results of the analysis in this tutorial include the following:\n",
    "\n",
    "- The BLASTP and HMMer searches (both followed by reverse BLASTP searches) yielded the similar results in this analysis.\n",
    "\n",
    "\n",
    "- The TBLASTN searches were able to identify most of the genes represented by the peptide sequences identified by BLASTP and HMMer searches.\n",
    "\n",
    "\n",
    "- A TBLASTN hit in the *A. thaliana* chromosome 5 (NC_003076.8) met the forward and reverse search criteria, but was excluded because the translation of the region that aligned to the query was only 50 amino acids long (this sequence also contained stop codons). If you look on the NCBI genome browser for *A. thaliana* you will see that this region on chromosome 5 (as indicated in the summary CSV file) corresponds to a pseudogene for AP-2 sigma with the gene ID AT5G42568.\n",
    "\n",
    "\n",
    "- The two *A. thaliana* AP-1/2 beta paralogues and the two *S. cerevisiae* paralogues are brassicalid and fungal inparalogues, respectively, which arose from independent gene duplications. Phylogenetic analysis would be required to determine this (see Larson et al. (2019) and Barlow et al. (2014)).\n",
    "\n",
    "\n",
    "- An *Arabidopsis thaliana* AP-2 mu splice variant was excluded after running the ’find_redun_seqs’ command, because it was found to be encoded by the same gene as the other splice variant based on information in the GFF3 annotation file.\n",
    "\n",
    "\n",
    "- An *A. thaliana* AP-2 alpha gene was excluded after running the ’find_redun_seqs’ command, because it shows over 98% identity with the other AP-2 alpha gene. The summary CSV file indicates which file contains an alignment of these two sequences (see Larson et al. (2019) for relevant discussion).\n",
    "\n",
    "\n",
    "- The results for Rab2 illustrate a limitation of the type of reciprocal-best-hit search strategy employed in this tutorial. A positive hit for Rab2 was identified in *Allomyces macrogynus* using these methods. However, Elias et al. (2012) did not identify a Rab2 orthologue *A. macrogynus* in their comprehensive analysis. The positive hit in the analysis herein is a false positive result that occurs due to the absence of Rab4 (a close relative of Rab2) in *A. thaliana*. This example highlights the importance of following up similarity searching with phylogenetic analysis, which compares identified sequences to many homologues simultaneously.\n",
    "\n",
    "If the analysis in this tutorial were a project you were working on for publication, then upon completing the above analysis steps you work would have only just begun. Careful inspection of the summary CSV file will reveal that minor adjustments to the search criteria would cause the analysis to yield different results. Moreover, there are many different possibilities that would lead to innacurate results based on the criteria applied in the above analysis. A comprehensive discussion of this is beyond the scope of this tutorial. In general, I recommend that you take an iterative approach to analysis involving adjustment of search criteria and re-analysis to include sequences that you know are homologues of interest, but to exclude those that you know are not homologues of interest.\n",
    "\n",
    "To generate an alignment of homologous sequences identified using AMOEBAE, use the ’csv_to_fasta’ command to generate FASTA files for alignment, and then align using your preferred software (e.g., MUSCLE or MAFFT). For visually assessing the sequences for possible issues such as contrasting domain topologies, you may wish to generate FASTA files including all your forward search results for each query title. If you are planning to run a phylognetic analysis, you may wish to generate a FASTA file with only those sequences that match all your search criteria, and with abbreviated sequence headers the csv_to_fasta comand as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "CSVLIST=( $SRCHRESDIR/${FWDSRCHDIR}_sum.csv_out_1_interp_*_with_ali_col_paralogue_count_*.csv )\n",
    "\n",
    "amoebae csv_to_fasta ${CSVLIST[-1]} --abbrev --split_by_query_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete search output files (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete temporary files.\n",
    "#rm -r temporary_alignment_dir\n",
    "#rm -r temporary_db_dir\n",
    "#rm -r temporary_query_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete all data and results files (WARNING you may want to keep these!).\n",
    "#rm -r $DATADIR\n",
    "#rm -r $SRCHRESDIR\n",
    "#rm -r $REVSRCHDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to go from here?\n",
    "\n",
    "First, try modifying the parameters and observe how that changes the results. Then customize this notebook to search with different queries in different genomes, or run amoebae from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print this notebook\n",
    "\n",
    "This can be done at any stage; you don't need to run any of the above code first.\n",
    "\n",
    "\n",
    "First, provide a title and author name for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparams\n",
    "\n",
    "# Get name of current notebook.\n",
    "current_notebook = ipyparams.notebook_name\n",
    "print(\"Name of current notebook: \" + current_notebook)\n",
    "\n",
    "# Define author name for this notebook.\n",
    "author_name = \"\"\n",
    "\n",
    "# Define title of this notebook.\n",
    "notebook_title = current_notebook.rsplit('.', 1)[0].replace('_', ' ') # Or, write your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and checkpoint the current notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// Save and checkpoint the current notebook (same as doing it manually through the GUI).\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write this notebook to a PDF file (with formatted citations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules.\n",
    "import os\n",
    "from string import Template\n",
    "\n",
    "# Check that a file containing citation information in bibtex format exists.\n",
    "bibtex_file_path = notebook_title.rsplit('.', 1)[0] + '.bib'\n",
    "assert os.path.isfile(bibtex_file_path), \"\"\"A file containing citation information could not be found.\"\"\"\n",
    "\n",
    "# Write a latex template file for converting this notebook to latex (as an intermediate to PDF).\n",
    "latex_template_string = Template(r\"\"\"\n",
    "((*- extends 'article.tplx' -*))\n",
    "\n",
    "((* block author *))\n",
    "\\author{$an}\n",
    "((* endblock author *))\n",
    "\n",
    "((* block title *))\n",
    "\\title{$nt}\n",
    "((* endblock title *))\n",
    "\n",
    "((* block bibliography *))\n",
    "\\bibliographystyle{plain}\n",
    "\\bibliography{$rf}\n",
    "((* endblock bibliography *))\n",
    "\"\"\")\n",
    "latex_file_contents =\\\n",
    "latex_template_string.substitute(an=author_name,\n",
    "                                 nt=notebook_title,\n",
    "                                 rf=bibtex_file_path\n",
    "                                )\n",
    "latex_template_file_path = 'latex_template.tplx'\n",
    "with open(latex_template_file_path, 'w') as o:\n",
    "    o.write(latex_file_contents)\n",
    "\n",
    "# Convert notebook to PDF (with latex as an intermediate to process bibtex citations, etc.).\n",
    "!jupyter nbconvert {current_notebook} --to pdf --template {latex_template_file_path}\n",
    "\n",
    "# Remove latex template file.\n",
    "os.remove(latex_template_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Barlow, L.D., Dacks, J.B., Wideman, J.G., 2014. From all to (nearly) none: Tracing adaptin evolution in Fungi. Cellular Logistics 4, e28114. https://doi.org/10.4161/cl.28114\n",
    "\n",
    "Dacks, J.B., Poon, P.P., Field, M.C., 2008. Phylogeny of endocytic components yields insight into the process of nonendosymbiotic organelle evolution. Proceedings of the National Academy of Sciences 105, 588–593. https://doi.org/10.1073/pnas.0707318105\n",
    "\n",
    "Elias, M., Brighouse, A., Gabernet-Castello, C., Field, M.C., Dacks, J.B., 2012. Sculpting the endomembrane system in deep time: high resolution phylogenetics of Rab GTPases. Journal of Cell Science 125, 2500–2508. https://doi.org/10.1242/jcs.101378\n",
    "\n",
    "Hirst, J., D. Barlow, L., Francisco, G.C., Sahlender, D.A., Seaman, M.N.J., Dacks, J.B., Robinson, M.S., 2011. The Fifth Adaptor Protein Complex. PLoS Biology 9, e1001170. https://doi.org/10.1371/journal.pbio.1001170\n",
    "\n",
    "Larson, R.T., Dacks, J.B., Barlow, L.D., 2019. Recent gene duplications dominate evolutionary dynamics of adaptor protein complex subunits in embryophytes. Traffic 20, 961–973. https://doi.org/10.1111/tra.12698\n",
    "\n",
    "Manna, P.T., Kelly, S., Field, M.C., 2013. Adaptin evolution in kinetoplastids and emergence of the variant surface glycoprotein coat in African trypanosomatids. Molecular Phylogenetics and Evolution 67, 123–128. https://doi.org/10.1016/j.ympev.2013.01.002\n",
    "\n",
    "Robinson, M.S., 2004. Adaptable adaptors for coated vesicles. Trends in Cell Biology 14, 167–174. https://doi.org/10.1016/j.tcb.2004.02.002\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "559px",
    "width": "543px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
