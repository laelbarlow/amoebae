{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This jupyter notebook is intended purely for training purposes, and illustrates how easy it is to perform similarity searches and summarize the results with a few short lines of code. None of the code in this notebook is dependent on the main AMOEBAE library, but it reproduces some of the core functionality in a self-sufficient manner. Accordingly, it is easier to see how lines of code generate lines of results in the output files. For an introduction to running the main AMOEBAE scripts, see the amoebae/notebooks/amoebae_tutorial_2.ipynb notebook.\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Apply a reciprocal-best-hit (RBH) search strategy using Basic Local Alignment Search Tool for Protein (BLASTP) to search for orthologues of a small collection of membrane trafficking proteins in predicted peptide sequences from a handfull of genomes. \n",
    "\n",
    "- Generate a spreadsheet summarizing results of reciprocal BLASTP searches.\n",
    "\n",
    "- Visually inspect the summary of results to distinguish between positive and negative results.\n",
    "\n",
    "\n",
    "## Requirements\n",
    " \n",
    "If you are new to Jupyter notebooks, see this documentation: https://jupyter-notebook.readthedocs.io/en/stable/notebook.html. Or here: https://jupyter.brynmawr.edu/services/public/dblank/Jupyter%20Notebook%20Users%20Manual.ipynb. Or, just try it out; it's rather intuitive.\n",
    "\n",
    "You do not necessarily need to be able to read or write complex computer code to use this notebook. However, basic understanding of bash (the language used in the unix/linux shell) and python (version 3) would be advantageous. The code contained in the cells in this notebook are written in either bash or python, and the bash cells have \"%%bash\" as the first line to indicate that bash is being used.\n",
    "\n",
    "The dependencies for this notebooks are simply NCBI BLAST+ as well as some popular python libraries (biopython and pandas) that can be installed via the pip command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "from Bio import SeqIO\n",
    "import glob\n",
    "from Bio.Blast import NCBIXML\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record which version of the AMOEBAE repository this notebook is from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record git repository version information.\n",
    "wd = !pwd\n",
    "script_dir = wd[0] \n",
    "git_hash = str(subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], cwd=script_dir).strip())\n",
    "git_branch = str(subprocess.check_output([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=script_dir).strip())  \n",
    "print('\\nGit repository (code) version: ' + git_hash + ' (branch name: ' + git_branch + ')\\n')\n",
    "\n",
    "# Record system information.\n",
    "print('System info: ' + str(platform.uname()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all RefSeq peptide sequences for specific genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new subdirectory to contain output files.\n",
    "%env SD=amoebae_tutorial_1_output\n",
    "!mkdir $SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_protein.faa.gz --output $SD/Athaliana_database.faa.gz\n",
    "gunzip $SD/Athaliana_database.faa.gz\n",
    "\n",
    "curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/210/295/GCF_000210295.1_ASM21029v1/GCF_000210295.1_ASM21029v1_protein.faa.gz --output $SD/Tbrucei_database.faa.gz\n",
    "gunzip $SD/Tbrucei_database.faa.gz\n",
    "\n",
    "curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/146/045/GCF_000146045.2_R64/GCF_000146045.2_R64_protein.faa.gz --output $SD/Scerevisiae_database.faa.gz\n",
    "gunzip $SD/Scerevisiae_database.faa.gz\n",
    "\n",
    "curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/151/295/GCA_000151295.1_A_macrogynus_V3/GCA_000151295.1_A_macrogynus_V3_protein.faa.gz --output $SD/Amacrogynus_database.faa.gz\n",
    "gunzip $SD/Amacrogynus_database.faa.gz\n",
    "\n",
    "curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/004/695/GCF_000004695.1_dicty_2.7/GCF_000004695.1_dicty_2.7_protein.faa.gz --output $SD/Ddiscoideum_database.faa.gz\n",
    "gunzip $SD/Ddiscoideum_database.faa.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List downloaded FASTA files.\n",
    "ls $SD/*database.faa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate BLASTable databases from sequence files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for X in $SD/*_database.faa; do makeblastdb -in $X -dbtype prot; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List BLASTable database files.\n",
    "ls $SD/*database.faa.p*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download query peptide sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary with NCBI sequence accessions as keys and filenames to write\n",
    "# the corresponding sequences to as values.\n",
    "query_dict = {\"NP_194077.1\": \"AP1beta_Athaliana_NP_194077.1_query.faa\",\n",
    "              \"NP_851058.1\": \"AP2alpha_Athaliana_NP_851058.1_query.faa\",\n",
    "              \"NP_974895.1\": \"AP2mu_Athaliana_NP_974895.1_query.faa\",\n",
    "              \"NP_175219.1\": \"AP2sigma_Athaliana_NP_175219.1_query.faa\",\n",
    "              \"NP_566961.1\": \"Sec12_Athaliana_NP_566961.1_query.faa\",\n",
    "              \"NP_200929.1\": \"SNAP33_Athaliana_NP_200929.1_query.faa\",\n",
    "              \"NP_193449.1\": \"Rab2_Athaliana_NP_193449.1_query.faa\"\n",
    "          }\n",
    "\n",
    "# Loop over keys in the query_dict dictionary.\n",
    "for accession in query_dict.keys():\n",
    "    # Retrieve the corresponding filename from the dictionary.\n",
    "    filename = os.path.join(os.environ['SD'], query_dict[accession])\n",
    "    # Only download sequences that have not already been downloaded.\n",
    "    if not os.path.isfile(filename):\n",
    "        # Download the sequence from the NCBI Protein database.\n",
    "        accessions = [accession]\n",
    "        url = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=protein&rettype=fasta&retmode=text&id=' + ','.join(accessions)\n",
    "        r = requests.get(url)\n",
    "        with open(filename, 'w') as o:\n",
    "            o.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List downloaded query FASTA files.\n",
    "ls $SD/*_Athaliana_*_query.faa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run BLASTP searches with all queries in all databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd $SD\n",
    "for QUERY in *_query.faa\n",
    "do\n",
    "    for DATABASE in *_database.faa\n",
    "    do\n",
    "        OUTPUT=$QUERY'__'$DATABASE'__blastp_search_output.txt'\n",
    "        blastp -query $QUERY -db $DATABASE -out $OUTPUT\n",
    "        OUTPUT2=$QUERY'__'$DATABASE'__blastp_search_output.xml'\n",
    "        blastp -query $QUERY -db $DATABASE -out $OUTPUT2 -outfmt 5\n",
    "    done\n",
    "done\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List forward search BLASTP output files.\n",
    "ls $SD/*__blastp_search_output.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize initial search results in a spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a CSV file.\n",
    "summary_file_path = os.path.join(os.environ['SD'], '0_summary_of_forward_blastp_searches.csv')\n",
    "with open(summary_file_path, 'w') as o:\n",
    "    # Write a line containing column headers.\n",
    "    o.write(','.join(['Query',\n",
    "                      'Database',\n",
    "                      'Hit rank',\n",
    "                      'ID',\n",
    "                      'Description',\n",
    "                      'E-value\\n']))\n",
    "    # Loop over the XML format BLASTP output files.\n",
    "    for blastp_output in glob.glob(os.path.join(os.environ['SD'], '*blastp_search_output.xml')):\n",
    "        # Open XML file.\n",
    "        with open(blastp_output) as blastp_output_handle:\n",
    "            # Loop over BLAST results (only one query was used, so there should only be one BLAST result anyway).\n",
    "            for blast_record in NCBIXML.parse(blastp_output_handle):\n",
    "                hit_rank = 0\n",
    "                # Loop over hits in the BLAST result.\n",
    "                for hit in blast_record.descriptions:\n",
    "                    hit_rank += 1\n",
    "                    # Ignore hits after the first 10 hits.\n",
    "                    if hit_rank <= 10:\n",
    "                        # Parse the sequence ID/accession out of the title attribute of the hit object.\n",
    "                        hit_id = hit.title.split(' ', 2)[1]\n",
    "                        # Parse the sequence description out of the title attribute of the hit object.\n",
    "                        hit_description = hit.title.split(' ', 2)[2]\n",
    "                        # Write a line with information about this hit to the open CSV file. \n",
    "                        o.write(','.join([os.path.basename(blastp_output).split('__')[0], os.path.basename(blastp_output).split('__')[1],\n",
    "                            str(hit_rank), hit_id, '\\\"' + hit_description + '\\\"', str(hit.e)]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visually inspect summary of forward search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the CSV file using the pandas library.\n",
    "df = pd.read_csv(summary_file_path)\n",
    "# Display the data in an HTML table.\n",
    "print('Contents of %s:' % summary_file_path)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate reverse search query files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a dictionary to keep track of which sequences...\n",
    "db_hit_id_dict = {}\n",
    "with open(summary_file_path) as infh:\n",
    "    for line in infh:\n",
    "        if not line.startswith('Query') and not line.startswith('\\n'):\n",
    "            db_file = os.path.join(os.environ['SD'], line.split(',')[1])\n",
    "            hit_id = line.split(',')[3]\n",
    "            if db_file not in db_hit_id_dict.keys():\n",
    "                db_hit_id_dict[db_file] = [hit_id]\n",
    "            else:\n",
    "                db_hit_id_dict[db_file].append(hit_id)\n",
    "for database in db_hit_id_dict.keys():\n",
    "    with open(database) as infh:\n",
    "        for seq in SeqIO.parse(infh, 'fasta'):\n",
    "            if seq.id in set(db_hit_id_dict[database]):\n",
    "                with open(database + '_' + seq.id + '_reverse_query.faa', 'w') as o:\n",
    "                    SeqIO.write(seq, o, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List reverse search query FASTA files.\n",
    "ls $SD/*_reverse_query.faa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run BLASTP to search with all reverse search queries in a sequence database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "REVSRCHDB='Athaliana_database.faa'\n",
    "cd $SD\n",
    "for QUERY in *_reverse_query.faa\n",
    "do\n",
    "    OUTPUT=$QUERY'__'$DATABASE'__blastp_reverse_search_output.txt'\n",
    "    blastp -query $QUERY -db $REVSRCHDB -out $OUTPUT\n",
    "    OUTPUT2=$QUERY'__'$DATABASE'__blastp_reverse_search_output.xml'\n",
    "    blastp -query $QUERY -db $REVSRCHDB -out $OUTPUT2 -outfmt 5\n",
    "done\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List reverse search BLAST output files.\n",
    "ls $SD/*__blastp_reverse_search_output.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize reverse search results by appending columns in a new spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_file_path_2 = os.path.join(os.environ['SD'], '0_summary_of_forward_and_reverse_blastp_searches.csv')\n",
    "with open(summary_file_path) as infh,\\\n",
    "    open(summary_file_path_2, 'w') as o:\n",
    "    o.write(','.join(['Query',\n",
    "                      'Database',\n",
    "                      'Hit rank',\n",
    "                      'ID',\n",
    "                      'Hit description',\n",
    "                      'E-value',\n",
    "                      'Top reverse search hit ID',\n",
    "                      'Top reverse search hit description',\n",
    "                      'Top reverse search hit E-value\\n']))\n",
    "           \n",
    "    for line in infh:\n",
    "        if not line.startswith('Query'):\n",
    "            fwd_hit_id = line.split(',')[3]\n",
    "            for blastp_output in glob.glob(os.path.join(os.environ['SD'], '*blastp_reverse_search_output.xml')):\n",
    "                if fwd_hit_id in blastp_output:\n",
    "                    at_least_one_hit = False\n",
    "                    with open(blastp_output) as blastp_output_handle:\n",
    "                        if len(list(NCBIXML.parse(blastp_output_handle))[0].descriptions) >= 1:\n",
    "                            at_least_one_hit = True\n",
    "                    if at_least_one_hit:\n",
    "                        with open(blastp_output) as blastp_output_handle:\n",
    "                            top_rev_hit = list(NCBIXML.parse(blastp_output_handle))[0].descriptions[0]\n",
    "                            top_rev_hit_id = top_rev_hit.title.split(' ', 2)[1]\n",
    "                            top_rev_hit_description = top_rev_hit.title.split(' ', 2)[2]\n",
    "                            o.write(','.join([line.strip(), top_rev_hit_id, '\\\"' + top_rev_hit_description + '\\\"', str(top_rev_hit.e)]) + '\\n')\n",
    "                    else:\n",
    "                        o.write(line.strip() + ',No reverse search hits\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visually inspect summary of forward and reverse search results\n",
    "\n",
    "The cell below will display the data in an HTML table, but you will probably find it more useful to view the contents of the CSV file using a spreadsheet program like microsoft Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data from the CSV file using the pandas library.\n",
    "df = pd.read_csv(summary_file_path_2)\n",
    "# Display the data in an HTML table.\n",
    "print('Contents of %s:' % summary_file_path_2)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret results\n",
    "Modify the table below to reflect your interpretation of the above results. Carefully consider potential sources of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1: Summary of similarity search results.** Numbers indicate the number of orthologues of each protein in each genome.\n",
    "\n",
    "|     Genome      | AP-2 beta | AP-2 alpha | AP-2 mu | AP-2 sigma | Sec12 | SNAP33 | Rab2 |\n",
    "|       ---       |    ---    |    ---     |   ---   |    ---     |  ---  |   ---  | ---  |\n",
    "| *A. thaliana*   |     0     |     0      |    0    |     0      |   0   |    0   |  0   |\n",
    "| *T. brucei*     |     0     |     0      |    0    |     0      |   0   |    0   |  0   |\n",
    "| *D. discoideum* |     0     |     0      |    0    |     0      |   0   |    0   |  0   |\n",
    "| *A. macrogynus* |     0     |     0      |    0    |     0      |   0   |    0   |  0   |\n",
    "| *S. cerevisiae* |     0     |     0      |    0    |     0      |   0   |    0   |  0   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// Save and checkpoint the current notebook (same as doing it manually through the GUI).\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print this notebook\n",
    "This is optional, and will require installation of additional dependencies: nbconvert, pandoc, and latex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define author name for this notebook.\n",
    "author_name = \"\"\n",
    "\n",
    "# Define title of this notebook.\n",
    "notebook_title = 'amoebae_tutorial_1'.replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules.\n",
    "import os\n",
    "from string import Template\n",
    "\n",
    "# Write a latex template file for converting this notebook to latex (as an intermediate to PDF).\n",
    "latex_template_string = Template(r\"\"\"\n",
    "((*- extends 'article.tplx' -*))\n",
    "\n",
    "((* block author *))\n",
    "\\author{$an}\n",
    "((* endblock author *))\n",
    "\n",
    "((* block title *))\n",
    "\\title{$nt}\n",
    "((* endblock title *))\n",
    "\"\"\")\n",
    "latex_file_contents =\\\n",
    "latex_template_string.substitute(an=author_name,\n",
    "                                 nt=notebook_title\n",
    "                                )\n",
    "latex_template_file_path = 'latex_template.tplx'\n",
    "with open(latex_template_file_path, 'w') as o:\n",
    "    o.write(latex_file_contents)\n",
    "\n",
    "# Convert notebook to PDF (with latex as an intermediate to process bibtex citations, etc.).\n",
    "!jupyter nbconvert ./amoebae_tutorial_1.ipynb --to pdf --template {latex_template_file_path}\n",
    "\n",
    "# Remove latex template file and bibtex file.\n",
    "os.remove(latex_template_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "**The first publication to report the use of AMOEBAE for comparative genomics**:\n",
    "\n",
    "Larson, R.T., Dacks, J.B., Barlow, L.D., 2019. Recent gene duplications dominate evolutionary dynamics of adaptor protein complex subunits in embryophytes. Traffic tra.12698. https://doi.org/10.1111/tra.12698\n",
    "\n",
    "\n",
    "**The AMOEBAE GitHub Repository**:\n",
    "\n",
    "https://github.com/laelbarlow/amoebae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
