
"""
SnakeMake workflow definition file for AMOEBAE workflow.

"""

import os
import shutil
import glob
from snakemake.io import expand, glob_wildcards
from scripts.get_filenames_from_csv import get_query_filenames_from_csv,\
    get_database_filenames_from_csv
from scripts.get_param_dict_for_add_to_dbs import get_param_dict_for_add_to_dbs

# Define path to CSV file listing genomes/proteomes/transcriptomes to use.
database_csv = 'config/genomes.csv'
# Get a complete list of filenames for genomes/proteomes/transcriptome files.
database_names_all = get_database_filenames_from_csv(database_csv, 'all')
# Get a list of filenames for genomes/proteomes/transcriptome files to be
# downloaded.
database_names_ncbi = get_database_filenames_from_csv(database_csv, 'ncbi')
# Get a list of filenames for genomes/proteomes/transcriptome files that are
# already present in a local directory.
database_names_local = get_database_filenames_from_csv(database_csv, 'local')

# Define path to CSV file listing queries to use.
query_csv = 'config/queries.csv'
# Get inclusive lists of query filenames for queries to be downloaded from NCBI.
single_fasta_query_names_all, multi_fasta_query_names_all =\
get_query_filenames_from_csv(query_csv, 'all')
# Get lists of query filenames for queries to be downloaded from NCBI.
single_fasta_query_names_ncbi, multi_fasta_query_names_ncbi =\
get_query_filenames_from_csv(query_csv, 'ncbi')
# Get lists of query filenames that are already present in a local directory.
single_fasta_query_names_local, multi_fasta_query_names_local =\
get_query_filenames_from_csv(query_csv, 'local')

# Define path to report file.
report: "report/workflow.rst"

# Compile a nonredundant list of query titles.
query_titles = list(set([x.split('_', 1)[0] for x in \
    single_fasta_query_names_all + multi_fasta_query_names_all]))

# Get a dictionary of parameters for the add_to_dbs rule with filenames as keys
# and parameter tuples as values.
add_to_dbs_params = get_param_dict_for_add_to_dbs(database_csv)

# Check that all local database and query files in resources directory are
# listed in the relevant CSV files (this could indicate that the CSV files are
# not complete).
for f in glob.glob('resources/local_db_files/*'):
    if f.endswith('.fna') or f.endswith('.faa') or f.endswith('.gff3'):
        # Check that input file name is in the list from the CSV file.
        assert os.path.basename(f) in add_to_dbs_params.keys(),\
        """Error: File with filename %s is not listed in CSV file %s.""" % \
        (os.path.basename(f), database_csv)
for f in glob.glob('resources/local_query_files/*'):
    if f.endswith('.faa') or f.endswith('.afaa'):
        # Check that file is listed in input CSV file.
        assert os.path.basename(f) in single_fasta_query_names_all + multi_fasta_query_names_all,\
        """Error: File with filename %s is not listed in CSV file %s.""" % \
        (os.path.basename(f), query_csv)

# Copy template data directory from the resources directory to the results
# directory if it doesn't already exist.
if not os.path.isdir('results/AMOEBAE_Data'):
    shutil.copytree('resources/AMOEBAE_Data_template_directory', 'results/AMOEBAE_Data')

# Define rules to be run on the head/login node of a cluster as opposed to
# being submitted as jobs via the job scheduler.
localrules: final_results, download_dbs, download_queries, \
list_dbs_and_queries, help


# Define global container image (for if the --use-singularity option is used).
container: "docker://continuumio/miniconda3:4.8.2"

# Define global environment modules (for if the --use-envmodules option is
# used).
global_envmodules = open('workflow/envs/amoebae_env_modules.txt', 'r').read().split('\n')


# Define rules for the workflow.

rule final_results:
    """
    Run the full workflow to analyze a given set of peptide sequence files.

    All output files (including intermediate data files) are written to the
    results directory.
    """
    input:
        'results/workflow_diagram.pdf',
        'config/Ref_seqs_1_manual_selections.csv',
        'results/fwd_srchs_1_rev_srch_1_interp_with_ali_col_nonredun.csv',
        'results/plot.pdf',
        'results/plot_coulson_both.pdf',
        'results/fwd_srchs_1_rev_srch_1_interp_with_ali_col_nonredun_fasta_ali_files'


rule help:
    """
    Print list of all targets with help.
    """
    run:
        for rule in workflow.rules:
            print(rule.name)
            print(rule.docstring)


rule download_dbs:
    """
    Download FASTA files to use for analysis.
    """
    input:
        script = 'workflow/scripts/download_dbs.py',
        csv_file = database_csv
    output:
        expand('results/temp_databases/{data_file}', data_file = database_names_ncbi)
    conda:
        'envs/amoebae_env.yaml'
    shell:
        """
    	python3 {input.script} {input.csv_file} results/temp_databases
        """


rule download_queries:
    """
    Download (and construct) query sequences and multiple sequence alignments to use as search queries.
    """
    input:
        script = 'workflow/scripts/download_ncbi_query_seqs.py',
        csv_file = query_csv

    output:
        expand('results/temp_single_seq_queries/{query_file}', \
            query_file = single_fasta_query_names_ncbi) + \
        expand('results/temp_multi_seq_queries/{query_file}', \
            query_file = multi_fasta_query_names_ncbi)

    shell:
        """
    	python3 {input.script} {input.csv_file} \
                results/temp_single_seq_queries \
                results/temp_multi_seq_queries
        """


rule add_to_dbs:        
    """
    Format and add a file to a formatted directory.

    ***Currently assumes output files have same extension as input files!
    """
    input:
        amoebae_script = 'amoebae',
        split_char_script = 'workflow/scripts/get_split_char_for_file.py',
        split_pos_script = 'workflow/scripts/get_split_pos_for_file.py',
        data_dir = 'results/AMOEBAE_Data',
        input_file = 'results/temp_databases/{file}'

    output:
        output_file = 'results/AMOEBAE_Data/Genomes/{file}'

    wildcard_constraints:
        input_file = "(.+faa|.+fna|.+gff3)"

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'sc=$( python3 {input.split_char_script} ' + database_csv + ' {input.input_file} ) && '
        'sp=$( python3 {input.split_pos_script} ' + database_csv + ' {input.input_file} ) && '
        'python3 {input.amoebae_script} '
         'add_to_dbs '
             '{input.input_file} '
             '{input.data_dir} '
             '--split_char $sc '
             '--split_pos $sp '


rule add_to_dbs_local:        
    """
    Format and add a file to a formatted directory.

    ***Currently assumes output files have same extension as input files!
    """
    input:
        data_dir = 'results/AMOEBAE_Data',
        split_char_script = 'workflow/scripts/get_split_char_for_file.py',
        split_pos_script = 'workflow/scripts/get_split_pos_for_file.py',
        amoebae_script = 'amoebae',
        input_file = 'resources/local_db_files/{file}'

    output:
        output_file = 'results/AMOEBAE_Data/Genomes/{file}'

    wildcard_constraints:
        input_file = "(.+faa|.+fna|.+gff3)"

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'sc=$( python3 {input.split_char_script} ' + database_csv + ' {input.input_file} ) && '
        'sp=$( python3 {input.split_pos_script} ' + database_csv + ' {input.input_file} ) && '
        'python3 {input.amoebae_script} '
            'add_to_dbs '
                '{input.input_file} '
                '{input.data_dir} '
                     '--split_char $sc '
                     '--split_pos $sp '


rule add_to_query_seqs:
    """
    Add a query file to a formatted directory.

    ***Currently assumes output files have same extension as input files!
    """
    input:
        data_dir = 'results/AMOEBAE_Data',
        amoebae_script = 'amoebae',
        input_file = 'results/temp_single_seq_queries/{query_file}.faa'

    output:
        output_file = 'results/AMOEBAE_Data/Queries/{query_file}.faa'

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
        'add_to_queries '
            '{input.input_file} '
            '{input.data_dir} '
        


rule add_to_query_seqs_local:
    """
    Add a query file to a formatted directory.

    ***Currently assumes output files have same extension as input files!
    """
    input:
        data_dir = 'results/AMOEBAE_Data',
        amoebae_script = 'amoebae',
        input_file = 'resources/local_query_files/{query_file}.faa'

    output:
        output_file = 'results/AMOEBAE_Data/Queries/{query_file}.faa'

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
        'add_to_queries '
            '{input.input_file} '
            '{input.data_dir} '
        


rule align_fasta:
    """
    Align FASTA files used for HMM searching.
    """
    input:
        amoebae_script = 'amoebae',
        input_file = 'results/temp_multi_seq_queries/{file}.faa'
    
    output:
        output_file = 'results/temp_multi_seq_queries/{file}.afaa'
        
    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
            'align_fa '
                '{input.input_file} '
                '--output_format fasta '


rule add_to_query_ali:
    """
    Add a query file to a formatted directory.

    ***Currently assumes output files have same extension as input files!
    """
    input:
        data_dir = 'results/AMOEBAE_Data',
        amoebae_script = 'amoebae',
        input_file = 'results/temp_multi_seq_queries/{query_file}.afaa'

    output:
        output_file = 'results/AMOEBAE_Data/Queries/{query_file}.afaa'

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
           'add_to_queries '
               '{input.input_file} '
               '{input.data_dir} '
        


rule add_to_query_ali_local:
    """
    Add a query file to a formatted directory.

    ***Currently assumes output files have same extension as input files!
    """
    input:
        data_dir = 'results/AMOEBAE_Data',
        amoebae_script = 'amoebae',
        input_file = 'resources/local_query_files/{query_file}.afaa'

    output:
        output_file = 'results/AMOEBAE_Data/Queries/{query_file}.afaa'

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
           'add_to_queries '
               '{input.input_file} '
               '{input.data_dir} '
        

rule list_dbs_and_queries:
    """
    Read input CSV file listing information about database/genome FASTA files
    and CSV file listing information about query files, and write text files
    listing all filenames from each to the results directory. 
    """
    input:
        'config/genomes.csv',
        'config/queries.csv'

    output:
        dbs = 'results/db_list.txt',
        queries = 'results/query_list.txt',
        coulson_plot_org = 'results/coulson_plot_organization.csv',

    run:
        # Get list of .faa and .fna database files (excluding .gff3 files).
        fa_database_names_all = [x for x in database_names_all if \
                                 x.endswith('.faa') or x.endswith('.fna')]

        # Get alignment names from multifasta names.
        alignment_fasta_query_names = [x.rsplit('.', 1)[0] + '.afaa' for x in \
        multi_fasta_query_names_all]

        # Write lists to files.
        with open(output.dbs, 'w') as dbsfh, open(output.queries, 'w') as queriesfh:
            # Write database name list to file.
            dbsfh.write('\n'.join(fa_database_names_all))
            # Write query name list to file.
            queriesfh.write('\n'.join(single_fasta_query_names_all + alignment_fasta_query_names))

        # Write basic pre-filled coulson plot organization CSV file.
        with open(output.coulson_plot_org, 'w') as coulson_plot_org:
            # Make a non-redundant list of query names.
            nonredun_query_names = list(set(single_fasta_query_names_all + \
                                   alignment_fasta_query_names))
            # Sort alphabetically.
            nonredun_query_names.sort()
            # Write lines, just using the first character of the query name as the
            # category name.
            for i in nonredun_query_names:
                coulson_plot_org.write(i[0] + ',' + i + '\n')


rule get_ref_seqs:
    """
    Run searches with queries to find redundant hits in databases (for
    interpreting results).
    """
    input:
        expand('results/AMOEBAE_Data/Genomes/{data_file}', data_file=database_names_all),
        expand('results/AMOEBAE_Data/Queries/{query_file}', query_file = \
            single_fasta_query_names_all + \
            [x.rsplit('.', 1)[0] + '.afaa' for x in multi_fasta_query_names_all]),
        amoebae_script = 'amoebae',
        db_list_file = 'config/reference_db_list.txt',
        query_list_file = 'results/query_list.txt'

    output:
        ref_seq_dir = directory('results/Ref_seqs_1'),
        ref_seq_pred_csv_copy = 'results/Ref_seqs_1_auto_predictions.csv',
        #ref_seq_manual_copy_csv = 'results/Ref_seqs_1_manual_selections.csv'

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} ' + \
            'get_redun_hits ' + \
                'results/Ref_seqs_1 ' + \
                'results/AMOEBAE_Data ' + \
                '--db_list_file {input.db_list_file} ' + \
                '--query_list_file {input.query_list_file} ' + \
                '--csv_file ' + \
                'results/Ref_seqs_1/0_ref_seqs.csv ' + \
                '--predict_redun_hit_selection ' + \
                '--max_number_of_hits_to_summarize 200 ' + \
        ' && cp results/Ref_seqs_1/0_ref_seqs_auto_predictions.csv ' + \
               'results/Ref_seqs_1_auto_predictions.csv'


rule setup_and_run_fwd_srch:
    """
    Make directory in which to perform forward searches.
    Perform searches with given queries into given dbs.
    """
    input:
        expand('results/AMOEBAE_Data/Genomes/{data_file}', data_file=database_names_all),
        expand('results/AMOEBAE_Data/Queries/{query_file}', query_file = \
            single_fasta_query_names_all + \
            [x.rsplit('.', 1)[0] + '.afaa' for x in multi_fasta_query_names_all]),
        amoebae_script = 'amoebae',
        db_list_file = 'results/db_list.txt',
        query_list_file = 'results/query_list.txt'

    output:
        outdir = directory('results/fwd_srchs_1')

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} ' + \
            'setup_fwd_srch ' + \
                'results ' + \
                '{input.query_list_file} ' + \
                '{input.db_list_file} ' + \
                '--outdir {output.outdir} ' + \
        '&& ' + \
        'python3 {input.amoebae_script} ' + \
            'run_fwd_srch ' + \
                '{output.outdir} ' + \
                'results/AMOEBAE_Data '


rule sum_fwd_srch:
    """
    Append information about forward searches to csv summary file (this is used
    to organize reverse searches).
    """
    input:
        amoebae_script = 'amoebae',
        main_data_dir = 'results/AMOEBAE_Data',
        fwd_srch_out = 'results/fwd_srchs_1' 

    output:
        csv_out_path = 'results/fwd_srchs_1.csv'

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
            'sum_fwd_srch ' + \
                '{input.fwd_srch_out} ' + \
                '{output.csv_out_path} ' + \
                '{input.main_data_dir} ' + \
                '--max_hits_to_sum 5 ' + \
                '--max_gap_between_tblastn_hsps 1000 '


rule setup_and_run_rev_srch:
    """
    Make a directory in which to perform reverse searches.  Perform searches
    with given forward search hits into given db.
    """
    input:
        reverse_srch_db_file = 'config/reference_db_list.txt',
        amoebae_script = 'amoebae',
        main_data_dir = 'results/AMOEBAE_Data',
        fwd_srch_csv = 'results/fwd_srchs_1.csv'

    output:
        out_dir = directory('results/rev_srchs_1')

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
            'setup_rev_srch ' + \
                'results ' + \
                '{input.fwd_srch_csv} ' + \
                '{input.reverse_srch_db_file} ' + \
                '{input.main_data_dir} ' + \
                '--outdir  {output.out_dir} ' + \
                '--aasubseq ' + \
        '&& ' + \
        'python3 {input.amoebae_script} '
            'run_rev_srch ' + \
                '{output.out_dir} ' + \
                '{input.main_data_dir} '


rule sum_rev_srch:
    """
    Append information about reverse searches to csv summary file.
    """
    input:
        amoebae_script = 'amoebae',
        main_data_dir = 'results/AMOEBAE_Data',
        rev_srch_dir = 'results/rev_srchs_1',
        fwd_srch_csv = 'results/fwd_srchs_1.csv',
        ref_seq_csv = 'config/Ref_seqs_1_manual_selections.csv'

    output:
        csv_out_path = 'results/fwd_srchs_1_rev_srch_1.csv'

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
            'sum_rev_srch ' + \
                '{input.fwd_srch_csv} ' + \
                '{input.rev_srch_dir} ' + \
                '{output.csv_out_path} ' + \
                '{input.main_data_dir} ' + \
                '--redun_hit_csv {input.ref_seq_csv} ' + \
                '--aasubseq ' + \
                '--min_evaldiff 2 '


rule interp_srchs:
    """
    Interpret search results based on summary.
    """
    input:
        amoebae_script = 'amoebae',
        rev_srch_csv = 'results/fwd_srchs_1_rev_srch_1.csv',

    output:
        interp_csv = 'results/fwd_srchs_1_rev_srch_1_interp.csv'

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
            'interp_srchs ' + \
                '{input.rev_srch_csv} ' + \
                '--out_csv_path {output.interp_csv} '


rule find_redun_seqs:
    """
    First add a column to the CSV file listing relevant alignment files to use
    for pairwise sequence comparisons. Then identify sequences likely encoded
    on redundant loci predicted for the same species.
    """ 
    input:
        amoebae_script = 'amoebae',
        main_data_dir = 'results/AMOEBAE_Data',
        interp_csv = 'results/fwd_srchs_1_rev_srch_1_interp.csv'

    output:
        interp_csv_with_ali_col = 'results/fwd_srchs_1_rev_srch_1_interp_with_ali_col.csv',
        nonredun_seq_csv = \
        report('results/fwd_srchs_1_rev_srch_1_interp_with_ali_col_nonredun.csv',
               caption='./report/find_redun_seqs.rst',
               category="Tables")

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
            'find_redun_seqs ' + \
                '{input.interp_csv} ' + \
                '{input.main_data_dir} ' + \
                '--out_csv_path {output.interp_csv_with_ali_col} ' + \
                '--add_ali_col ' + \
        '&& ' + \
        'python3 {input.amoebae_script} '
            'find_redun_seqs ' + \
                '{output.interp_csv_with_ali_col} ' + \
                '{input.main_data_dir} ' + \
                '--out_csv_path {output.nonredun_seq_csv} '


rule plot_results:
    """
    Plot search results.
    """
    input:
        amoebae_script = 'amoebae',
        coulson_org_csv = 'config/coulson_plot_organization.csv',
        nonredun_seq_csv =\
        'results/fwd_srchs_1_rev_srch_1_interp_with_ali_col_nonredun.csv',
        row_order = 'config/output_plot_row_order.txt'

    output:
        main_plot_pdf = report('results/plot.pdf',
                          caption='./report/plot_results.rst',
                          category="Plots"),
        coulson_plot_pdf = report('results/plot_coulson_both.pdf',
                          caption='./report/plot_results_coulson.rst',
                          category="Plots")

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
            'plot ' + \
                '{input.nonredun_seq_csv} ' + \
                '--complex_info {input.coulson_org_csv} ' + \
                '--row_order {input.row_order} ' + \
                '--out_pdf {output.main_plot_pdf} '


rule get_positive_hit_fastas:
    """
    Generate FASTA files of positive hits from summary CSV file.

    """
    input:
        amoebae_script = 'amoebae',
        nonredun_seq_csv = 'results/fwd_srchs_1_rev_srch_1_interp_with_ali_col_nonredun.csv'

    output:
        fasta_dir = directory('results/fwd_srchs_1_rev_srch_1_interp_with_ali_col_nonredun_fasta_files')

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
            'csv_to_fasta ' + \
                '{input.nonredun_seq_csv} ' + \
                '--abbrev ' + \
                '--split_by_query_title '


rule get_fwd_hit_seqs:
    """
    Generate FASTA files of positive hits from summary CSV file.

    """
    input:
        amoebae_script = 'amoebae',
        fwd_hit_seq_csv = 'results/fwd_srchs_1.csv'

    output:
        fasta_dir = directory('results/fwd_srchs_1_fasta_files')

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'python3 {input.amoebae_script} '
            'csv_to_fasta ' + \
                '{input.fwd_hit_seq_csv} ' + \
                '--abbrev ' + \
                '--split_by_query_title ' + \
                '--all_hits '


rule align_positive_hit_fastas:
    """
    Generate FASTA files of positive hits from summary CSV file, and align with MUSCLE.
    """
    input:
        amoebae_script = 'amoebae',
        fasta_dir =\
        'results/fwd_srchs_1_rev_srch_1_interp_with_ali_col_nonredun_fasta_files' 

    output:
        ali_dir =\
        directory('results/fwd_srchs_1_rev_srch_1_interp_with_ali_col_nonredun_fasta_ali_files')

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell:
        'mkdir -p {output.ali_dir} && '
        'for X in {input.fasta_dir}/*.fa; '
        'do '
        'python3 {input.amoebae_script} ' + \
            'align_fa ' + \
                '{input.fasta_dir}/$( basename $X ) ' + \
                '--output_dir_path {output.ali_dir} ' + \
                '--output_format fasta '
        '; done '


rule generate_doc_pdf:
    """
    Run script to generate documentation PDF file.
    """
    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell: 
        'bash documentation/build_pdf.sh'


rule plot_workflow:
    """
    Plot the snakemake workflow defined in the Snakefile file.

    Use the Python PIL library to invert images:
    https://stackoverflow.com/questions/2498875/how-to-invert-colors-of-image-with-pil-python-imaging
    http://effbot.org/imagingbook/imageops.htm


    """
    output:
        'results/workflow_diagram.pdf'

    conda:
        'envs/amoebae_env.yaml'

    envmodules:
        global_envmodules

    shell: 
        'snakemake --cores 1 -p --rulegraph | dot -Tpdf > {output} && '
        'snakemake --cores 1 -p --rulegraph | dot -Tpng > images/example_workflow_diagram.png '

